{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pyproject.toml\n",
    "[build-system]\n",
    "requires = [\"setuptools>=70\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[project]\n",
    "name = \"data_science\"\n",
    "version = \"0.0.1\"\n",
    "description = \"General data science/ML environment\"\n",
    "authors = [{ name = \"Geoffrey Hadfield\" }]\n",
    "requires-python = \">=3.10,<3.12\"   # stay on 3.10/3.11; 3.12 still shaky for some wheels\n",
    "\n",
    "dependencies = [\n",
    "  \"numpy>=1.26\",              # keep modern NumPy\n",
    "  \"pandas>=2.2\",\n",
    "  \"scikit-learn>=1.5\",\n",
    "  \"joblib\",\n",
    "  \"matplotlib\",\n",
    "  \"seaborn\",\n",
    "  \"jupyterlab<5.0\",\n",
    "  \"ipykernel<6.30\",\n",
    "  \"dash\",\n",
    "  \"dash-bootstrap-components\",\n",
    "  \"plotly\",\n",
    "  \"opencv-python-headless\",\n",
    "  \"pillow\",\n",
    "  \"tqdm\",\n",
    "  \"statsmodels\",\n",
    "  \"streamlit\",\n",
    "  \"xgboost\",\n",
    "  \"lightgbm\",\n",
    "  \"requests\",\n",
    "  \"IPython\",\n",
    "  \"tabulate\",\n",
    "  \"pyarrow>=10.0.0\",\n",
    "  \"requests-cache\",\n",
    "  \"diskcache\",\n",
    "  \"unidecode\",\n",
    "  \"cpi>=2.0.0\",\n",
    "  \"lxml\",\n",
    "  \"duckdb>=0.10.0\",\n",
    "  \"apache-airflow>=2.9.0\",\n",
    "  # ---- Explainability stack ----\n",
    "  \"shap>=0.46.0\",             # supports NumPy 2, so fine with 1.26+\n",
    "  \"numba>=0.58.1,<0.61\",      # 0.58.1 adds NumPy 1.26 support; 0.60 adds NumPy2\n",
    "  # llvmlite will be pulled transitively with the correct version\n",
    "  # ---- NBA tooling ----\n",
    "  \"nba_api<=1.4.1\",\n",
    "  \"beautifulsoup4\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "spark = [\n",
    "  \"pyspark\",\n",
    "  \"install-jdk>=1.1.0\",\n",
    "]\n",
    "dev = [\n",
    "  \"pytest\",\n",
    "  \"black\",\n",
    "  \"flake8\",\n",
    "  \"mypy\",\n",
    "]\n",
    "\n",
    "[tool.black]\n",
    "line-length = 88\n",
    "target-version = [\"py310\"]\n",
    "\n",
    "[tool.flake8]\n",
    "max-line-length = 88\n",
    "extend-ignore = [\"E203\"]\n",
    "\n",
    "[tool.mypy]\n",
    "python_version = \"3.10\"\n",
    "ignore_missing_imports = true\n",
    "strict_optional = true\n",
    "\n",
    "[tool.setuptools.packages.find]\n",
    "where = [\"src\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/__init__.py\n",
    "\"\"\"\n",
    "NBA Data Pull Package\n",
    "\n",
    "A comprehensive package for fetching, processing, and analyzing NBA player data\n",
    "including salaries, statistics, and advanced metrics.\n",
    "\"\"\"\n",
    "\n",
    "__version__ = \"0.1.0\"\n",
    "__all__ = [\n",
    "    \"main\",\n",
    "    \"fetch_utils\", \n",
    "    \"process_utils\",\n",
    "    \"scrape_utils\",\n",
    "    \"data_utils\",\n",
    "    \"settings\",\n",
    "    \"notebook_helper\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/settings.py\n",
    "# src/salary_nba_data_pull/settings.py\n",
    "from pathlib import Path\n",
    "import os\n",
    "import typing as _t\n",
    "\n",
    "# 🗂️  Central data directory (override via env if needed)\n",
    "DATA_PROCESSED_DIR = Path(\n",
    "    (Path(__file__).resolve().parent.parent.parent)  # project root\n",
    "    / \"data\"\n",
    "    / \"new_processed\"\n",
    ")\n",
    "\n",
    "# optional: allow `DATA_PROCESSED_DIR=/tmp/demo python main.py …`\n",
    "ENV_OVERRIDE: _t.Optional[str] = os.getenv(\"DATA_PROCESSED_DIR\")\n",
    "if ENV_OVERRIDE:\n",
    "    DATA_PROCESSED_DIR = Path(ENV_OVERRIDE).expanduser().resolve()\n",
    "\n",
    "# Legacy path for backward compatibility\n",
    "LEGACY_DATA_PROCESSED_DIR = Path(\n",
    "    (Path(__file__).resolve().parent.parent.parent)  # project root\n",
    "    / \"data\"\n",
    "    / \"processed\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/fetch_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/fetch_utils.py\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache, wraps\n",
    "from http import HTTPStatus\n",
    "from typing import Callable\n",
    "import requests\n",
    "from nba_api.stats.endpoints import commonallplayers, commonplayerinfo, playercareerstats, leaguestandings\n",
    "from requests.exceptions import RequestException\n",
    "from json.decoder import JSONDecodeError\n",
    "from joblib import Memory\n",
    "from unidecode import unidecode\n",
    "from tenacity import (\n",
    "    retry, retry_if_exception, wait_random_exponential,\n",
    "    stop_after_attempt, before_log\n",
    ")\n",
    "\n",
    "# --- NEW: Team game logs endpoint detection ---\n",
    "try:\n",
    "    # newer nba_api\n",
    "    from nba_api.stats.endpoints import teamgamelogs as _teamgamelogs_mod\n",
    "    _HAVE_TEAMGAMELOGS_PLURAL = True\n",
    "except Exception:\n",
    "    _HAVE_TEAMGAMELOGS_PLURAL = False\n",
    "try:\n",
    "    # older nba_api\n",
    "    from nba_api.stats.endpoints import teamgamelog as _teamgamelog_mod\n",
    "    _HAVE_TEAMGAMELOG_SINGULAR = True\n",
    "except Exception:\n",
    "    _HAVE_TEAMGAMELOG_SINGULAR = False\n",
    "\n",
    "REQUESTS_PER_MIN = 8   # ↓ a bit safer for long pulls (NBA suggests ≤10)\n",
    "_SEM = threading.BoundedSemaphore(REQUESTS_PER_MIN)\n",
    "\n",
    "# Set up joblib memory for caching API responses\n",
    "cache_dir = os.path.join(os.path.dirname(__file__), '../../data/cache/nba_api')\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "\n",
    "def _throttle():\n",
    "    \"\"\"Global semaphore + sleep to stay under REQUESTS_PER_MIN.\"\"\"\n",
    "    _SEM.acquire()\n",
    "    time.sleep(60 / REQUESTS_PER_MIN)\n",
    "    _SEM.release()\n",
    "\n",
    "def _needs_retry(exc: Exception) -> bool:\n",
    "    \"\"\"Return True if we should retry.\"\"\"\n",
    "    if isinstance(exc, requests.HTTPError) and exc.response is not None:\n",
    "        code = exc.response.status_code\n",
    "        if code in (HTTPStatus.TOO_MANY_REQUESTS, HTTPStatus.SERVICE_UNAVAILABLE):\n",
    "            return True\n",
    "    return isinstance(exc, (requests.ConnectionError, requests.Timeout))\n",
    "\n",
    "def _respect_retry_after(resp: requests.Response):\n",
    "    \"\"\"Sleep for server‑suggested time if header present.\"\"\"\n",
    "    if resp is not None and 'Retry-After' in resp.headers:\n",
    "        try:\n",
    "            sleep = int(resp.headers['Retry-After'])\n",
    "            logging.warning(\"↺ server asked to wait %ss\", sleep)\n",
    "            time.sleep(sleep)\n",
    "        except ValueError:\n",
    "            pass   # header unparsable, ignore\n",
    "\n",
    "def _make_retry(fn: Callable) -> Callable:\n",
    "    \"\"\"Decorator to add tenacity retry with jitter + respect Retry-After.\"\"\"\n",
    "    @retry(\n",
    "        retry=retry_if_exception(_needs_retry),\n",
    "        wait=wait_random_exponential(multiplier=2, max=60),\n",
    "        stop=stop_after_attempt(5),\n",
    "        before_sleep=before_log(logging.getLogger(__name__), logging.WARNING),\n",
    "        reraise=True,\n",
    "    )\n",
    "    @wraps(fn)\n",
    "    def _wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except requests.HTTPError as exc:\n",
    "            _respect_retry_after(exc.response)\n",
    "            raise\n",
    "    return _wrapper\n",
    "\n",
    "@memory.cache\n",
    "@_make_retry\n",
    "def fetch_with_retry(endpoint, *, timeout=90, debug=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Thread‑safe, rate‑limited, cached NBA‑Stats call with adaptive back‑off.\n",
    "    \"\"\"\n",
    "    _throttle()\n",
    "    start = time.perf_counter()\n",
    "    resp = endpoint(timeout=timeout, **kwargs)\n",
    "    df = resp.get_data_frames()[0]\n",
    "    if debug:\n",
    "        logging.debug(\"✓ %s in %.1fs %s\", endpoint.__name__,\n",
    "                      time.perf_counter() - start, kwargs)\n",
    "    return df\n",
    "\n",
    "@memory.cache\n",
    "def fetch_all_players(season: str, debug: bool = False) -> dict[str, dict]:\n",
    "    \"\"\"Return {clean_name: {'player_id':…, 'team_id':…}} for *active* roster.\"\"\"\n",
    "    roster_df = fetch_with_retry(\n",
    "        commonallplayers.CommonAllPlayers,\n",
    "        season=season,\n",
    "        is_only_current_season=1,        # <‑‑ key fix\n",
    "        league_id=\"00\",\n",
    "        debug=debug,\n",
    "    )\n",
    "    players: dict[str, dict] = {}\n",
    "    if roster_df is not None:\n",
    "        for _, row in roster_df.iterrows():\n",
    "            clean = unidecode(row[\"DISPLAY_FIRST_LAST\"]).strip().lower()\n",
    "            players[clean] = {\n",
    "                \"player_id\": int(row[\"PERSON_ID\"]),\n",
    "                \"team_id\": int(row[\"TEAM_ID\"]),\n",
    "            }\n",
    "    if debug:\n",
    "        print(f\"[fetch_all_players] {len(players)} active players for {season}\")\n",
    "    return players\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def fetch_season_players(season: str, debug: bool = False) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Return {clean_name: {'player_id':…, 'team_id':…}} for *everyone who was\n",
    "    on a roster at any time during the given season*.\n",
    "    \"\"\"\n",
    "    # call once for the whole database (not \"current‑season only\")\n",
    "    df = fetch_with_retry(\n",
    "        commonallplayers.CommonAllPlayers,\n",
    "        season=season,\n",
    "        is_only_current_season=0,         # <-- key change\n",
    "        league_id=\"00\",\n",
    "        debug=debug,\n",
    "    )\n",
    "    players: dict[str, dict] = {}\n",
    "    if df is not None:\n",
    "        yr = int(season[:4])\n",
    "        # keep rows whose career window encloses this season\n",
    "        df = df[(df.FROM_YEAR.astype(int) <= yr) & (df.TO_YEAR.astype(int) >= yr)]\n",
    "        for _, row in df.iterrows():\n",
    "            clean = unidecode(row[\"DISPLAY_FIRST_LAST\"]).strip().lower()\n",
    "            players[clean] = {\n",
    "                \"player_id\": int(row[\"PERSON_ID\"]),\n",
    "                \"team_id\": int(row[\"TEAM_ID\"]),\n",
    "            }\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[fetch_season_players] {len(players)} players for {season}\")\n",
    "    return players\n",
    "\n",
    "@memory.cache\n",
    "def fetch_player_info(player_id, debug=False):\n",
    "    return fetch_with_retry(commonplayerinfo.CommonPlayerInfo, player_id=player_id, debug=debug)\n",
    "\n",
    "@memory.cache\n",
    "def fetch_career_stats(player_id, debug=False):\n",
    "    return fetch_with_retry(playercareerstats.PlayerCareerStats, player_id=player_id, debug=debug)\n",
    "\n",
    "@memory.cache\n",
    "def fetch_league_standings(season, debug=False):\n",
    "    return fetch_with_retry(leaguestandings.LeagueStandings, season=season, debug=debug)\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear the joblib memory cache.\"\"\"\n",
    "    memory.clear()\n",
    "\n",
    "@memory.cache\n",
    "def fetch_team_wl_by_season(season: str,\n",
    "                            season_type: str = \"Regular Season\",\n",
    "                            debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return per‑team W/L for a season from team game logs.\n",
    "    Robust to nba_api versions:\n",
    "      - TeamGameLogs(...).get_data_frames()[0]  (new)\n",
    "      - TeamGameLog(...).get_data_frames()[0]   (old)\n",
    "    We do not fill; if logs are empty, we return an empty DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    if _HAVE_TEAMGAMELOGS_PLURAL:\n",
    "        # new endpoint signature (nullable arg names in newer APIs)\n",
    "        df = fetch_with_retry(\n",
    "            _teamgamelogs_mod.TeamGameLogs,\n",
    "            season_nullable=season,\n",
    "            season_type_nullable=season_type,\n",
    "            debug=debug,\n",
    "        )\n",
    "    elif _HAVE_TEAMGAMELOG_SINGULAR:\n",
    "        # older endpoint\n",
    "        df = fetch_with_retry(\n",
    "            _teamgamelog_mod.TeamGameLog,\n",
    "            season=season,\n",
    "            season_type_all_star=season_type,\n",
    "            debug=debug,\n",
    "        )\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"[fetch_team_wl_by_season] no team game log endpoint available\")\n",
    "        return pd.DataFrame(columns=[\"TeamID\", \"Wins\", \"Losses\"])\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        if debug:\n",
    "            print(f\"[fetch_team_wl_by_season] empty logs for {season}\")\n",
    "        return pd.DataFrame(columns=[\"TeamID\", \"Wins\", \"Losses\"])\n",
    "\n",
    "    # Normalize column names used across versions\n",
    "    # Expect at least TEAM_ID and WL fields.\n",
    "    cols = {c.upper(): c for c in df.columns}\n",
    "    team_id_col = cols.get(\"TEAM_ID\", None)\n",
    "    wl_col = cols.get(\"WL\", None)\n",
    "\n",
    "    if team_id_col is None or wl_col is None:\n",
    "        if debug:\n",
    "            print(f\"[fetch_team_wl_by_season] required columns missing in logs \"\n",
    "                  f\"{list(df.columns)}\")\n",
    "        return pd.DataFrame(columns=[\"TeamID\", \"Wins\", \"Losses\"])\n",
    "\n",
    "    # Count W/L by team\n",
    "    grp = (df.assign(_W=(df[wl_col] == \"W\").astype(int),\n",
    "                     _L=(df[wl_col] == \"L\").astype(int))\n",
    "             .groupby(df[team_id_col], dropna=False)[[\"_W\", \"_L\"]]\n",
    "             .sum()\n",
    "             .rename(columns={\"_W\": \"Wins\", \"_L\": \"Losses\"})\n",
    "             .reset_index()\n",
    "             .rename(columns={team_id_col: \"TeamID\"}))\n",
    "\n",
    "    if debug:\n",
    "        tot_w = int(grp[\"Wins\"].sum())\n",
    "        tot_l = int(grp[\"Losses\"].sum())\n",
    "        print(f\"[fetch_team_wl_by_season] {season} totals: W={tot_w}, L={tot_l}\")\n",
    "\n",
    "    return grp\n",
    "\n",
    "@memory.cache\n",
    "def fetch_team_wl_lookup(season: str,\n",
    "                         season_type: str = \"Regular Season\",\n",
    "                         debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Unified W/L by TeamID for a season.\n",
    "    Primary: team game logs aggregation (robust across nba_api versions).\n",
    "    Fallback: LeagueStandings endpoint.\n",
    "    Returns columns: TeamID, Wins, Losses (one row per TeamID).\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Primary\n",
    "    logs = fetch_team_wl_by_season(season, season_type=season_type, debug=debug)\n",
    "    logs = logs.rename(columns={\"Wins\": \"Wins_logs\", \"Losses\": \"Losses_logs\"})\n",
    "\n",
    "    # Fallback (LeagueStandings)\n",
    "    st = fetch_league_standings(season, debug=debug)\n",
    "    # nba_api LeagueStandings uses uppercase WINS/LOSSES\n",
    "    want_cols = {}\n",
    "    for c in st.columns:\n",
    "        uc = str(c).upper()\n",
    "        if uc == \"TEAM_ID\": want_cols[c] = \"TeamID\"\n",
    "        if uc == \"WINS\":    want_cols[c] = \"Wins_stand\"\n",
    "        if uc == \"LOSSES\":  want_cols[c] = \"Losses_stand\"\n",
    "    st = st.rename(columns=want_cols)\n",
    "    st = st[[c for c in [\"TeamID\", \"Wins_stand\", \"Losses_stand\"] if c in st.columns]].drop_duplicates(\"TeamID\")\n",
    "\n",
    "    # Outer join both sources on TeamID, then coalesce\n",
    "    out = pd.merge(logs, st, on=\"TeamID\", how=\"outer\", validate=\"1:1\")\n",
    "    out[\"Wins\"]   = out[\"Wins_logs\"].combine_first(out[\"Wins_stand\"])\n",
    "    out[\"Losses\"] = out[\"Losses_logs\"].combine_first(out[\"Losses_stand\"])\n",
    "    out = out[[\"TeamID\", \"Wins\", \"Losses\"]].drop_duplicates(\"TeamID\").reset_index(drop=True)\n",
    "\n",
    "    if debug:\n",
    "        miss = int(out[\"Wins\"].isna().sum())\n",
    "        if miss:\n",
    "            print(f\"[fetch_team_wl_lookup] WARN: {miss} TeamID rows still missing Wins/Losses\")\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    debug = True\n",
    "    season = \"2022-23\"\n",
    "    sample_player_name = \"LeBron James\"\n",
    "\n",
    "    # Fetch all players\n",
    "    all_players = fetch_all_players(season, debug=debug)\n",
    "    print(f\"Total players fetched: {len(all_players)}\")\n",
    "\n",
    "    # Fetch player info for a sample player\n",
    "    if sample_player_name.lower() in all_players:\n",
    "        sample_player_id = all_players[sample_player_name.lower()]['player_id']\n",
    "        player_info = fetch_player_info(sample_player_id, debug=debug)\n",
    "        print(f\"Sample player info for {sample_player_name}:\")\n",
    "        print(player_info)\n",
    "\n",
    "        # Fetch career stats for the sample player\n",
    "        career_stats = fetch_career_stats(sample_player_id, debug=debug)\n",
    "        print(f\"Sample player career stats for {sample_player_name}:\")\n",
    "        print(career_stats)\n",
    "    else:\n",
    "        print(f\"Player {sample_player_name} not found in the {season} season data.\")\n",
    "\n",
    "    # Fetch league standings\n",
    "    standings = fetch_league_standings(season, debug=debug)\n",
    "    print(\"League standings:\")\n",
    "    print(standings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/scrape_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/scrape_utils.py\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from typing import Optional\n",
    "import os\n",
    "import requests_cache\n",
    "from unidecode import unidecode\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "from functools import lru_cache\n",
    "import threading\n",
    "_ADV_LOCK   = threading.Lock()\n",
    "_ADV_CACHE: dict[str, pd.DataFrame] = {}   # season -> DataFrame\n",
    "\n",
    "# Install cache for all requests\n",
    "requests_cache.install_cache('nba_scraping', expire_after=86400)  # 24 hours\n",
    "\n",
    "# Create cached session with stale-if-error capability\n",
    "session = requests_cache.CachedSession(\n",
    "    'nba_scraping',\n",
    "    expire_after=86400,\n",
    "    stale_if_error=True       # <-- NEW: serve expired cache if remote 429s\n",
    ")\n",
    "\n",
    "def scrape_salary_cap_history(*, debug: bool = False) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Robust pull of historical cap / tax / apron lines.\n",
    "\n",
    "    Strategy:\n",
    "    1. Try RealGM (live HTML).\n",
    "    2. If the selector fails, look for an existing CSV in DATA_PROCESSED_DIR.\n",
    "    3. As a last‑chance fallback, hit NBA.com / Reuters bulletins for the\n",
    "       current season only (so we still merge *something*).\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "    url = \"https://basketball.realgm.com/nba/info/salary_cap\"\n",
    "\n",
    "    try:\n",
    "        html = requests.get(url, timeout=30).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # -------- 1️⃣  RealGM table (new markup) --------------------\n",
    "        blk = soup.find(\"pre\")                      # new 2025 layout\n",
    "        if blk:                                     # parse fixed‑width block\n",
    "            rows = [r.strip().split() for r in blk.text.strip().splitlines()]\n",
    "            header = rows[0]\n",
    "            data = rows[1:]\n",
    "            df = pd.DataFrame(data, columns=header)\n",
    "        else:\n",
    "            # Legacy table path (kept for safety)\n",
    "            tbl = soup.select_one(\"table\")\n",
    "            if not tbl:\n",
    "                raise ValueError(\"salary_cap table not found\")\n",
    "            df = pd.read_html(str(tbl))[0]\n",
    "\n",
    "        # ---- normalise ----\n",
    "        df[\"Season\"] = df[\"Season\"].str.extract(r\"(\\d{4}-\\d{4})\")\n",
    "        money_cols = [c for c in df.columns if c != \"Season\"]\n",
    "        for c in money_cols:\n",
    "            df[c] = (\n",
    "                df[c]\n",
    "                .astype(str)\n",
    "                .str.replace(r\"[$,]\", \"\", regex=True)\n",
    "                .replace(\"\", pd.NA)\n",
    "                .astype(float)\n",
    "            )\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[salary‑cap] scraped {len(df)} rows from RealGM\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as exc:\n",
    "        if debug:\n",
    "            print(f\"[salary‑cap] primary scrape failed → {exc!s}\")\n",
    "\n",
    "        # -------- 2️⃣  local cached CSV ----------------------------\n",
    "        fallback = DATA_PROCESSED_DIR / \"salary_cap_history_inflated.csv\"\n",
    "        if fallback.exists():\n",
    "            if debug:\n",
    "                print(f\"[salary‑cap] using cached CSV at {fallback}\")\n",
    "            return pd.read_csv(fallback)\n",
    "\n",
    "        # -------- 3️⃣  NBA.com / Reuters one‑liner -----------------\n",
    "        try:\n",
    "            # Latest season only\n",
    "            # For now, create a minimal fallback with current season data\n",
    "            year = datetime.now().year\n",
    "            cap = 140.588  # 2024-25 cap as fallback\n",
    "            df = pd.DataFrame(\n",
    "                {\"Season\": [f\"{year}-{str(year+1)[-2:]}\"],\n",
    "                 \"Salary Cap\": [cap * 1_000_000]}\n",
    "            )\n",
    "            if debug:\n",
    "                print(\"[salary‑cap] built minimal one‑row DataFrame \"\n",
    "                      \"from fallback values\")\n",
    "            return df\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if debug:\n",
    "        print(\"[salary‑cap] giving up – no data available\")\n",
    "    return None\n",
    "\n",
    "# User-Agent header to avoid Cloudflare blocks\n",
    "UA = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/126.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 3  # seconds\n",
    "\n",
    "# Define column templates to guarantee DataFrame structure\n",
    "PLAYER_COLS = [\"Player\", \"Salary\", \"Season\"]\n",
    "TEAM_COLS = [\"Team\", \"Team_Salary\", \"Season\"]\n",
    "\n",
    "# Salary parsing pattern\n",
    "_salary_pat = re.compile(r\"\\$?\\d[\\d,]*\")\n",
    "\n",
    "def _clean_salary(text: str) -> int | None:\n",
    "    \"\"\"Return salary as int or None when text has no digits.\"\"\"\n",
    "    m = _salary_pat.search(text)\n",
    "    return int(m.group(0).replace(\",\", \"\").replace(\"$\", \"\")) if m else None\n",
    "\n",
    "# Name normalization pattern with unidecode\n",
    "def _normalise_name(raw: str) -> str:\n",
    "    \"\"\"ASCII‑fold, trim, lower.\"\"\"\n",
    "    return unidecode(raw).split(\",\")[0].split(\"(\")[0].strip().lower()\n",
    "\n",
    "\n",
    "# ------- INTERNAL HELPER --------\n",
    "def _get_hoopshype_soup(url: str, debug: bool = False) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Hit HoopsHype once with a realistic UA.  \n",
    "    Return BeautifulSoup if the page looks OK, else None.\n",
    "    \"\"\"\n",
    "    for attempt in range(2):\n",
    "        try:\n",
    "            if debug:\n",
    "                print(f\"[fetch] {url} (attempt {attempt+1})\")\n",
    "            resp = requests.get(url, headers=UA, timeout=30)\n",
    "            if resp.status_code != 200:\n",
    "                if debug:\n",
    "                    print(f\"  -> HTTP {resp.status_code}, skipping.\")\n",
    "                return None\n",
    "            html = resp.text\n",
    "            # crude Cloudflare challenge check\n",
    "            if (\"Access denied\" in html) or (\"cf-chl\" in html):\n",
    "                if debug:\n",
    "                    print(\"  -> Cloudflare challenge detected; giving up.\")\n",
    "                return None\n",
    "            return BeautifulSoup(html, \"html.parser\")\n",
    "        except requests.RequestException as e:\n",
    "            if debug:\n",
    "                print(f\"  -> network error {e}, retrying…\")\n",
    "            time.sleep(2 ** attempt + random.random())\n",
    "    return None\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _espn_salary_url(year: int, page: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Build the new ESPN salary URL. Examples:\n",
    "      page 1 → https://www.espn.com/nba/salaries/_/year/2024/seasontype/4\n",
    "      page 3 → https://www.espn.com/nba/salaries/_/year/2024/page/3/seasontype/4\n",
    "    \"\"\"\n",
    "    base = f\"https://www.espn.com/nba/salaries/_/year/{year}\"\n",
    "    return f\"{base}/seasontype/4\" if page == 1 else f\"{base}/page/{page}/seasontype/4\"\n",
    "\n",
    "\n",
    "def _scrape_espn_player_salaries(season_start: int, debug: bool = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Salary scraping was removed – consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Salary scraping was removed – consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def scrape_player_salary_data(start_season: int, end_season: int,\n",
    "                              player_filter: str | None = None,\n",
    "                              debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Salary scraping was removed – consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Salary scraping was removed – consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _scrape_espn_team_salaries(season: str, debug: bool = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Team salary scraping was removed – consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Team salary scraping was removed – consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def scrape_team_salary_data(season: str, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Team salary scraping was removed – consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Team salary scraping was removed – consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "# --- Season‑level advanced stats --------------------------------------------\n",
    "ADV_METRIC_COLS = [\n",
    "    \"PER\", \"TS%\", \"3PAr\", \"FTr\", \"ORB%\", \"DRB%\", \"TRB%\", \"AST%\", \"STL%\", \"BLK%\",\n",
    "    \"TOV%\", \"USG%\", \"OWS\", \"DWS\", \"WS\", \"WS/48\", \"OBPM\", \"DBPM\", \"BPM\", \"VORP\",\n",
    "    \"ORtg\", \"DRtg\",  # extra goodies if you want them\n",
    "]\n",
    "\n",
    "def _season_advanced_df(season: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Thread‑safe, memoised download of the *season‑wide* advanced‑stats table.\n",
    "    \n",
    "    Root-cause fixes:\n",
    "      • Use bytes (resp.content) instead of text to avoid encoding guesswork\n",
    "      • Let lxml parser handle UTF-8 charset from page's <meta> tag\n",
    "      • Use centralized name normalization\n",
    "      • Validate encoding with known Unicode names\n",
    "    \n",
    "    The first thread to request a given season does the HTTP work while holding\n",
    "    a lock; all others simply wait for the result instead of firing duplicate\n",
    "    requests. The DataFrame is cached in‑process for the life of the run.\n",
    "    \"\"\"\n",
    "    if season in _ADV_CACHE:            # fast path, no lock\n",
    "        return _ADV_CACHE[season]\n",
    "\n",
    "    with _ADV_LOCK:                     # only one thread may enter the block\n",
    "        if season in _ADV_CACHE:        # double‑checked locking\n",
    "            return _ADV_CACHE[season]\n",
    "\n",
    "        end_year = int(season[:4]) + 1\n",
    "        url = f\"https://www.basketball-reference.com/leagues/NBA_{end_year}_advanced.html\"\n",
    "        print(f\"[adv] fetching {url}\")\n",
    "        \n",
    "        # Get raw bytes to avoid encoding guesswork\n",
    "        resp = session.get(url, headers=UA, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        raw_content = resp.content  # Use bytes, not resp.text\n",
    "        \n",
    "        # Parse tables from bytes - let lxml handle charset detection\n",
    "        from io import BytesIO\n",
    "        dfs = pd.read_html(BytesIO(raw_content), flavor=\"lxml\", header=0)\n",
    "        \n",
    "        if not dfs:\n",
    "            raise ValueError(f\"No tables found at {url}\")\n",
    "        \n",
    "        # Find the table with Player column\n",
    "        df = next((t for t in dfs if \"Player\" in t.columns), dfs[0]).copy()\n",
    "        \n",
    "        # Remove repeated header rows that BBR embeds\n",
    "        if \"Player\" in df.columns:\n",
    "            df = df[df[\"Player\"] != \"Player\"]\n",
    "        \n",
    "        # Use centralized normalization\n",
    "        from salary_nba_data_pull.name_utils import normalize_name, validate_name_encoding\n",
    "        df[\"player_key\"] = df[\"Player\"].map(normalize_name)\n",
    "        \n",
    "        # Validate encoding (will raise if critical issues detected)\n",
    "        try:\n",
    "            validate_name_encoding(df, season, debug=True)\n",
    "        except AssertionError as e:\n",
    "            print(f\"[adv] WARNING: {e}\")\n",
    "            # Continue anyway but log the issue\n",
    "        \n",
    "        # Convert numeric columns\n",
    "        avail = [c for c in ADV_METRIC_COLS if c in df.columns]\n",
    "        if avail:\n",
    "            df[avail] = df[avail].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "        _ADV_CACHE[season] = df                # memoise\n",
    "        time.sleep(random.uniform(1.5, 2.5))   # be polite\n",
    "        return df\n",
    "\n",
    "def scrape_advanced_metrics(player_name: str,\n",
    "                            season: str,\n",
    "                            *,\n",
    "                            debug: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    O(1) lookup in the cached season DataFrame – zero extra HTTP traffic.\n",
    "    Uses a shared normalizer (nba_utils.normalize_name) to reduce mismatches.\n",
    "    Prints closest suggestions when no row is found (no filling).\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "\n",
    "    # Prefer the shared normalizer from nba_utils; fall back to local\n",
    "    try:\n",
    "        from api.src.airflow_project.utils.nba_utils import normalize_name as _norm\n",
    "    except Exception:\n",
    "        _norm = _normalise_name\n",
    "\n",
    "    df = _season_advanced_df(season)\n",
    "    # Ensure the season table uses the same normalizer\n",
    "    if \"player_key\" not in df.columns or df[\"player_key\"].isna().all():\n",
    "        df = df.copy()\n",
    "        df[\"player_key\"] = df[\"Player\"].map(_norm)\n",
    "\n",
    "    key = _norm(player_name)\n",
    "    row = df.loc[df.player_key == key]\n",
    "\n",
    "    if row.empty:\n",
    "        if debug:\n",
    "            # Provide top-3 closest suggestions to help diagnose mismatches\n",
    "            all_keys = df[\"player_key\"].dropna().unique().tolist()\n",
    "            suggestions = difflib.get_close_matches(key, all_keys, n=3, cutoff=0.75)\n",
    "            print(f\"[adv] no advanced stats for '{player_name}' (key='{key}') in {season}. \"\n",
    "                  f\"Closest: {suggestions}\")\n",
    "        return {}\n",
    "\n",
    "    row = row.iloc[0]\n",
    "    # Only return columns that actually exist in the DataFrame\n",
    "    available_cols = [col for col in ADV_METRIC_COLS if col in row.index]\n",
    "    result = {col: row[col] for col in available_cols}\n",
    "    if debug:\n",
    "        print(f\"[adv] {player_name} → {result}\")\n",
    "    return result\n",
    "# --- End of new season-level advanced stats ---------------------------------\n",
    "\n",
    "def load_injury_data(\n",
    "    file_path: str | Path | None = None,\n",
    "    *,\n",
    "    base_dir: str | Path | None = None,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the historical injury CSV. By default we look inside the *new*\n",
    "    processed folder; pass ``file_path`` to override a specific file,\n",
    "    or ``base_dir`` to point at a different processed directory.\n",
    "    \"\"\"\n",
    "    root = Path(base_dir) if base_dir else DATA_PROCESSED_DIR\n",
    "    if file_path is None:\n",
    "        file_path = root / \"NBA Player Injury Stats(1951 - 2023).csv\"\n",
    "    file_path = Path(file_path).expanduser().resolve()\n",
    "\n",
    "    try:\n",
    "        injury = (\n",
    "            pd.read_csv(file_path)\n",
    "            .assign(Date=lambda d: pd.to_datetime(d[\"Date\"]))\n",
    "        )\n",
    "        injury[\"Season\"] = injury[\"Date\"].apply(\n",
    "            lambda x: (\n",
    "                f\"{x.year}-{str(x.year + 1)[-2:]}\"\n",
    "                if x.month >= 10\n",
    "                else f\"{x.year - 1}-{str(x.year)[-2:]}\"\n",
    "            )\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"[load_injury_data] loaded {len(injury):,} rows from {file_path}\")\n",
    "        return injury\n",
    "    except FileNotFoundError:\n",
    "        if debug:\n",
    "            print(f\"[load_injury_data] ✖ no injury file at {file_path}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage and testing of all functions\n",
    "    debug = True\n",
    "    start_season = 2022\n",
    "    end_season = 2023\n",
    "    sample_player = \"Ja Morant\"  # Example player\n",
    "\n",
    "    print(\"1. Testing scrape_salary_cap_history:\")\n",
    "    salary_cap_history = scrape_salary_cap_history(debug=debug)\n",
    "\n",
    "    print(\"\\n2. Testing scrape_player_salary_data:\")\n",
    "    player_salary_data = scrape_player_salary_data(start_season, end_season, player_filter=sample_player, debug=debug)\n",
    "\n",
    "    print(\"\\n3. Testing scrape_team_salary_data:\")\n",
    "    team_salary_data = scrape_team_salary_data(f\"{start_season}-{str(start_season+1)[-2:]}\", debug=debug)\n",
    "\n",
    "    print(\"\\n4. Testing scrape_advanced_metrics:\")\n",
    "    advanced_metrics = scrape_advanced_metrics(sample_player, f\"{start_season}-{str(start_season+1)[-2:]}\", debug=debug)\n",
    "    print(f\"Advanced Metrics for {sample_player}:\")\n",
    "    print(advanced_metrics)\n",
    "\n",
    "    print(\"\\n5. Testing load_injury_data and merge_injury_data:\")\n",
    "    injury_data = load_injury_data()\n",
    "    if injury_data is not None:\n",
    "        print(injury_data.head())\n",
    "    else:\n",
    "        print(\"No injury data loaded.\")\n",
    "    if not player_salary_data.empty and injury_data is not None:\n",
    "        from salary_nba_data_pull.process_utils import merge_injury_data\n",
    "        merged_data = merge_injury_data(player_salary_data, injury_data)\n",
    "        print(\"Merged data with injury info:\")\n",
    "        columns_to_display = ['Player', 'Season', 'Salary']\n",
    "        if 'Injured' in merged_data.columns:\n",
    "            columns_to_display.append('Injured')\n",
    "        if 'Injury_Periods' in merged_data.columns:\n",
    "            columns_to_display.append('Injury_Periods')\n",
    "        if 'Total_Days_Injured' in merged_data.columns:\n",
    "            columns_to_display.append('Total_Days_Injured')\n",
    "        if 'Injury_Risk' in merged_data.columns:\n",
    "            columns_to_display.append('Injury_Risk')\n",
    "        print(merged_data[columns_to_display].head())\n",
    "\n",
    "    if not player_salary_data.empty:\n",
    "        avg_salary = player_salary_data['Salary'].mean()\n",
    "        print(f\"Average salary for {sample_player} from {start_season} to {end_season}: ${avg_salary:,.2f}\")\n",
    "\n",
    "    if not team_salary_data.empty:\n",
    "        highest_team_salary = team_salary_data.loc[team_salary_data['Team_Salary'].idxmax()]\n",
    "        print(f\"Team with highest salary in {start_season}-{end_season}: {highest_team_salary['Team']} (${highest_team_salary['Team_Salary']:,.2f})\")\n",
    "\n",
    "    if not injury_data.empty:\n",
    "        injury_count = injury_data['Relinquished'].str.contains(sample_player, case=False).sum()\n",
    "        print(f\"Number of injuries/illnesses for {sample_player} from {start_season} to {end_season}: {injury_count}\")\n",
    "\n",
    "    print(\"\\nAll tests completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/name_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/name_utils.py\n",
    "\"\"\"\n",
    "Centralized name normalization utilities for salary_nba_data_pull.\n",
    "\n",
    "This module provides a single source of truth for player name normalization\n",
    "to ensure consistent matching between NBA API and Basketball Reference data.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Common suffix patterns to remove\n",
    "_SUFFIX_RE = re.compile(r\"\\b(jr|sr|ii|iii|iv|v)\\.?$\", flags=re.I)\n",
    "\n",
    "# Conservative mapping of dotted initials to compact forms\n",
    "_INITIAL_MAP = {\n",
    "    \"a.j.\": \"aj\", \"b.j.\": \"bj\", \"c.j.\": \"cj\", \"d.j.\": \"dj\", \"e.j.\": \"ej\",\n",
    "    \"g.g.\": \"gg\", \"j.j.\": \"jj\", \"k.j.\": \"kj\", \"p.j.\": \"pj\", \"r.j.\": \"rj\",\n",
    "    \"t.j.\": \"tj\", \"m.j.\": \"mj\", \"w.j.\": \"wj\",\n",
    "}\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a deterministic key from a player name.\n",
    "    \n",
    "    Process:\n",
    "    1. Trim and lowercase\n",
    "    2. Fold dotted initials (A.J. -> aj)\n",
    "    3. Remove common suffixes (jr/sr/ii/iii/iv/v)\n",
    "    4. Unicode NFKD normalization + remove combining marks\n",
    "    5. Keep only alphanumeric and spaces\n",
    "    6. Collapse multiple spaces\n",
    "    \n",
    "    Args:\n",
    "        name: Raw player name string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized key string for matching\n",
    "        \n",
    "    Examples:\n",
    "        >>> normalize_name(\"Luka Dončić\")\n",
    "        'luka doncic'\n",
    "        >>> normalize_name(\"A.J. Green Jr.\")\n",
    "        'aj green'\n",
    "        >>> normalize_name(\"Dennis Schröder\")\n",
    "        'dennis schroder'\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Basic cleanup\n",
    "    s = str(name).strip().lower()\n",
    "    \n",
    "    # Step 2: Fold dotted initials\n",
    "    for pattern, replacement in _INITIAL_MAP.items():\n",
    "        s = s.replace(pattern, replacement)\n",
    "    \n",
    "    # Step 3: Remove suffixes\n",
    "    s = _SUFFIX_RE.sub(\"\", s).strip()\n",
    "    \n",
    "    # Step 4: Remove obvious punctuation used on some sites\n",
    "    s = re.sub(r\"[(),]\", \" \", s)\n",
    "    \n",
    "    # Step 5: Unicode normalization (NFKD decomposes diacritics)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    # Remove combining marks (diacritics)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    \n",
    "    # Step 6: Keep only alphanumeric and spaces\n",
    "    s = re.sub(r\"[^0-9a-z\\s]\", \" \", s)\n",
    "    # Step 7: Collapse multiple spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "def validate_name_encoding(df: 'pd.DataFrame', season: str, debug: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that expected Unicode names appear correctly in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Player' column\n",
    "        season: Season string for logging\n",
    "        debug: Whether to print detailed diagnostics\n",
    "        \n",
    "    Returns:\n",
    "        True if all expected names are found, False otherwise\n",
    "        \n",
    "    Raises:\n",
    "        AssertionError: If critical encoding issues are detected\n",
    "    \"\"\"\n",
    "    if 'Player' not in df.columns:\n",
    "        if debug:\n",
    "            print(f\"[validate_name_encoding] {season}: No 'Player' column found\")\n",
    "        return False\n",
    "    \n",
    "    # Test cases with diacritics that should appear correctly\n",
    "    test_names = [\n",
    "        \"Luka Dončić\",\n",
    "        \"Nikola Jokić\", \n",
    "        \"Dennis Schröder\",\n",
    "        \"Bojan Bogdanović\"\n",
    "    ]\n",
    "    \n",
    "    found_names = []\n",
    "    missing_names = []\n",
    "    \n",
    "    for test_name in test_names:\n",
    "        if (df['Player'] == test_name).any():\n",
    "            found_names.append(test_name)\n",
    "        else:\n",
    "            missing_names.append(test_name)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[validate_name_encoding] {season}: Found {len(found_names)}/{len(test_names)} expected names\")\n",
    "        if missing_names:\n",
    "            print(f\"  Missing: {missing_names}\")\n",
    "            # Show some actual names for debugging\n",
    "            sample_names = df['Player'].dropna().head(10).tolist()\n",
    "            print(f\"  Sample actual names: {sample_names}\")\n",
    "    \n",
    "    # If we're missing critical names, this indicates encoding issues\n",
    "    if len(missing_names) > 2:  # Allow for some variation\n",
    "        raise AssertionError(\n",
    "            f\"[validate_name_encoding] {season}: Critical encoding issues detected. \"\n",
    "            f\"Missing {len(missing_names)} expected Unicode names: {missing_names}\"\n",
    "        )\n",
    "    \n",
    "    return len(missing_names) == 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/process_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/process_utils.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from salary_nba_data_pull.fetch_utils import fetch_all_players, fetch_career_stats, fetch_player_info, fetch_league_standings, fetch_season_players\n",
    "from salary_nba_data_pull.scrape_utils import scrape_advanced_metrics\n",
    "from salary_nba_data_pull.name_utils import normalize_name\n",
    "\n",
    "# --- CPI lazy‑loader --------------------------------------------------\n",
    "_CPI_AVAILABLE = False  # toggled at runtime\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _ensure_cpi_ready(debug: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Import `cpi` lazily and guarantee its internal SQLite DB is usable.\n",
    "    Returns True when inflation data are available, False otherwise.\n",
    "    \"\"\"\n",
    "    global _CPI_AVAILABLE\n",
    "    try:\n",
    "        import importlib\n",
    "        cpi = importlib.import_module(\"cpi\")        # late import\n",
    "        try:\n",
    "            _ = cpi.models.Series.get_by_id(\"0000\")  # 1‑row sanity query\n",
    "            _CPI_AVAILABLE = True\n",
    "            return True\n",
    "        except sqlite3.OperationalError:\n",
    "            if debug:\n",
    "                logging.warning(\"[CPI] DB invalid – rebuilding from BLS…\")\n",
    "            cpi.update(rebuild=True)                # expensive network call\n",
    "            _CPI_AVAILABLE = True\n",
    "            return True\n",
    "    except ModuleNotFoundError:\n",
    "        if debug:\n",
    "            logging.warning(\"[CPI] package not installed\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            logging.error(\"[CPI] unexpected CPI failure: %s\", e)\n",
    "    return False\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def inflate_value(value: float, year_str: str,\n",
    "                  *, debug: bool = False, skip_inflation: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Inflate `value` from the dollars of `year_str` (YYYY or YYYY‑YY) to 2022 USD.\n",
    "    If CPI data are unavailable or the user opts out, return the original value.\n",
    "    \"\"\"\n",
    "    if skip_inflation or not _ensure_cpi_ready(debug):\n",
    "        return value\n",
    "    try:\n",
    "        import cpi                                       # safe: DB ready\n",
    "        year = int(year_str[:4])\n",
    "        if year >= datetime.now().year:\n",
    "            return value\n",
    "        return float(cpi.inflate(value, year, to=2022))\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            logging.error(\"[CPI] inflate failed for %s: %s\", year_str, e)\n",
    "        return value\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def calculate_percentages(df, debug=False):\n",
    "    \"\"\"\n",
    "    Calculate shooting percentages and other derived statistics,\n",
    "    adding indicators and imputing zeros for zero‐denominator cases.\n",
    "    \n",
    "    Enhanced Features:\n",
    "    - Indicator columns (3PA_zero, FTA_zero) flag zero attempts\n",
    "    - Zero imputation for undefined percentages (no attempts → 0% success)\n",
    "    - Debug counts for zero-denominator cases\n",
    "    - ML-ready numeric dataset with preserved semantic meaning\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 1️⃣ Compute FG% (unchanged - no zero denominator issues)\n",
    "    if 'FGA' in df.columns and 'FG' in df.columns:\n",
    "        df['FG%'] = (df['FG'] / df['FGA'] * 100).round(2)\n",
    "        df['FG%'] = df['FG%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 2️⃣ Compute 3P% with debug, indicator, and zero fill\n",
    "    if '3PA' in df.columns and '3P' in df.columns:\n",
    "        # Debug: count zero-attempts\n",
    "        zero_3pa = (df['3PA'] == 0).sum()\n",
    "        if debug:\n",
    "            print(f\"[calculate_percentages] 3PA==0 count: {zero_3pa}\")\n",
    "        \n",
    "        # Indicator for zero attempts (preserves information)\n",
    "        df['3PA_zero'] = df['3PA'] == 0\n",
    "        \n",
    "        # Raw percentage calculation (NaN where 3PA==0)\n",
    "        df['3P%'] = (df['3P'] / df['3PA'] * 100).round(2)\n",
    "        df['3P%'] = df['3P%'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Impute zeros for undefined cases (no attempts → 0% success)\n",
    "        df.loc[df['3PA_zero'], '3P%'] = 0.0\n",
    "\n",
    "    # 3️⃣ Compute FT% with debug, indicator, and zero fill\n",
    "    if 'FTA' in df.columns and 'FT' in df.columns:\n",
    "        zero_fta = (df['FTA'] == 0).sum()\n",
    "        if debug:\n",
    "            print(f\"[calculate_percentages] FTA==0 count: {zero_fta}\")\n",
    "        \n",
    "        # Indicator for zero attempts\n",
    "        df['FTA_zero'] = df['FTA'] == 0\n",
    "        \n",
    "        # Raw percentage calculation (NaN where FTA==0)\n",
    "        df['FT%'] = (df['FT'] / df['FTA'] * 100).round(2)\n",
    "        df['FT%'] = df['FT%'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Impute zeros for undefined cases (no attempts → 0% success)\n",
    "        df.loc[df['FTA_zero'], 'FT%'] = 0.0\n",
    "\n",
    "    # 4️⃣ Calculate efficiency metrics (unchanged)\n",
    "    if 'PTS' in df.columns and 'FGA' in df.columns and 'FTA' in df.columns:\n",
    "        df['TS%'] = (df['PTS'] / (2 * (df['FGA'] + 0.44 * df['FTA'])) * 100).round(2)\n",
    "        df['TS%'] = df['TS%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 5️⃣ Per‐36 min rates (unchanged)\n",
    "    if 'PTS' in df.columns and 'MP' in df.columns:\n",
    "        df['PTS_per_36'] = (df['PTS'] / df['MP'] * 36).round(2)\n",
    "        df['PTS_per_36'] = df['PTS_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'AST' in df.columns and 'MP' in df.columns:\n",
    "        df['AST_per_36'] = (df['AST'] / df['MP'] * 36).round(2)\n",
    "        df['AST_per_36'] = df['AST_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'TRB' in df.columns and 'MP' in df.columns:\n",
    "        df['TRB_per_36'] = (df['TRB'] / df['MP'] * 36).round(2)\n",
    "        df['TRB_per_36'] = df['TRB_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Percentage calculations completed with zero-denominator handling\")\n",
    "    return df\n",
    "\n",
    "def process_player_data(player_name: str, season: str,\n",
    "                        all_players: dict[str, dict], *,\n",
    "                        debug: bool = False) -> dict | None:\n",
    "    \"\"\"\n",
    "    Build a single‑player dict for a given season with a concrete Team/TeamID.\n",
    "    For traded players:\n",
    "      • Prefer a non‑TOT row for that season.\n",
    "      • Pick the row with max GP (tie‑break by MIN).\n",
    "    This avoids ambiguous team context that breaks W/L joins.\n",
    "\n",
    "    Returns:\n",
    "        dict with uppercased display names and PlayerID included.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    meta = all_players.get(player_name.lower().strip())\n",
    "    if not meta:\n",
    "        return None\n",
    "\n",
    "    pid = meta[\"player_id\"]\n",
    "    info_df   = fetch_player_info(pid, debug=debug)\n",
    "    career_df = fetch_career_stats(pid, debug=debug)\n",
    "    if career_df is None or career_df.empty:\n",
    "        return None\n",
    "\n",
    "    # rows for the requested season (may include multiple teams + a total row)\n",
    "    srows = career_df.loc[career_df.SEASON_ID.eq(season)].copy()\n",
    "    if srows.empty:\n",
    "        return None\n",
    "\n",
    "    # Prefer concrete team rows over season \"TOT/2TM/3TM\" rows\n",
    "    def _is_total_label(x: str) -> bool:\n",
    "        x = str(x).upper()\n",
    "        return x in {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}  # BBR uses 2TM/3TM; NBA may have \"TOT\"\n",
    "    non_tot = srows[~srows[\"TEAM_ABBREVIATION\"].map(_is_total_label)]\n",
    "\n",
    "    pick_from = non_tot if not non_tot.empty else srows\n",
    "    # pick the most representative stint: max GP, then MIN\n",
    "    season_row = (pick_from.sort_values([\"GP\", \"MIN\"], ascending=False)\n",
    "                           .iloc[0])\n",
    "\n",
    "    # Build the record; enforce uppercase for display names\n",
    "    data = {\n",
    "        \"Player\": player_name.upper(),\n",
    "        \"Season\": season,\n",
    "        \"Team\":   str(season_row[\"TEAM_ABBREVIATION\"]).upper(),\n",
    "        \"Age\":    season_row[\"PLAYER_AGE\"],\n",
    "        \"GP\":     season_row[\"GP\"],\n",
    "        \"GS\":     season_row.get(\"GS\", 0),\n",
    "        \"MP\":     season_row[\"MIN\"],\n",
    "\n",
    "        \"PTS\": season_row[\"PTS\"],\n",
    "        \"FG\":  season_row[\"FGM\"],  \"FGA\": season_row[\"FGA\"],\n",
    "        \"3P\":  season_row[\"FG3M\"], \"3PA\": season_row[\"FG3A\"],\n",
    "        \"FT\":  season_row[\"FTM\"],  \"FTA\": season_row[\"FTA\"],\n",
    "\n",
    "        \"TRB\": season_row[\"REB\"], \"AST\": season_row[\"AST\"],\n",
    "        \"STL\": season_row[\"STL\"], \"BLK\": season_row[\"BLK\"],\n",
    "        \"TOV\": season_row[\"TOV\"], \"PF\":  season_row[\"PF\"],\n",
    "\n",
    "        \"ORB\": season_row.get(\"OREB\", np.nan),\n",
    "        \"DRB\": season_row.get(\"DREB\", np.nan),\n",
    "    }\n",
    "\n",
    "    # Include the PlayerID explicitly\n",
    "    data[\"PlayerID\"] = pid\n",
    "\n",
    "    # TeamID from the chosen season row whenever possible\n",
    "    data[\"TeamID\"] = season_row.get(\"TEAM_ID\", np.nan)\n",
    "\n",
    "    # roster meta (position, experience)\n",
    "    if info_df is not None and not info_df.empty:\n",
    "        ir = info_df.iloc[0]\n",
    "        data[\"Position\"]          = ir.get(\"POSITION\", \"\")\n",
    "        data[\"Years_of_Service\"]  = ir.get(\"SEASON_EXP\", None)\n",
    "\n",
    "    # Derived splits (leave denominator=0 as NaN, do not fill)\n",
    "    two_att     = data[\"FGA\"] - data[\"3PA\"]\n",
    "    data[\"2P\"]  = data[\"FG\"] - data[\"3P\"]\n",
    "    data[\"2PA\"] = two_att\n",
    "    data[\"eFG%\"] = round((data[\"FG\"] + 0.5 * data[\"3P\"]) / data[\"FGA\"] * 100, 2) if data[\"FGA\"] else np.nan\n",
    "    data[\"2P%\"]  = round(data[\"2P\"] / two_att * 100, 2)                           if two_att else np.nan\n",
    "\n",
    "    return data\n",
    "\n",
    "def merge_injury_data(player_data: pd.DataFrame,\n",
    "                      injury_data: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach four injury‑related columns. If a player has no injuries, leave the fields as NA\n",
    "    (pd.NA) instead of empty strings so repeated runs compare equal.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    if player_data.empty:\n",
    "        return player_data\n",
    "\n",
    "    out = player_data.copy()\n",
    "\n",
    "    # Ensure columns exist with NA defaults\n",
    "    defaults = {\n",
    "        \"Injured\": False,\n",
    "        \"Injury_Periods\": pd.NA,\n",
    "        \"Total_Days_Injured\": 0,\n",
    "        \"Injury_Risk\": \"Low Risk\",\n",
    "    }\n",
    "    for c, v in defaults.items():\n",
    "        if c not in out.columns:\n",
    "            out[c] = v\n",
    "\n",
    "    if injury_data is None or injury_data.empty:\n",
    "        # normalize empties just in case\n",
    "        out[\"Injury_Periods\"] = out[\"Injury_Periods\"].replace(\"\", pd.NA)\n",
    "        return out\n",
    "\n",
    "    # Process each player/season\n",
    "    for idx, row in out.iterrows():\n",
    "        pname = row[\"Player\"]\n",
    "        season = row[\"Season\"]\n",
    "\n",
    "        mask = (injury_data[\"Season\"] == season) & \\\n",
    "               (injury_data[\"Relinquished\"].str.contains(pname, case=False, na=False))\n",
    "        player_inj = injury_data.loc[mask]\n",
    "\n",
    "        if player_inj.empty:\n",
    "            continue  # keep defaults\n",
    "\n",
    "        periods = []\n",
    "        total_days = 0\n",
    "        for _, inj in player_inj.iterrows():\n",
    "            start = inj[\"Date\"]\n",
    "            # find the first acquired record after start\n",
    "            got_back = injury_data[\n",
    "                (injury_data[\"Date\"] > start) &\n",
    "                (injury_data[\"Acquired\"].str.contains(pname, case=False, na=False))\n",
    "            ]\n",
    "            if not got_back.empty:\n",
    "                end = got_back.iloc[0][\"Date\"]\n",
    "            else:\n",
    "                end_year = int(season.split(\"-\")[1])\n",
    "                end = pd.Timestamp(f\"{end_year}-06-30\")\n",
    "\n",
    "            total_days += (end - start).days\n",
    "            periods.append(f\"{start:%Y-%m-%d} - {end:%Y-%m-%d}\")\n",
    "\n",
    "        out.at[idx, \"Injured\"] = True\n",
    "        out.at[idx, \"Injury_Periods\"] = \"; \".join(periods) if periods else pd.NA\n",
    "        out.at[idx, \"Total_Days_Injured\"] = total_days\n",
    "\n",
    "        if total_days < 10:\n",
    "            risk = \"Low Risk\"\n",
    "        elif total_days <= 20:\n",
    "            risk = \"Moderate Risk\"\n",
    "        else:\n",
    "            risk = \"High Risk\"\n",
    "        out.at[idx, \"Injury_Risk\"] = risk\n",
    "\n",
    "    # final normalization\n",
    "    out[\"Injury_Periods\"] = out[\"Injury_Periods\"].replace(\"\", pd.NA)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# USAGE / LOAD METRICS\n",
    "# Inspired by Basketball-Reference (USG%), Nylon Calculus (True Usage parts),\n",
    "# and Thinking Basketball (Offensive Load). See docs in code.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "USAGE_COMPONENT_COLS = [\n",
    "    \"USG%\",               # already scraped but we may recompute if missing\n",
    "    \"Scoring_Usage%\",     # (FGA + 0.44*FTA) share of team poss\n",
    "    \"Playmaking_Usage%\",  # (AST-created FG poss) share\n",
    "    \"Turnover_Usage%\",    # TOV share\n",
    "    \"True_Usage%\",        # Scoring + Playmaking + TO\n",
    "    \"Offensive_Load%\",    # Thinking Basketball style\n",
    "    \"Player_Poss\",        # est. possessions used by player\n",
    "    \"Team_Poss\",          # est. team possessions (for join/QA)\n",
    "]\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return np.where(b == 0, np.nan, a / b)\n",
    "\n",
    "def add_usage_components(df: pd.DataFrame, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute Scoring‑/Playmaking‑/Turnover usage plus Offensive‑Load.\n",
    "\n",
    "    The helper now:\n",
    "      • Renames OREB/DREB → ORB/DRB if needed.\n",
    "      • Warns – but does not crash – when expected stats are missing.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # ── 0. Normalise column spelling ───────────────────────────────\n",
    "    col_map = {\"OREB\": \"ORB\", \"DREB\": \"DRB\"}\n",
    "    out.rename(columns={k: v for k, v in col_map.items() if k in out.columns}, inplace=True)\n",
    "\n",
    "    # ── 1. Summarise team totals ───────────────────────────────────\n",
    "    want = [\"FGA\", \"FTA\", \"TOV\", \"FG\", \"ORB\", \"DRB\", \"TRB\", \"MP\", \"AST\"]\n",
    "    have = [c for c in want if c in out.columns]\n",
    "    if len(have) < len(want) and debug:\n",
    "        print(f\"[usage] missing cols → {sorted(set(want) - set(have))}\")\n",
    "\n",
    "    grp = out.groupby([\"Season\", \"Team\"], dropna=False)\n",
    "    team_totals = grp[have].sum(min_count=1).rename(columns=lambda c: f\"Tm_{c}\")\n",
    "    out = out.merge(team_totals, left_on=[\"Season\", \"Team\"], right_index=True, how=\"left\")\n",
    "\n",
    "    # ── 2. Possession estimates ────────────────────────────────────\n",
    "    out[\"Team_Poss\"]   = out[\"Tm_FGA\"] + 0.44 * out[\"Tm_FTA\"] + out[\"Tm_TOV\"]\n",
    "    out[\"Player_Poss\"] = out[\"FGA\"]    + 0.44 * out[\"FTA\"]    + out[\"TOV\"]\n",
    "\n",
    "    share = out[\"Player_Poss\"] / out[\"Team_Poss\"]\n",
    "\n",
    "    # fill USG% if missing\n",
    "    if \"USG%\" not in out.columns or out[\"USG%\"].isna().all():\n",
    "        out[\"USG%\"] = (share * 100).round(2)\n",
    "\n",
    "    scor = out[\"FGA\"] + 0.44 * out[\"FTA\"]\n",
    "    tov  = out[\"TOV\"]\n",
    "    ast_cre = 0.37 * out[\"AST\"]\n",
    "\n",
    "    out[\"Scoring_Usage%\"]     = (scor / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"Turnover_Usage%\"]    = (tov  / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"Playmaking_Usage%\"]  = (ast_cre / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"True_Usage%\"]        = (out[\"Scoring_Usage%\"] + out[\"Turnover_Usage%\"] +\n",
    "                                 out[\"Playmaking_Usage%\"]).round(2)\n",
    "\n",
    "    tsa       = scor\n",
    "    creation  = 0.8 * out[\"AST\"]\n",
    "    non_cre   = 0.2 * out[\"AST\"]\n",
    "    out[\"Offensive_Load%\"] = ((tsa + creation + non_cre + tov) / out[\"Team_Poss\"] * 100).round(2)\n",
    "\n",
    "    return out\n",
    "\n",
    "# --- NEW: Advanced metrics audit helper ---\n",
    "def report_advanced_mismatches(player_names: list[str], season: str, *, topk: int = 3):\n",
    "    \"\"\"\n",
    "    Prints players we couldn't match in the BBR advanced table and closest suggestions.\n",
    "    No filling - just diagnostics.\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "    from salary_nba_data_pull.scrape_utils import _season_advanced_df\n",
    "    \n",
    "    # Prefer the shared normalizer from nba_utils; fall back to local\n",
    "    try:\n",
    "        from api.src.airflow_project.utils.nba_utils import normalize_name as _norm\n",
    "    except Exception:\n",
    "        from salary_nba_data_pull.scrape_utils import _normalise_name as _norm\n",
    "\n",
    "    df = _season_advanced_df(season)\n",
    "    # Ensure the season table uses the same normalizer\n",
    "    if \"player_key\" not in df.columns or df[\"player_key\"].isna().all():\n",
    "        df = df.copy()\n",
    "        df[\"player_key\"] = df[\"Player\"].map(_norm)\n",
    "\n",
    "    keys = set(df[\"player_key\"].dropna())\n",
    "    all_keys = list(keys)\n",
    "    misses = []\n",
    "    \n",
    "    for raw in player_names:\n",
    "        q = _norm(raw)\n",
    "        if q not in keys:\n",
    "            suggestions = difflib.get_close_matches(q, all_keys, n=topk, cutoff=0.75)\n",
    "            print(f\"[adv-miss] {raw}  → key='{q}'  suggestions={suggestions}\")\n",
    "            misses.append(raw)\n",
    "    \n",
    "    print(f\"[adv-miss] total misses: {len(misses)}/{len(player_names)}\")\n",
    "    return misses\n",
    "\n",
    "def attach_wins_losses_using_logs(df_season: pd.DataFrame,\n",
    "                                  season: str,\n",
    "                                  logs_wl: pd.DataFrame,\n",
    "                                  *,\n",
    "                                  debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-merge W/L by TeamID using precomputed team-game-log totals.\n",
    "    No filling. If TeamID is missing or ambiguous (e.g., TOT), W/L stays NaN.\n",
    "    \"\"\"\n",
    "    if df_season.empty or logs_wl.empty:\n",
    "        return df_season\n",
    "\n",
    "    out = df_season.merge(\n",
    "        logs_wl.drop_duplicates(\"TeamID\"),\n",
    "        on=\"TeamID\", how=\"left\", validate=\"m:1\"\n",
    "    )\n",
    "    if debug:\n",
    "        null_rate = out[\"Wins\"].isna().mean() * 100\n",
    "        print(f\"[attach_wins_losses_using_logs] {season} W/L null% = {null_rate:.2f}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def diagnose_wl_nulls(df_after_merge: pd.DataFrame,\n",
    "                      season: str,\n",
    "                      *,\n",
    "                      debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attribute W/L nulls to concrete reasons:\n",
    "      - TeamID missing\n",
    "      - team label equals 'TOT' for multi-team season rows\n",
    "      - TeamID present but no match in W/L lookup\n",
    "      - Player has 0 GP (edge case)\n",
    "    Returns a small DataFrame with reason counts/samples.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    if df_after_merge.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mask_null = df_after_merge[\"Wins\"].isna()\n",
    "    sub = df_after_merge.loc[mask_null].copy()\n",
    "\n",
    "    reasons = []\n",
    "    if \"Team\" in sub.columns:\n",
    "        reasons.append((\"TOT team label\", sub[\"Team\"].str.upper().eq(\"TOT\")))\n",
    "    reasons.append((\"TeamID missing\", sub[\"TeamID\"].isna()))\n",
    "    reasons.append((\"Zero GP\", sub.get(\"GP\", pd.Series(index=sub.index)).fillna(0).eq(0)))\n",
    "    # anything else falls into \"No W/L match for TeamID\"\n",
    "    base_mask = pd.Series(False, index=sub.index)\n",
    "    for _, m in reasons:\n",
    "        base_mask |= m.fillna(False)\n",
    "    reasons.append((\"No W/L match for TeamID\", ~base_mask))\n",
    "\n",
    "    rows = []\n",
    "    for label, m in reasons:\n",
    "        cnt = int(m.sum())\n",
    "        ex = sub.loc[m, [\"Player\",\"Team\",\"TeamID\"]].head(5).to_dict(\"records\") if cnt else []\n",
    "        rows.append({\"season\": season, \"reason\": label, \"count\": cnt, \"examples\": ex})\n",
    "\n",
    "    diag = pd.DataFrame(rows).sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "    if debug:\n",
    "        print(\"[diagnose_wl_nulls]\")\n",
    "        print(diag)\n",
    "    return diag\n",
    "\n",
    "\n",
    "def diagnose_injury_nulls(df: pd.DataFrame,\n",
    "                          injury_df: pd.DataFrame | None,\n",
    "                          *,\n",
    "                          debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Break down Injury_Periods nulls by season:\n",
    "      - season beyond injury source coverage\n",
    "      - player has no injury rows in covered season (legit NA)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    by_season = (df.groupby(\"Season\")\n",
    "                   .agg(total=(\"Player\",\"count\"),\n",
    "                        nulls=(\"Injury_Periods\", lambda s: int(s.isna().sum())))\n",
    "                   .assign(null_pct=lambda d: 100*d[\"nulls\"]/d[\"total\"])\n",
    "                   .reset_index())\n",
    "\n",
    "    if injury_df is not None and not injury_df.empty:\n",
    "        covered = set(injury_df[\"Season\"].dropna().unique())\n",
    "        by_season[\"in_coverage\"] = by_season[\"Season\"].isin(covered)\n",
    "    else:\n",
    "        by_season[\"in_coverage\"] = False\n",
    "\n",
    "    if debug:\n",
    "        print(\"[diagnose_injury_nulls]\")\n",
    "        print(by_season)\n",
    "    return by_season\n",
    "\n",
    "\n",
    "def audit_min_date_alignment(source_map: dict[str, tuple[pd.DataFrame, list[str]]],\n",
    "                             *,\n",
    "                             debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each source, compute the earliest season where *all listed columns*\n",
    "    are non‑NA for at least one row.\n",
    "    `source_map[name] = (df, [\"colA\",\"colB\",...])`\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    rows = []\n",
    "    for name, (df, cols) in source_map.items():\n",
    "        if df is None or df.empty:\n",
    "            rows.append({\"source\": name, \"min_non_na_season\": None, \"cols\": cols})\n",
    "            continue\n",
    "        # seasons with any non-NA across the requested columns\n",
    "        ok = (df[cols].notna().any(axis=1))\n",
    "        seasons = pd.Series(df[\"Season\"][ok].dropna().unique())\n",
    "        min_seas = seasons.sort_values().iloc[0] if not seasons.empty else None\n",
    "        rows.append({\"source\": name, \"min_non_na_season\": min_seas, \"cols\": cols})\n",
    "    rep = pd.DataFrame(rows)\n",
    "    if debug:\n",
    "        print(\"[audit_min_date_alignment]\")\n",
    "        print(rep)\n",
    "    return rep\n",
    "\n",
    "\n",
    "def enhanced_normalize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced name normalization that handles common edge cases.\n",
    "    \n",
    "    Handles:\n",
    "    - \"Jr.\", \"Sr.\", \"II\", \"III\", \"IV\" suffixes\n",
    "    - \"A.J.\" vs \"AJ\" abbreviations\n",
    "    - \"G.G.\" vs \"GG\" abbreviations\n",
    "    - Accented characters and special characters\n",
    "    - Extra spaces and punctuation\n",
    "    \"\"\"\n",
    "    if not name or pd.isna(name):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    name = str(name).lower().strip()\n",
    "    \n",
    "    # Handle common abbreviations\n",
    "    name = name.replace(\"a.j.\", \"aj\")\n",
    "    name = name.replace(\"g.g.\", \"gg\")\n",
    "    name = name.replace(\"j.j.\", \"jj\")\n",
    "    name = name.replace(\"t.j.\", \"tj\")\n",
    "    name = name.replace(\"d.j.\", \"dj\")\n",
    "    name = name.replace(\"k.j.\", \"kj\")\n",
    "    name = name.replace(\"m.j.\", \"mj\")\n",
    "    name = name.replace(\"p.j.\", \"pj\")\n",
    "    name = name.replace(\"r.j.\", \"rj\")\n",
    "    name = name.replace(\"s.j.\", \"sj\")\n",
    "    name = name.replace(\"v.j.\", \"vj\")\n",
    "    name = name.replace(\"w.j.\", \"wj\")\n",
    "    \n",
    "    # Remove common suffixes that cause matching issues\n",
    "    suffixes_to_remove = [\" jr.\", \" sr.\", \" ii\", \" iii\", \" iv\", \" v\"]\n",
    "    for suffix in suffixes_to_remove:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[:-len(suffix)]\n",
    "            break\n",
    "    \n",
    "    # Handle accented characters\n",
    "    import unicodedata\n",
    "    name = unicodedata.normalize('NFD', name)\n",
    "    name = ''.join(c for c in name if not unicodedata.combining(c))\n",
    "    \n",
    "    # Remove extra spaces and punctuation\n",
    "    import re\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation except spaces\n",
    "    name = re.sub(r'\\s+', ' ', name)     # Normalize spaces\n",
    "    name = name.strip()\n",
    "    \n",
    "    return name\n",
    "\n",
    "def diagnose_advanced_nulls(df: pd.DataFrame,\n",
    "                            season: str,\n",
    "                            *,\n",
    "                            max_print: int = 5,\n",
    "                            tail_print: int = 0,\n",
    "                            show_all: bool = False,\n",
    "                            debug: bool = True,\n",
    "                            prefer_left: set[str] = {\"TS%\", \"USG%\"},\n",
    "                           ) -> dict:\n",
    "    \"\"\"\n",
    "    Summarise advanced-stat availability for a season based on *BBR-only* columns.\n",
    "    We exclude columns we intentionally keep from the left (e.g., TS%, USG%)\n",
    "    so that unmatched BBR rows are not masked.\n",
    "\n",
    "    Parameters:\n",
    "        max_print: int - how many head samples to show (default: 5)\n",
    "        tail_print: int - how many tail samples to show (default: 0)\n",
    "        show_all: bool - if True, return and print ALL unmatched players\n",
    "        debug: bool - whether to print diagnostics\n",
    "        prefer_left: set - columns to prefer from left side (TS%, USG%)\n",
    "\n",
    "    Returns a dict with:\n",
    "      - season\n",
    "      - players\n",
    "      - adv_all_na_rows: count of rows where *all attached-from-BBR* cols are NA\n",
    "      - per_col_nulls: per-column NA counts for those BBR-only cols\n",
    "      - sample_head: first max_print unmatched players\n",
    "      - sample_tail: last tail_print unmatched players\n",
    "      - sample_all: all unmatched players if show_all=True\n",
    "\n",
    "    No filling, no mutation.\n",
    "    \"\"\"\n",
    "    from salary_nba_data_pull.scrape_utils import ADV_METRIC_COLS\n",
    "\n",
    "    if df.empty:\n",
    "        out = {\n",
    "            \"season\": season,\n",
    "            \"players\": 0,\n",
    "            \"adv_all_na_rows\": 0,\n",
    "            \"per_col_nulls\": {},\n",
    "            \"sample_head\": [],\n",
    "            \"sample_tail\": [],\n",
    "            \"sample_all\": []\n",
    "        }\n",
    "        if debug: print(\"[diagnose_advanced_nulls]\", out)\n",
    "        return out\n",
    "\n",
    "    # Focus only on columns we expect to be attached from BBR\n",
    "    bbr_only = [c for c in ADV_METRIC_COLS if c in df.columns and c not in prefer_left]\n",
    "    if not bbr_only:\n",
    "        out = {\n",
    "            \"season\": season,\n",
    "            \"players\": len(df),\n",
    "            \"adv_all_na_rows\": 0,\n",
    "            \"per_col_nulls\": {},\n",
    "            \"sample_head\": [],\n",
    "            \"sample_tail\": [],\n",
    "            \"sample_all\": []\n",
    "        }\n",
    "        if debug:\n",
    "            print(f\"[diagnose_advanced_nulls] {season}: no BBR-only adv cols present\")\n",
    "        return out\n",
    "\n",
    "    # Mask rows where *all* BBR-only cols are NA\n",
    "    all_na_mask = df[bbr_only].isna().all(axis=1)\n",
    "    adv_all_na_rows = int(all_na_mask.sum())\n",
    "\n",
    "    # Extract the unmatched Player names\n",
    "    unmatched_players = df.loc[all_na_mask, \"Player\"]\n",
    "    sample_head = unmatched_players.head(max_print).tolist()\n",
    "    sample_tail = unmatched_players.tail(tail_print).tolist() if tail_print > 0 else []\n",
    "    sample_all = unmatched_players.tolist() if show_all else []\n",
    "\n",
    "    # Per-column null counts\n",
    "    per_col_nulls = df[bbr_only].isna().sum().sort_values(ascending=False).to_dict()\n",
    "\n",
    "    out = {\n",
    "        \"season\": season,\n",
    "        \"players\": len(df),\n",
    "        \"adv_all_na_rows\": adv_all_na_rows,\n",
    "        \"per_col_nulls\": per_col_nulls,\n",
    "        \"sample_head\": sample_head,\n",
    "        \"sample_tail\": sample_tail,\n",
    "        \"sample_all\": sample_all,\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[diagnose_advanced_nulls] {season}: rows with ALL BBR-only adv cols NA = \"\n",
    "              f\"{adv_all_na_rows}/{len(df)}\")\n",
    "        if adv_all_na_rows:\n",
    "            print(f\"  head sample ({max_print}): {sample_head}\")\n",
    "            if tail_print > 0:\n",
    "                print(f\"  tail sample ({tail_print}): {sample_tail}\")\n",
    "            if show_all:\n",
    "                print(f\"  all unmatched ({len(sample_all)}): {sample_all}\")\n",
    "        print(f\"  per-col nulls (BBR-only): {per_col_nulls}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def attach_wins_losses(df_season: pd.DataFrame,\n",
    "                       season: str,\n",
    "                       *,\n",
    "                       debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-merge W/L using unified lookup (game logs + standings).\n",
    "    Emits a reason breakdown for any residual nulls.\n",
    "    \"\"\"\n",
    "    if df_season.empty:\n",
    "        return df_season\n",
    "\n",
    "    from salary_nba_data_pull.fetch_utils import fetch_team_wl_lookup\n",
    "    wl = fetch_team_wl_lookup(season, season_type=\"Regular Season\", debug=debug)\n",
    "\n",
    "    out = df_season.merge(wl.drop_duplicates(\"TeamID\"),\n",
    "                          on=\"TeamID\", how=\"left\", validate=\"m:1\")\n",
    "    if debug:\n",
    "        null_rate = out[\"Wins\"].isna().mean() * 100\n",
    "        print(f\"[attach_wins_losses] {season} W/L null% = {null_rate:.2f}\")\n",
    "        if null_rate > 0:\n",
    "            _ = diagnose_wl_nulls(out, season, debug=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  Consolidate _x / _y duplicate columns\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _choose_preferred_column(df: pd.DataFrame, col_a: str, col_b: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the column name (col_a or col_b) that should survive a\n",
    "    consolidate-duplicates decision.\n",
    "\n",
    "    Heuristic:\n",
    "    1. Keep the one with *fewer NaNs*.\n",
    "    2. If tied, keep the one that is *not all-NaN*.\n",
    "    3. If still tied (both fully populated or both empty), keep the one that\n",
    "       comes first alphabetically (stable with previous behaviour).\n",
    "\n",
    "    The caller is responsible for ensuring both columns exist in *df*.\n",
    "    \"\"\"\n",
    "    na_a = df[col_a].isna().sum()\n",
    "    na_b = df[col_b].isna().sum()\n",
    "\n",
    "    if na_a < na_b:\n",
    "        return col_a\n",
    "    if na_b < na_a:\n",
    "        return col_b\n",
    "\n",
    "    # tie – favour the column that isn't entirely NaN\n",
    "    if df[col_a].notna().any() and df[col_b].isna().all():\n",
    "        return col_a\n",
    "    if df[col_b].notna().any() and df[col_a].isna().all():\n",
    "        return col_b\n",
    "\n",
    "    # final tie-break – deterministic old rule\n",
    "    return min(col_a, col_b)\n",
    "\n",
    "\n",
    "\n",
    "def consolidate_duplicate_columns(df: pd.DataFrame,\n",
    "                                  *,\n",
    "                                  preferred: str | None = None,\n",
    "                                  debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collapse *_x / *_y duplicates **and** guarantee no duplicate labels remain.\n",
    "    Additionally, assert that every column in CRITICAL_ID_COLS still exists.\n",
    "    \"\"\"\n",
    "    from salary_nba_data_pull.data_utils import CRITICAL_ID_COLS\n",
    "    from salary_nba_data_pull.main import _almost_equal_numeric\n",
    "    \n",
    "    out = df.copy()\n",
    "    suff_pairs: dict[str, list[str]] = {}\n",
    "\n",
    "    # ── Phase 1 – detect duplicate suffix pairs ───────────────────────────\n",
    "    for col in out.columns:\n",
    "        if col.endswith((\"_x\", \"_y\")):\n",
    "            base = col[:-2]\n",
    "            suff_pairs.setdefault(base, []).append(col)\n",
    "\n",
    "    # ── Phase 2 – resolve each pair/group ────────────────────────────────\n",
    "    for base, cols in suff_pairs.items():\n",
    "        cols = sorted(cols)                                  # deterministic\n",
    "        winner: str\n",
    "\n",
    "        if len(cols) == 1:                                   # only _x **or** _y\n",
    "            winner = cols[0]\n",
    "        else:                                                # both present\n",
    "            if preferred in {\"_x\", \"_y\"}:                    # explicit hint\n",
    "                pref_col = f\"{base}{preferred}\"\n",
    "                if pref_col in cols:\n",
    "                    winner = pref_col\n",
    "                else:\n",
    "                    winner = _choose_preferred_column(out, *cols)\n",
    "            else:\n",
    "                winner = _choose_preferred_column(out, *cols)\n",
    "\n",
    "            # Show mismatches that matter\n",
    "            other = [c for c in cols if c != winner][0]\n",
    "            unequal = ~_almost_equal_numeric(out[winner], out[other])\n",
    "            if debug and unequal.any():\n",
    "                nbad = int(unequal.sum())\n",
    "                print(f\"[consolidate] '{base}': kept '{winner}' \"\n",
    "                      f\"(better NaN profile) – {nbad}/{len(out)} rows differed\")\n",
    "\n",
    "        # finally: rename winner → base, drop the rest\n",
    "        out.rename(columns={winner: base}, inplace=True)\n",
    "        drop_cols = [c for c in cols if c != winner]\n",
    "        out.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # ── Phase 3 – guarantee column-label uniqueness ──────────────────────\n",
    "    dup_labels = out.columns[out.columns.duplicated()].unique()\n",
    "    if dup_labels.size:\n",
    "        if debug:\n",
    "            print(f\"[consolidate] WARNING: removing duplicate labels \"\n",
    "                  f\"{dup_labels.tolist()}\")\n",
    "        out = out.loc[:, ~out.columns.duplicated()]\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 🔒 Sanity – critical IDs must survive\n",
    "    # -------------------------------------------------------------\n",
    "    missing = [c for c in CRITICAL_ID_COLS if c not in out.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(\n",
    "            f\"[consolidate_duplicate_columns] lost critical columns: {missing}\"\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_advanced_metrics(df_season: pd.DataFrame,\n",
    "                           season: str,\n",
    "                           *,\n",
    "                           debug: bool = False,\n",
    "                           name_overrides: dict[str, str] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach Basketball-Reference advanced metrics for one season.\n",
    "\n",
    "    • Keeps caller’s TS% / USG% values (NBA API derived) **without creating\n",
    "      duplicate *_x / *_y columns**.                    ← CHG\n",
    "    • Drops BBR “Team”, “MP”, “TS%”, “USG%” **before** merging. ← NEW\n",
    "    • Emits match diagnostics but never fills data.\n",
    "    \"\"\"\n",
    "    if df_season.empty:\n",
    "        return df_season\n",
    "\n",
    "    from difflib import get_close_matches\n",
    "    from salary_nba_data_pull.scrape_utils import _season_advanced_df, ADV_METRIC_COLS\n",
    "    from salary_nba_data_pull.name_utils     import normalize_name\n",
    "\n",
    "    adv = _season_advanced_df(season)\n",
    "    if adv is None or adv.empty:\n",
    "        if debug:\n",
    "            print(f\"[merge_advanced_metrics] no advanced table for {season}\")\n",
    "        return df_season\n",
    "\n",
    "    # ── Build player keys ─────────────────────────────────────────────\n",
    "    adv = adv.copy()\n",
    "    adv[\"player_key\"] = adv[\"Player\"].map(normalize_name)\n",
    "\n",
    "    df  = df_season.copy()\n",
    "    df[\"player_key\"] = df[\"Player\"].map(normalize_name)\n",
    "\n",
    "    # ── Decide which columns to attach ───────────────────────────────\n",
    "    prefer_left = {\"TS%\", \"USG%\"}                 # we already calculated these\n",
    "    adv_cols_available = [c for c in ADV_METRIC_COLS if c in adv.columns]\n",
    "    attach_cols = [c for c in adv_cols_available if c not in prefer_left]\n",
    "\n",
    "    # ── Prepare a one-row-per-key BBR slice ──────────────────────────\n",
    "    team_col = next((c for c in [\"Team\", \"Tm\", \"TEAM\"] if c in adv.columns), None)\n",
    "\n",
    "    pick_cols = [\"player_key\"] + attach_cols\n",
    "    if \"MP\" in adv.columns:               # needed only for sorting, drop later\n",
    "        pick_cols.append(\"MP\")\n",
    "    if team_col:\n",
    "        pick_cols.append(team_col)\n",
    "\n",
    "    adv_small = adv.loc[:, pick_cols].copy()\n",
    "\n",
    "    # Pick 'TOT' first (if present), then max MP\n",
    "    def _is_tot(x: str) -> bool:\n",
    "        return str(x).upper() in {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}\n",
    "\n",
    "    sort_cols, ascending = [\"player_key\"], [True]\n",
    "    if team_col:\n",
    "        adv_small[\"_is_tot\"] = adv_small[team_col].map(_is_tot).astype(int)\n",
    "        sort_cols += [\"_is_tot\"]; ascending += [False]\n",
    "    if \"MP\" in adv_small.columns:\n",
    "        sort_cols += [\"MP\"]; ascending += [False]\n",
    "\n",
    "    adv_small = adv_small.sort_values(sort_cols, ascending=ascending)\\\n",
    "                         .drop_duplicates(\"player_key\", keep=\"first\")\n",
    "\n",
    "    # Drop helper cols so they can't collide in the merge            ← NEW\n",
    "    adv_small = adv_small.drop(columns=[c for c in [\"MP\", team_col, \"_is_tot\"] if c in adv_small.columns])\n",
    "\n",
    "    # ── Merge (no suffixes needed now) ───────────────────────────────\n",
    "    merged = df.merge(adv_small, on=\"player_key\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "    # ── Diagnostics ─────────────────────────────────────────────────\n",
    "    if debug:\n",
    "        added_cols = [c for c in attach_cols if c in merged.columns]\n",
    "        print(f\"[merge_advanced_metrics] {season}: attached {len(added_cols)} cols – {added_cols[:10]}…\")\n",
    "\n",
    "        # Success rate\n",
    "        if attach_cols:\n",
    "            all_na = merged[attach_cols].isna().all(axis=1)\n",
    "            matched = (~all_na).sum()\n",
    "            print(f\"[merge_advanced_metrics] {season}: matched {matched}/{len(merged)} \"\n",
    "                  f\"players ({matched/len(merged)*100:.1f} %)\")\n",
    "\n",
    "            if all_na.any():\n",
    "                sample = merged.loc[all_na, [\"Player\"]].head(5)[\"Player\"].tolist()\n",
    "                print(f\"  unmatched sample: {sample}\")\n",
    "\n",
    "    # ── Display normalization: uppercase the Player and Team for consistent downstream visibility\n",
    "    if \"Player\" in merged.columns:\n",
    "        merged[\"Player\"] = merged[\"Player\"].str.upper()\n",
    "    if \"Team\" in merged.columns:\n",
    "        merged[\"Team\"] = merged[\"Team\"].astype(str).str.upper()\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def guard_advanced_null_regress(df_new: pd.DataFrame,\n",
    "                                season: str,\n",
    "                                *,\n",
    "                                base_dir: str | None = None,\n",
    "                                debug: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Season-aware guard: compare the count of rows whose *all* advanced metrics\n",
    "    are NA in the *new* season vs the *previous* parquet for the SAME season.\n",
    "\n",
    "    This is READ-ONLY and prints diagnostics. It does not mutate data or fill.\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    from salary_nba_data_pull.scrape_utils import ADV_METRIC_COLS\n",
    "\n",
    "    if df_new.empty:\n",
    "        if debug:\n",
    "            print(f\"[guard_advanced_null_regress] {season}: empty new df\")\n",
    "        return\n",
    "\n",
    "    # Only check columns that actually exist after merge\n",
    "    adv_cols_present = [c for c in ADV_METRIC_COLS if c in df_new.columns]\n",
    "    if not adv_cols_present:\n",
    "        if debug:\n",
    "            print(f\"[guard_advanced_null_regress] {season}: no advanced cols present to check\")\n",
    "        return\n",
    "\n",
    "    # New all-advanced-NA count (per row)\n",
    "    na_mask_new = df_new[adv_cols_present].isna().all(axis=1)\n",
    "    new_all_na = int(na_mask_new.sum())\n",
    "    total = len(df_new)\n",
    "\n",
    "    # Load previous parquet for the same season\n",
    "    root = Path(base_dir) if base_dir else Path(__file__).resolve().parents[3] / \"data\" / \"new_processed\"\n",
    "    prev_path = root / f\"season={season}\" / \"part.parquet\"\n",
    "\n",
    "    prev_all_na = None\n",
    "    if prev_path.exists():\n",
    "        try:\n",
    "            prev = pd.read_parquet(prev_path)\n",
    "            prev_cols = [c for c in adv_cols_present if c in prev.columns]\n",
    "            if prev_cols:\n",
    "                na_mask_prev = prev[prev_cols].isna().all(axis=1)\n",
    "                prev_all_na = int(na_mask_prev.sum())\n",
    "        except Exception as exc:\n",
    "            if debug:\n",
    "                print(f\"[guard_advanced_null_regress] {season}: failed to read previous parquet -> {exc!s}\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[guard_advanced_null_regress] {season}: all-advanced-NA rows: \"\n",
    "              f\"new={new_all_na}/{total}\"\n",
    "              + (f\"  prev={prev_all_na}\" if prev_all_na is not None else \"  prev=<none>\"))\n",
    "\n",
    "    # This is a guardrail/print only. We DO NOT fail the run or fill values.\n",
    "\n",
    "def report_advanced_join_issues(df_after_merge: pd.DataFrame,\n",
    "                                season: str,\n",
    "                                *,\n",
    "                                topk: int = 3,\n",
    "                                max_rows: int = 25,\n",
    "                                debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Diagnostic only: list rows where *all BBR-only* advanced metrics are NA,\n",
    "    and show how keys differ between normalizers.\n",
    "\n",
    "    Columns returned:\n",
    "      Player, Team, player_key_left,\n",
    "      adv_key_enh (enhanced_normalize_name on BBR 'Player'),\n",
    "      adv_key_bbr (legacy _normalise_name (Unidecode) on BBR 'Player'),\n",
    "      close_matches_enh, close_matches_bbr\n",
    "\n",
    "    No mutation. Safe to run anytime after merge_advanced_metrics.\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "    from salary_nba_data_pull.scrape_utils import _season_advanced_df, ADV_METRIC_COLS\n",
    "    # Normalizers\n",
    "    try:\n",
    "        # the same enhanced used on the left df\n",
    "        norm_left = enhanced_normalize_name\n",
    "    except NameError:\n",
    "        from salary_nba_data_pull.process_utils import enhanced_normalize_name as norm_left\n",
    "    try:\n",
    "        from salary_nba_data_pull.scrape_utils import _normalise_name as norm_bbr_legacy\n",
    "    except Exception:\n",
    "        norm_bbr_legacy = norm_left  # fallback\n",
    "\n",
    "    if df_after_merge.empty:\n",
    "        if debug: print(\"[report_advanced_join_issues] empty frame\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Identify rows where all BBR-only cols are NA\n",
    "    bbr_only = [c for c in ADV_METRIC_COLS if c in df_after_merge.columns and c not in {\"TS%\", \"USG%\"}]\n",
    "    if not bbr_only:\n",
    "        if debug: print(\"[report_advanced_join_issues] no BBR-only columns present\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mask = df_after_merge[bbr_only].isna().all(axis=1)\n",
    "    left_unmatched = df_after_merge.loc[mask, [\"Player\", \"Team\"]].copy()\n",
    "    if left_unmatched.empty:\n",
    "        if debug: print(\"[report_advanced_join_issues] no unmatched rows on BBR-only cols\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Build left keys\n",
    "    left_unmatched[\"player_key_left\"] = left_unmatched[\"Player\"].map(norm_left)\n",
    "\n",
    "    # Load the season advanced table\n",
    "    adv = _season_advanced_df(season)\n",
    "    adv = adv[[\"Player\"]].drop_duplicates()\n",
    "    adv[\"adv_key_enh\"] = adv[\"Player\"].map(norm_left)\n",
    "    adv[\"adv_key_bbr\"] = adv[\"Player\"].map(norm_bbr_legacy)\n",
    "\n",
    "    keys_enh = adv[\"adv_key_enh\"].dropna().unique().tolist()\n",
    "    keys_bbr = adv[\"adv_key_bbr\"].dropna().unique().tolist()\n",
    "\n",
    "    def _cmatch(k, pool):\n",
    "        return difflib.get_close_matches(k, pool, n=topk, cutoff=0.80)\n",
    "\n",
    "    out_rows = []\n",
    "    for _, r in left_unmatched.head(max_rows).iterrows():\n",
    "        lk = r[\"player_key_left\"]\n",
    "        out_rows.append({\n",
    "            \"Player\": r[\"Player\"],\n",
    "            \"Team\": r.get(\"Team\"),\n",
    "            \"player_key_left\": lk,\n",
    "            \"close_matches_enh\": _cmatch(lk, keys_enh),\n",
    "            \"close_matches_bbr\": _cmatch(lk, keys_bbr),\n",
    "        })\n",
    "    out = pd.DataFrame(out_rows)\n",
    "    if debug:\n",
    "        print(\"[report_advanced_join_issues] sample unmatched with suggestions:\")\n",
    "        print(out.to_string(index=False))\n",
    "    return out\n",
    "\n",
    "def investigate_unmatched_players(df_after_merge: pd.DataFrame,\n",
    "                                 season: str,\n",
    "                                 *,\n",
    "                                 debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Investigate players that exist in NBA API but not in BBR advanced stats.\n",
    "    \n",
    "    This helps identify legitimate data source discrepancies vs encoding issues.\n",
    "    \n",
    "    Args:\n",
    "        df_after_merge: DataFrame after merge_advanced_metrics\n",
    "        season: Season string\n",
    "        debug: Whether to print diagnostics\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with investigation results\n",
    "    \"\"\"\n",
    "    from salary_nba_data_pull.scrape_utils import ADV_METRIC_COLS\n",
    "    \n",
    "    if df_after_merge.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find rows where all BBR-only advanced metrics are NA\n",
    "    bbr_only = [c for c in ADV_METRIC_COLS if c in df_after_merge.columns and c not in {\"TS%\", \"USG%\"}]\n",
    "    if not bbr_only:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    unmatched_mask = df_after_merge[bbr_only].isna().all(axis=1)\n",
    "    unmatched_df = df_after_merge.loc[unmatched_mask].copy()\n",
    "    \n",
    "    if unmatched_df.empty:\n",
    "        if debug:\n",
    "            print(f\"[investigate_unmatched_players] {season}: No unmatched players found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Add investigation columns\n",
    "    unmatched_df['MP_threshold'] = unmatched_df['MP'] < 200  # Common BBR threshold\n",
    "    unmatched_df['GP_threshold'] = unmatched_df['GP'] < 10   # Common BBR threshold\n",
    "    \n",
    "    # Load BBR data to check if player exists\n",
    "    from src.salary_nba_data_pull.scrape_utils import _season_advanced_df\n",
    "    from src.salary_nba_data_pull.name_utils import normalize_name\n",
    "    bbr_df = _season_advanced_df(season)\n",
    "    \n",
    "    investigation_results = []\n",
    "    for _, row in unmatched_df.iterrows():\n",
    "        player_name = row['Player']\n",
    "        player_key = normalize_name(player_name)\n",
    "        \n",
    "        # Check if player exists in BBR\n",
    "        bbr_match = bbr_df[bbr_df['player_key'] == player_key]\n",
    "        exists_in_bbr = len(bbr_match) > 0\n",
    "        \n",
    "        # Check for similar names\n",
    "        similar_names = bbr_df[bbr_df['Player'].str.contains(player_name.split()[-1], case=False, na=False)]\n",
    "        \n",
    "        result = {\n",
    "            'Player': player_name,\n",
    "            'Team': row.get('Team', ''),\n",
    "            'GP': row.get('GP', 0),\n",
    "            'MP': row.get('MP', 0),\n",
    "            'exists_in_bbr': exists_in_bbr,\n",
    "            'low_mp': row.get('MP', 0) < 200,\n",
    "            'low_gp': row.get('GP', 0) < 10,\n",
    "            'similar_names_count': len(similar_names),\n",
    "            'similar_names': similar_names['Player'].tolist()[:3] if len(similar_names) > 0 else []\n",
    "        }\n",
    "        investigation_results.append(result)\n",
    "    \n",
    "    results_df = pd.DataFrame(investigation_results)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[investigate_unmatched_players] {season}: Found {len(results_df)} unmatched players\")\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"  {row['Player']} ({row['Team']}) - {row['GP']} GP, {row['MP']} MP\")\n",
    "            print(f\"    Exists in BBR: {row['exists_in_bbr']}\")\n",
    "            print(f\"    Low MP: {row['low_mp']}, Low GP: {row['low_gp']}\")\n",
    "            if row['similar_names_count'] > 0:\n",
    "                print(f\"    Similar names: {row['similar_names']}\")\n",
    "            print()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# NEW utility – guarantee PlayerID / TeamID are present\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def ensure_player_ids(df: pd.DataFrame, season: str,\n",
    "                      *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For legacy parquet partitions that pre-date the PlayerID field.\n",
    "\n",
    "    • Uses `fetch_season_players()` once per season (cached)  \n",
    "    • Matches on the same `normalize_name()` key the rest of the pipeline uses  \n",
    "    • Fills **only the null rows** – never overwrites an existing ID  \n",
    "    • Also fills `TeamID` when missing and unambiguous\n",
    "\n",
    "    Returns a *copy* so the caller keeps purity.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    if \"PlayerID\" in df.columns and df[\"PlayerID\"].notna().all():\n",
    "        # nothing to do – fast path\n",
    "        return df\n",
    "\n",
    "    roster = fetch_season_players(season, debug=debug)      # cached\n",
    "    if debug:\n",
    "        print(f\"[ensure_player_ids] back-filling IDs for {season} \"\n",
    "              f\"(roster size {len(roster)})\")\n",
    "\n",
    "    out = df.copy()\n",
    "    # Build key once – works whether the column existed or not\n",
    "    out[\"__key\"] = out[\"Player\"].map(normalize_name)\n",
    "\n",
    "    # Create columns if they were totally missing\n",
    "    if \"PlayerID\" not in out.columns:\n",
    "        out[\"PlayerID\"] = pd.NA\n",
    "    if \"TeamID\" not in out.columns:\n",
    "        out[\"TeamID\"] = pd.NA\n",
    "\n",
    "    for k, meta in roster.items():\n",
    "        mask = (out[\"__key\"] == k) & (out[\"PlayerID\"].isna())\n",
    "        if mask.any():\n",
    "            out.loc[mask, \"PlayerID\"] = meta[\"player_id\"]\n",
    "            # Fill TeamID only when unambiguous (1 franchise per season key)\n",
    "            out.loc[mask & out[\"TeamID\"].isna(), \"TeamID\"] = meta[\"team_id\"]\n",
    "\n",
    "    out.drop(columns=\"__key\", inplace=True)\n",
    "\n",
    "    # Final sanity\n",
    "    if debug:\n",
    "        miss = int(out[\"PlayerID\"].isna().sum())\n",
    "        if miss:\n",
    "            print(f\"[ensure_player_ids] WARNING: {miss} rows still lack PlayerID\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/data_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/data_utils.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from salary_nba_data_pull.process_utils import (\n",
    "    inflate_value\n",
    ")\n",
    "from salary_nba_data_pull.quality import (\n",
    "    ExpectedSchema, audit_dataframe, write_audit_reports\n",
    ")\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "# Columns that must *always* be present – even if currently all NaN\n",
    "CRITICAL_ID_COLS: set[str] = {\"PlayerID\", \"TeamID\"}\n",
    "\n",
    "PRESERVE_EVEN_IF_ALL_NA = {\n",
    "    \"3P%\", \"Injured\", \"Injury_Periods\", \"Total_Days_Injured\", \"Injury_Risk\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- NEW: End-of-pipeline column pruning ---\n",
    "DROP_AT_END = {\n",
    "    \"Salary\",\n",
    "    \"2nd Apron\", \"Second Apron\",   # drop only second apron as requested\n",
    "}\n",
    "\n",
    "def prune_end_columns(df: pd.DataFrame, *, debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Drop end-of-pipeline columns without masking upstream issues.\"\"\"\n",
    "    existing = [c for c in df.columns if c in DROP_AT_END]\n",
    "    if debug and existing:\n",
    "        print(f\"[prune_end_columns] dropping columns at persist: {existing}\")\n",
    "    return df.drop(columns=existing, errors=\"ignore\")\n",
    "\n",
    "# --- NEW helper ------------------------------------------------------\n",
    "def load_salary_cap_parquet(path: str | Path, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the pre‑inflated salary‑cap parquet file; fall back to CSV loader\n",
    "    if the parquet is not found.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser().with_suffix(\".parquet\")\n",
    "    if path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary-cap] loading Parquet: {path}\")\n",
    "        return pd.read_parquet(path)\n",
    "    # fallback to old CSV helper for legacy compatibility\n",
    "    csv_path = path.with_suffix(\".csv\")\n",
    "    if csv_path.exists():\n",
    "        return load_salary_cap_csv(csv_path, debug=debug)\n",
    "    raise FileNotFoundError(f\"No salary‑cap parquet or CSV found at {path}\")\n",
    "\n",
    "def load_salary_cap_csv(path: str | Path, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the preprocessed salary cap CSV (inflated) instead of scraping.\n",
    "    We DO NOT fill or coerce silently – if a required column is missing,\n",
    "    we log it and let the caller decide.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser().resolve()\n",
    "    if debug:\n",
    "        print(f\"[salary-cap] loading local file: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    if debug:\n",
    "        print(f\"[salary-cap] rows={len(df)}, cols={df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"\n",
    "    Generic dataframe hygiene with *guarantees* that critical identifier\n",
    "    columns survive even if all values are currently missing.\n",
    "\n",
    "    Critical columns are defined in CRITICAL_ID_COLS at module scope.\n",
    "    \"\"\"\n",
    "    # Remove unnamed columns coming from CSV join artefacts\n",
    "    df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]\n",
    "\n",
    "    # Remove duplicate columns\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    # 🔒  Never drop the ID columns – keep them for schema stability\n",
    "    # --------------------------------------------------------------\n",
    "    keep_always = PRESERVE_EVEN_IF_ALL_NA.union(CRITICAL_ID_COLS)\n",
    "\n",
    "    all_na_cols = df.columns[df.isna().all()]\n",
    "    to_drop = [c for c in all_na_cols if c not in keep_always]\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "    # Remove rows that are entirely NaN\n",
    "    df = df.dropna(axis=0, how='all')\n",
    "\n",
    "    # Deduplicate 'Season' column if it slipped in twice\n",
    "    season_cols = [c for c in df.columns if 'Season' in c]\n",
    "    if len(season_cols) > 1:\n",
    "        df = df.rename(columns={season_cols[0]: 'Season'})\n",
    "        df = df.drop(columns=season_cols[1:])\n",
    "\n",
    "    # Optional removals\n",
    "    df = df.drop(columns=['3PAr', 'FTr'], errors='ignore')\n",
    "\n",
    "    # Round floats for storage\n",
    "    num = df.select_dtypes(include=[np.number]).columns\n",
    "    df[num] = df[num].round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_salary_cap_data(player_data: pd.DataFrame,\n",
    "                          salary_cap_data: pd.DataFrame,\n",
    "                          *,\n",
    "                          debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-merge cap data by season-year. Preserve all cap columns even if all NaN.\n",
    "    \"\"\"\n",
    "    if player_data.empty or salary_cap_data.empty:\n",
    "        if debug:\n",
    "            print(\"[merge_salary_cap_data] one side empty -> returning player_data unchanged\")\n",
    "        return player_data\n",
    "\n",
    "    # Make sure we don't mutate originals\n",
    "    p = player_data.copy()\n",
    "    cap = salary_cap_data.copy()\n",
    "\n",
    "    # Extract year\n",
    "    p[\"Season_Year\"]   = p[\"Season\"].str[:4].astype(int)\n",
    "    cap[\"Season_Year\"] = cap[\"Season\"].str[:4].astype(int)\n",
    "\n",
    "    # Inflate cap if not present\n",
    "    if \"Salary_Cap_Inflated\" not in cap.columns:\n",
    "        if debug:\n",
    "            print(\"[merge_salary_cap_data] computing Salary_Cap_Inflated\")\n",
    "        cap[\"Salary_Cap_Inflated\"] = cap.apply(\n",
    "            lambda r: inflate_value(r.get(\"Salary Cap\", np.nan), r.get(\"Season\", \"\")),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Merge\n",
    "    merged = pd.merge(p, cap, on=\"Season_Year\", how=\"left\", suffixes=(\"\", \"_cap\"))\n",
    "\n",
    "    # Figure out which columns came from cap\n",
    "    cap_cols = [c for c in cap.columns if c not in {\"Season_Year\"}]\n",
    "\n",
    "    # For each cap col, if we created a *_cap twin, consolidate\n",
    "    for col in cap_cols:\n",
    "        src = f\"{col}_cap\"\n",
    "        if src in merged.columns:\n",
    "            merged[col] = merged[col].where(~merged[col].isna(), merged[src])\n",
    "            merged.drop(columns=[src], inplace=True)\n",
    "\n",
    "    # Cleanup\n",
    "    merged.drop(columns=[\"Season_Year\"], inplace=True)\n",
    "\n",
    "    # Protect salary-cap columns from being dropped in clean_dataframe\n",
    "    global PRESERVE_EVEN_IF_ALL_NA\n",
    "    PRESERVE_EVEN_IF_ALL_NA = PRESERVE_EVEN_IF_ALL_NA.union(set(cap_cols))\n",
    "\n",
    "    merged = clean_dataframe(merged)\n",
    "\n",
    "    if debug:\n",
    "        miss = [c for c in cap_cols if c not in merged.columns]\n",
    "        if miss:\n",
    "            print(f\"[merge_salary_cap_data] WARNING missing cap cols after merge: {miss}\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "def load_external_salary_data(season: str,\n",
    "                              root: Path | str = DATA_PROCESSED_DIR / \"salary_external\",\n",
    "                              *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read player‑salary data from various formats.\n",
    "    Expected paths (in order of preference):\n",
    "    1. {root}/season={YYYY-YY}/part.parquet\n",
    "    2. {root}/comprehensive_salary_data.csv (with Season column)\n",
    "    3. {root}/sample_salary_data.csv (with Season column)\n",
    "    \"\"\"\n",
    "    # Try parquet file first\n",
    "    parquet_path = Path(root) / f\"season={season}/part.parquet\"\n",
    "    if parquet_path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary‑ext] loading parquet {parquet_path}\")\n",
    "        return pd.read_parquet(parquet_path)\n",
    "    \n",
    "    # Try comprehensive CSV file\n",
    "    csv_path = Path(root) / \"comprehensive_salary_data.csv\"\n",
    "    if csv_path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary‑ext] loading comprehensive CSV {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if 'Season' in df.columns:\n",
    "            season_data = df[df['Season'] == season]\n",
    "            if not season_data.empty:\n",
    "                return season_data\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"[salary‑ext] no data for season {season} in comprehensive CSV\")\n",
    "    \n",
    "    # Try sample CSV file\n",
    "    sample_csv_path = Path(root) / \"sample_salary_data.csv\"\n",
    "    if sample_csv_path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary‑ext] loading sample CSV {sample_csv_path}\")\n",
    "        df = pd.read_csv(sample_csv_path)\n",
    "        if 'Season' in df.columns:\n",
    "            season_data = df[df['Season'] == season]\n",
    "            if not season_data.empty:\n",
    "                return season_data\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"[salary‑ext] no data for season {season} in sample CSV\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[salary‑ext] no salary file found for season {season}\")\n",
    "    return pd.DataFrame(columns=[\"Player\", \"Salary\", \"Season\"])\n",
    "\n",
    "def validate_data(df: pd.DataFrame,\n",
    "                  *,\n",
    "                  name: str = \"player_dataset\",\n",
    "                  save_reports: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Basic schema and quality checks. `PlayerID` is now mandatory.\n",
    "    \"\"\"\n",
    "    schema = ExpectedSchema(\n",
    "        expected_cols=df.columns,\n",
    "        required_cols=[\"Season\", \"Player\", \"Team\", \"PlayerID\"],   #  ← added\n",
    "        dtypes={\n",
    "            \"Season\": \"object\",\n",
    "            \"Player\": \"object\",\n",
    "        },\n",
    "        # Salary & Team_Salary dropped from non‑neg / non‑constant\n",
    "        non_negative_cols=[\"GP\", \"MP\", \"PTS\", \"TRB\", \"AST\"],\n",
    "        non_constant_cols=[\"PTS\"],\n",
    "        unique_key=[\"Season\", \"Player\"]\n",
    "    )\n",
    "\n",
    "    reports = audit_dataframe(df, schema, name=name)\n",
    "\n",
    "    if save_reports:\n",
    "        out_dir = DATA_PROCESSED_DIR / \"audits\"\n",
    "        write_audit_reports(reports, out_dir, prefix=name)\n",
    "\n",
    "    # Print a one-liner summary (optional)\n",
    "    missing_req = reports[\"cols_overview\"].query(\"missing_required == True\")\n",
    "    if not missing_req.empty:\n",
    "        print(f\"[validate_data] Missing required columns: {missing_req['column'].tolist()}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/quality.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/quality.py\n",
    "# src/salary_nba_data_pull/quality.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Mapping, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class ExpectedSchema:\n",
    "    \"\"\"Describe what we *intended* to have in a dataframe.\"\"\"\n",
    "    # All columns we care about (order doesn't matter)\n",
    "    expected_cols: Iterable[str]\n",
    "\n",
    "    # Subset that must be present\n",
    "    required_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Expected pandas dtypes (string form, e.g. 'float64', 'object')\n",
    "    dtypes: Mapping[str, str] = field(default_factory=dict)\n",
    "\n",
    "    # Columns that must be >= 0\n",
    "    non_negative_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Columns that should not be all zeros / all NaN\n",
    "    non_constant_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Unique key columns (together must be unique)\n",
    "    unique_key: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Allowed value sets (enums)\n",
    "    allowed_values: Mapping[str, Iterable[Any]] = field(default_factory=dict)\n",
    "\n",
    "def _series_is_constant(s: pd.Series) -> bool:\n",
    "    return s.nunique(dropna=True) <= 1\n",
    "\n",
    "def audit_dataframe(df: pd.DataFrame,\n",
    "                    schema: ExpectedSchema,\n",
    "                    *,\n",
    "                    name: str = \"dataset\") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Return a dict of small DataFrames summarising quality checks.\n",
    "    Nothing is printed; caller decides how to persist/log.\n",
    "    \"\"\"\n",
    "    exp = set(schema.expected_cols)\n",
    "    req = set(schema.required_cols)\n",
    "\n",
    "    present = set(df.columns)\n",
    "    missing = sorted(list(exp - present))\n",
    "    extra   = sorted(list(present - exp))\n",
    "\n",
    "    # --- Column overview\n",
    "    cols_overview = pd.DataFrame({\n",
    "        \"column\": sorted(list(exp | present)),\n",
    "        \"expected\": [c in exp for c in sorted(list(exp | present))],\n",
    "        \"present\":  [c in present for c in sorted(list(exp | present))],\n",
    "        \"required\": [c in req for c in sorted(list(exp | present))]\n",
    "    })\n",
    "    cols_overview[\"missing_required\"] = cols_overview.apply(\n",
    "        lambda r: r[\"required\"] and not r[\"present\"], axis=1\n",
    "    )\n",
    "\n",
    "    # --- Null report\n",
    "    null_report = (df.isna().sum().to_frame(\"null_count\")\n",
    "                     .assign(total_rows=len(df))\n",
    "                     .assign(null_pct=lambda d: 100 * d[\"null_count\"] / d[\"total_rows\"])\n",
    "                     .reset_index()\n",
    "                     .rename(columns={\"index\": \"column\"}))\n",
    "\n",
    "    # --- Dtype report\n",
    "    type_rows = []\n",
    "    for col in df.columns:\n",
    "        exp_type = schema.dtypes.get(col)\n",
    "        type_rows.append({\n",
    "            \"column\": col,\n",
    "            \"expected_dtype\": exp_type,\n",
    "            \"actual_dtype\": str(df[col].dtype),\n",
    "            \"matches\": (exp_type is None) or (str(df[col].dtype) == exp_type)\n",
    "        })\n",
    "    type_report = pd.DataFrame(type_rows)\n",
    "\n",
    "    # --- Value checks\n",
    "    value_rows = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        series = df[col]\n",
    "        row = {\n",
    "            \"column\": col,\n",
    "            \"min\": series.min(skipna=True),\n",
    "            \"max\": series.max(skipna=True),\n",
    "            \"negatives\": int((series < 0).sum()),\n",
    "            \"zeros\": int((series == 0).sum()),\n",
    "            \"non_zero_pct\": 100 * (series != 0).sum() / len(series),\n",
    "        }\n",
    "        row[\"should_be_non_negative\"] = col in schema.non_negative_cols\n",
    "        row[\"violates_non_negative\"] = row[\"negatives\"] > 0 and row[\"should_be_non_negative\"]\n",
    "        value_rows.append(row)\n",
    "    value_report = pd.DataFrame(value_rows)\n",
    "\n",
    "    # Constant columns\n",
    "    constant_rows = []\n",
    "    for col in df.columns:\n",
    "        constant_rows.append({\n",
    "            \"column\": col,\n",
    "            \"is_constant\": _series_is_constant(df[col]),\n",
    "            \"should_not_be_constant\": col in schema.non_constant_cols\n",
    "        })\n",
    "    constant_report = pd.DataFrame(constant_rows).assign(\n",
    "        violates=lambda d: d[\"is_constant\"] & d[\"should_not_be_constant\"]\n",
    "    )\n",
    "\n",
    "    # Allowed values\n",
    "    enum_rows = []\n",
    "    for col, allowed in schema.allowed_values.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        bad = ~df[col].isin(allowed) & df[col].notna()\n",
    "        enum_rows.append({\n",
    "            \"column\": col,\n",
    "            \"bad_count\": int(bad.sum()),\n",
    "            \"sample_bad\": df.loc[bad, col].drop_duplicates().head(5).tolist()\n",
    "        })\n",
    "    enum_report = pd.DataFrame(enum_rows)\n",
    "\n",
    "    # Unique key\n",
    "    uniq_report = pd.DataFrame()\n",
    "    if schema.unique_key:\n",
    "        dup_mask = df.duplicated(subset=list(schema.unique_key), keep=False)\n",
    "        uniq_report = pd.DataFrame({\n",
    "            \"duplicate_rows\": [int(dup_mask.sum())],\n",
    "            \"subset\": [list(schema.unique_key)]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"cols_overview\": cols_overview,\n",
    "        \"null_report\": null_report,\n",
    "        \"type_report\": type_report,\n",
    "        \"value_report\": value_report,\n",
    "        \"constant_report\": constant_report,\n",
    "        \"enum_report\": enum_report,\n",
    "        \"unique_report\": uniq_report\n",
    "    }\n",
    "\n",
    "def assert_dataframe_ok(df: pd.DataFrame,\n",
    "                        schema: ExpectedSchema,\n",
    "                        *, name: str = \"dataset\") -> None:\n",
    "    \"\"\"\n",
    "    Raise AssertionError with a concise message if critical checks fail.\n",
    "    Designed for pytest or CI.\n",
    "    \"\"\"\n",
    "    rep = audit_dataframe(df, schema, name=name)\n",
    "    bad_missing = rep[\"cols_overview\"].query(\"missing_required == True\")\n",
    "    bad_types = rep[\"type_report\"].query(\"matches == False\")\n",
    "    bad_nonneg = rep[\"value_report\"].query(\"violates_non_negative == True\")\n",
    "    bad_constant = rep[\"constant_report\"].query(\"violates == True\")\n",
    "    dupes = rep[\"unique_report\"][\"duplicate_rows\"].iloc[0] if not rep[\"unique_report\"].empty else 0\n",
    "\n",
    "    msgs = []\n",
    "    if not bad_missing.empty:\n",
    "        msgs.append(f\"Missing required cols: {bad_missing['column'].tolist()}\")\n",
    "    if not bad_types.empty:\n",
    "        msgs.append(f\"Dtype mismatches: {bad_types[['column','expected_dtype','actual_dtype']].to_dict('records')}\")\n",
    "    if not bad_nonneg.empty:\n",
    "        msgs.append(f\"Negative values in non-negative cols: {bad_nonneg['column'].tolist()}\")\n",
    "    if not bad_constant.empty:\n",
    "        msgs.append(f\"Constant-but-shouldn't cols: {bad_constant['column'].tolist()}\")\n",
    "    if dupes:\n",
    "        msgs.append(f\"Duplicate key rows: {dupes}\")\n",
    "\n",
    "    if msgs:\n",
    "        raise AssertionError(f\"[{name}] data quality failures:\\n\" + \"\\n\".join(msgs))\n",
    "\n",
    "def write_audit_reports(reports: Mapping[str, pd.DataFrame],\n",
    "                        out_dir: Path,\n",
    "                        prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Save each report DataFrame as CSV for later inspection.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for key, df in reports.items():\n",
    "        df.to_csv(out_dir / f\"{prefix}_{key}.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/main.py\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import requests_cache\n",
    "from salary_nba_data_pull.fetch_utils import fetch_all_players, fetch_season_players, fetch_league_standings\n",
    "from salary_nba_data_pull.process_utils import (\n",
    "    process_player_data,\n",
    "    inflate_value,\n",
    "    calculate_percentages,\n",
    "    _ensure_cpi_ready,\n",
    "    add_usage_components,\n",
    "    consolidate_duplicate_columns,\n",
    "    ensure_player_ids,\n",
    ")\n",
    "\n",
    "    # Removed advanced metrics scraping imports to eliminate nulls\n",
    "from salary_nba_data_pull.data_utils import (\n",
    "    clean_dataframe,\n",
    "    validate_data,\n",
    ")\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "# Enable requests-cache for all HTTP traffic\n",
    "requests_cache.install_cache(\"nba_pull\", backend=\"sqlite\", allowable_codes=(200,))\n",
    "\n",
    "# CPI self-test - logs a warning once per run if CPI is unavailable\n",
    "_ensure_cpi_ready(debug=False)\n",
    "\n",
    "# Default number of worker threads\n",
    "DEFAULT_WORKERS = 8                # tweak ≤ CPU cores\n",
    "\n",
    "def _almost_equal_numeric(a: pd.Series, b: pd.Series, atol=1e-6, rtol=1e-9):\n",
    "    # Handle NA values first\n",
    "    mask = a.isna() & b.isna()\n",
    "    \n",
    "    # For non-NA values, compare them\n",
    "    both_numeric = pd.api.types.is_numeric_dtype(a) and pd.api.types.is_numeric_dtype(b)\n",
    "    if not both_numeric:\n",
    "        # For non-numeric columns, use pandas equals but handle NA carefully\n",
    "        non_na_mask = ~(a.isna() | b.isna())\n",
    "        eq_result = pd.Series(False, index=a.index)\n",
    "        if non_na_mask.any():\n",
    "            eq_result[non_na_mask] = a[non_na_mask].eq(b[non_na_mask])\n",
    "        return eq_result | mask\n",
    "    else:\n",
    "        # For numeric columns, use numpy isclose\n",
    "        non_na_mask = ~(a.isna() | b.isna())\n",
    "        diff_ok = pd.Series(False, index=a.index)\n",
    "        if non_na_mask.any():\n",
    "            diff_ok[non_na_mask] = np.isclose(\n",
    "                a[non_na_mask].astype(float), \n",
    "                b[non_na_mask].astype(float), \n",
    "                atol=atol, rtol=rtol\n",
    "            )\n",
    "        return diff_ok | mask\n",
    "\n",
    "# helper 1 ─ column drift\n",
    "def _columns_diff(old_df: pd.DataFrame, new_df: pd.DataFrame):\n",
    "    added   = sorted(set(new_df.columns) - set(old_df.columns))\n",
    "    removed = sorted(set(old_df.columns) - set(new_df.columns))\n",
    "    return added, removed\n",
    "\n",
    "# helper 2 ─ mean smoke‑test\n",
    "def _mean_diff(old_df: pd.DataFrame, new_df: pd.DataFrame,\n",
    "               tol_pct: float = 0.001) -> pd.DataFrame:\n",
    "    common = old_df.select_dtypes(\"number\").columns.intersection(\n",
    "             new_df.select_dtypes(\"number\").columns)\n",
    "    rows = []\n",
    "    for c in common:\n",
    "        o, n = old_df[c].mean(skipna=True), new_df[c].mean(skipna=True)\n",
    "        if pd.isna(o) or pd.isna(n):\n",
    "            continue\n",
    "        rel = abs(n - o) / (abs(o) + 1e-12)\n",
    "        if rel > tol_pct:\n",
    "            rows.append({\"column\": c, \"old_mean\": o, \"new_mean\": n, \"rel_diff\": rel})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _diff_report(old_df, new_df, key_cols=(\"Season\",\"Player\"),\n",
    "                 numeric_atol=1e-6, numeric_rtol=1e-9, max_print=10):\n",
    "    cols_add, cols_rem = _columns_diff(old_df, new_df)\n",
    "    mean_diffs = _mean_diff(old_df, new_df)\n",
    "\n",
    "    # value‑level diff (original logic)\n",
    "    common = [c for c in new_df.columns if c in old_df.columns]\n",
    "    old, new = old_df[common], new_df[common]\n",
    "\n",
    "    # Handle case where dataframes have different lengths\n",
    "    if len(old) != len(new):\n",
    "        # If lengths differ, we can't do row-by-row comparison\n",
    "        diffs = []\n",
    "    else:\n",
    "        if all(k in common for k in key_cols):\n",
    "            old = old.sort_values(list(key_cols)).reset_index(drop=True)\n",
    "            new = new.sort_values(list(key_cols)).reset_index(drop=True)\n",
    "        else:\n",
    "            key_cols = (\"__row__\",)\n",
    "            old[\"__row__\"] = new[\"__row__\"] = range(len(old))\n",
    "\n",
    "        diffs = []\n",
    "        for col in common:\n",
    "            equal = _almost_equal_numeric(old[col], new[col],\n",
    "                                          atol=numeric_atol, rtol=numeric_rtol)\n",
    "            for i in np.where(~equal)[0]:\n",
    "                if i < len(old) and i < len(new):  # Safety check\n",
    "                    row_keys = {k: new.iloc[i][k] for k in key_cols}\n",
    "                    diffs.append({**row_keys, \"column\": col,\n",
    "                                  \"old\": old.iloc[i][col], \"new\": new.iloc[i][col]})\n",
    "\n",
    "    is_equal = (not diffs) and (not cols_add) and (not cols_rem) and mean_diffs.empty\n",
    "    summary = (f\"cells:{len(diffs)}  col+:{len(cols_add)}  col-:{len(cols_rem)}  \"\n",
    "               f\"meanΔ:{len(mean_diffs)}\")\n",
    "    return is_equal, summary, pd.DataFrame(diffs), cols_add, cols_rem, mean_diffs\n",
    "\n",
    "def _file_md5(path: str, chunk: int = 1 << 20) -> str:\n",
    "    \"\"\"Return md5 hexdigest for *path* streaming in 1 MiB chunks.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for blk in iter(lambda: f.read(chunk), b\"\"):\n",
    "            h.update(blk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _season_partition_identical(season: str,\n",
    "                                base_dir: Path | str,\n",
    "                                new_df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if on-disk parquet for `season` is byte-wise equivalent (after\n",
    "    canonical sort & column alignment) to `new_df`.\n",
    "    \"\"\"\n",
    "    ckpt = Path(base_dir) / f\"season={season}\" / \"part.parquet\"\n",
    "    if not ckpt.exists():\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        old_df = pd.read_parquet(ckpt)\n",
    "    except Exception as exc:\n",
    "        logging.warning(\"[identical] failed to read %s → %s\", ckpt, exc)\n",
    "        return False\n",
    "\n",
    "    # STEP B1: align columns and sort only by stable key\n",
    "    cols = sorted(set(old_df.columns) | set(new_df.columns))\n",
    "    key = [\"Season\",\"Player\"]\n",
    "\n",
    "    old_cmp = (old_df.reindex(columns=cols)\n",
    "                     .sort_values(key)\n",
    "                     .reset_index(drop=True))\n",
    "    new_cmp = (new_df.reindex(columns=cols)\n",
    "                     .sort_values(key)\n",
    "                     .reset_index(drop=True))\n",
    "\n",
    "    return old_cmp.equals(new_cmp)   # NaNs treated equal if aligned\n",
    "\n",
    "def _season_partition_exists(season, base_dir):\n",
    "    \"\"\"Check if a season partition already exists in Parquet format.\"\"\"\n",
    "    return os.path.exists(os.path.join(base_dir, f\"season={season}\"))\n",
    "\n",
    "def _player_task(args):\n",
    "    \"\"\"Wrapper for ThreadPoolExecutor.\"\"\"\n",
    "    (player_name, season, salary, all_players, debug) = args\n",
    "    stats = process_player_data(player_name, season, all_players, debug=debug)\n",
    "    if stats:\n",
    "        stats['Salary'] = salary\n",
    "    return stats\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import logging, textwrap\n",
    "\n",
    "CORE_COLS = (\"FGA\", \"FTA\", \"MP\", \"PTS\")\n",
    "\n",
    "def debug_checkpoint(df: pd.DataFrame,\n",
    "                     label: str,\n",
    "                     *,\n",
    "                     core_cols: tuple[str, ...] = CORE_COLS,\n",
    "                     head: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Print a compact overview of the DataFrame at a pipeline milestone.\n",
    "\n",
    "    • Always shows #rows, #cols.\n",
    "    • Warns if any `core_cols` are missing.\n",
    "    • Optionally prints `df.head(head)` for a quick sanity scan.\n",
    "    \"\"\"\n",
    "    msg = f\"[chk:{label}] rows={len(df):,}  cols={len(df.columns):,}\"\n",
    "    missing = [c for c in core_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        msg += f\"  ❌ MISSING: {missing}\"\n",
    "    logging.debug(msg)\n",
    "    print(msg)                     # visible even without logging configured\n",
    "    if head > 0:\n",
    "        print(textwrap.indent(df.head(head).to_string(index=False), \"    \"))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "def update_data(existing_data,\n",
    "                start_year: int,\n",
    "                end_year: int,\n",
    "                *,\n",
    "                player_filter: str = \"all\",\n",
    "                min_avg_minutes: float | None = None,    # NEW: filter on avg minutes\n",
    "                min_shot_attempts: int | None = None,    # NEW: filter on shot attempts\n",
    "                nan_filter: bool = False,                 # NEW: enable threshold-aware NaN filtering\n",
    "                nan_filter_percentage: float = 0.01,      # NEW: threshold for low-missing columns\n",
    "                debug: bool = False,\n",
    "                small_debug: bool = False,\n",
    "                max_workers: int = 8,\n",
    "                output_base: str | Path = DATA_PROCESSED_DIR,\n",
    "                overwrite: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull seasons in [start_year, end_year], WITHOUT any salary or injury merges.\n",
    "    Ensures we only rely on nba_api rosters + career stats + W/L logs.\n",
    "    \n",
    "    FILTERS:\n",
    "    - min_avg_minutes: Filter out players averaging < this many minutes per game\n",
    "    - min_shot_attempts: Filter out players with fewer than this many total shot attempts (FGA+FTA)\n",
    "    - nan_filter: If True, apply threshold-aware NaN filtering instead of dropping all rows with any NaN\n",
    "    - nan_filter_percentage: Threshold for low-missing columns when nan_filter=True (default 1%)\n",
    "    \n",
    "    These filters help eliminate nulls from low-volume players who don't have enough\n",
    "    data for meaningful percentage calculations.\n",
    "    \"\"\"\n",
    "    output_base = Path(output_base)\n",
    "    helper_debug = debug and not small_debug\n",
    "\n",
    "    from salary_nba_data_pull.scrape_utils import _season_advanced_df\n",
    "    from salary_nba_data_pull.fetch_utils import (\n",
    "        fetch_season_players, fetch_league_standings, fetch_team_wl_by_season\n",
    "    )\n",
    "    from salary_nba_data_pull.process_utils import (\n",
    "        process_player_data, calculate_percentages, add_usage_components,\n",
    "        attach_wins_losses, merge_advanced_metrics,\n",
    "    )\n",
    "\n",
    "    out_frames: list[pd.DataFrame] = []\n",
    "    season_summaries: list[str] = []\n",
    "\n",
    "    for y in tqdm(range(start_year, end_year + 1),\n",
    "                  desc=\"Seasons\", disable=small_debug):\n",
    "        season = f\"{y}-{str(y+1)[-2:]}\"\n",
    "        ckpt_dir = output_base / f\"season={season}\"\n",
    "\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] Starting season {season}\")\n",
    "\n",
    "        # 1️⃣ Fetch the complete season roster\n",
    "        roster = fetch_season_players(season, debug=helper_debug)\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] fetched {len(roster)} players for {season}\")\n",
    "\n",
    "        # 2️⃣ Build args for each player (correct signature)\n",
    "        args = [\n",
    "            (name, season, roster, helper_debug)\n",
    "            for name in roster.keys()\n",
    "            if (player_filter == \"all\" or player_filter.lower() in name)\n",
    "        ]\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] processing {len(args)} players after filter\")\n",
    "\n",
    "        # 3️⃣ Process each player in parallel (correct signature)\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        results, failures = [], 0\n",
    "        with ThreadPoolExecutor(max_workers=min(max_workers, len(args) or 1)) as pool:\n",
    "            futures = {pool.submit(\n",
    "                lambda nm, ss, rp, dbg: process_player_data(nm, ss, rp, debug=dbg),\n",
    "                *arg\n",
    "            ): arg[0] for arg in args}\n",
    "\n",
    "            for fut in as_completed(futures):\n",
    "                pname = futures[fut]\n",
    "                try:\n",
    "                    res = fut.result()\n",
    "                    if res is None:\n",
    "                        if helper_debug:\n",
    "                            print(f\"[update_data][WARN] no data for player '{pname}' in {season}\")\n",
    "                    else:\n",
    "                        results.append(res)\n",
    "                except Exception as exc:\n",
    "                    failures += 1\n",
    "                    logging.exception(\"Player task failed for %s (%s): %s\", pname, season, exc)\n",
    "\n",
    "        if failures and debug:\n",
    "            print(f\"[update_data] ⚠️  {failures} player failures in {season}\")\n",
    "\n",
    "        df_season = pd.DataFrame(results)\n",
    "        \n",
    "        # NEW: repair legacy partitions that missed PlayerID / TeamID\n",
    "        df_season = ensure_player_ids(df_season, season, debug=helper_debug)\n",
    "        \n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] {season} → DataFrame with {len(df_season)} rows\")\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # A) **EARLY minutes-per-game filter**\n",
    "        # ------------------------------------------------------------------\n",
    "        if (min_avg_minutes is not None) and (\"MP\" in df_season.columns):\n",
    "            before = len(df_season)\n",
    "            df_season = df_season.query(\"MP >= @min_avg_minutes\")\n",
    "            if helper_debug:\n",
    "                print(f\"[filter-early] {season}: MP ≥ {min_avg_minutes}  \"\n",
    "                      f\"→ {before}→{len(df_season)} rows\")\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        # 4️⃣ Attach W/L using unified lookup\n",
    "        df_season = df_season.pipe(\n",
    "            attach_wins_losses, season=season, debug=helper_debug\n",
    "        )\n",
    "\n",
    "        # 5️⃣ Derived metrics & clean\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] {season} before derived metrics: {len(df_season.columns)} columns\")\n",
    "            print(f\"[update_data] {season} columns: {list(df_season.columns)}\")\n",
    "        \n",
    "        merged = (\n",
    "            df_season\n",
    "            .pipe(calculate_percentages, debug=helper_debug)\n",
    "            .pipe(add_usage_components, debug=helper_debug)\n",
    "            .pipe(merge_advanced_metrics, season=season, debug=helper_debug)  # Re-enabled advanced metrics\n",
    "            .pipe(consolidate_duplicate_columns, debug=helper_debug)\n",
    "        )\n",
    "        \n",
    "        # -------------------------------------------------------------\n",
    "        # 🔒 Debug sentinel – verify PlayerID survival after consolidation\n",
    "        # -------------------------------------------------------------\n",
    "        debug_checkpoint(merged, f\"{season}:post-consolidate\", head=0)\n",
    "        assert \"PlayerID\" in merged.columns, f\"[update_data] {season}: PlayerID lost after consolidate_duplicate_columns\"\n",
    "        \n",
    "        debug_checkpoint(merged, f\"{season}:post-derived\", head=3)\n",
    "        \n",
    "        \n",
    "        # --- season-aware guard & diagnostics (no filling) ---\n",
    "        if helper_debug:\n",
    "            from salary_nba_data_pull.process_utils import (\n",
    "                guard_advanced_null_regress, diagnose_advanced_nulls\n",
    "            )\n",
    "            guard_advanced_null_regress(merged, season, base_dir=output_base, debug=True)\n",
    "            _ = diagnose_advanced_nulls(merged, season, debug=True)\n",
    "        \n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] {season} after derived metrics: {len(merged.columns)} columns\")\n",
    "            advanced_cols = ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
    "            found_advanced = [col for col in advanced_cols if col in merged.columns]\n",
    "            print(f\"[update_data] {season} advanced columns found: {found_advanced}\")\n",
    "\n",
    "        # ── NEW: apply user‐specified filters ─────────────────────────\n",
    "        # 5️⃣ Filter low-minute players  (robust to missing column)\n",
    "        if min_avg_minutes is not None:\n",
    "            if \"MP\" in merged.columns:                    # keep the guarded fallback\n",
    "                before = len(merged)\n",
    "                merged = merged.query(\"MP >= @min_avg_minutes\")\n",
    "                if helper_debug:\n",
    "                    print(f\"[filter-late] {season}: MP ≥ {min_avg_minutes}  \"\n",
    "                          f\"→ {before}→{len(merged)} rows\")\n",
    "            else:\n",
    "                logging.warning(\"[filter-late] %s: 'MP' col missing – skipped\", season)\n",
    "        #  ^-- Only this guarded block remains.  **The stray unconditional query is gone.**\n",
    "        # ── NEW: shot-attempt filter with robust column handling ────────────\n",
    "        if min_shot_attempts is not None:\n",
    "            # We accept either 'FGA' directly *or* twins created by merges.\n",
    "            for _candidate in (\"FGA\", \"FGA_x\", \"FGA_y\"):\n",
    "                if _candidate in merged.columns:\n",
    "                    fga_col = _candidate\n",
    "                    break\n",
    "            else:   # no break → not found\n",
    "                raise KeyError(\n",
    "                    \"[filter-shots] 'FGA' column missing after merges – \"\n",
    "                    \"check consolidate_duplicate_columns or earlier transforms.\"\n",
    "                )\n",
    "\n",
    "            fta_col = \"FTA\" if \"FTA\" in merged.columns else \\\n",
    "                      \"FTA_x\" if \"FTA_x\" in merged.columns else \\\n",
    "                      \"FTA_y\" if \"FTA_y\" in merged.columns else None\n",
    "\n",
    "            if fta_col is None:\n",
    "                raise KeyError(\"[filter-shots] 'FTA' column missing after merges.\")\n",
    "\n",
    "            before = len(merged)\n",
    "            merged = (\n",
    "                merged\n",
    "                .assign(_shots=merged[fga_col].fillna(0) + merged[fta_col].fillna(0))\n",
    "                .query(\"_shots >= @min_shot_attempts\")\n",
    "                .drop(columns=[\"_shots\"])\n",
    "            )\n",
    "            if helper_debug:\n",
    "                print(f\"[filter-shots] {season}: ≥{min_shot_attempts} attempts \"\n",
    "                      f\"→ {before}→{len(merged)} rows\")\n",
    "\n",
    "        # ── NEW: apply NaN filtering ───────────────────────────────────\n",
    "        if nan_filter:\n",
    "            # Apply threshold-aware NaN filtering\n",
    "            before = len(merged)\n",
    "            null_pct = merged.isna().mean()\n",
    "            cols_to_strict_drop = null_pct[(null_pct > 0) & (null_pct <= nan_filter_percentage)].index.tolist()\n",
    "            \n",
    "            if cols_to_strict_drop:\n",
    "                merged = merged.dropna(subset=cols_to_strict_drop)\n",
    "                dropped = before - len(merged)\n",
    "                if helper_debug:\n",
    "                    print(f\"[nan_filter] {season}: dropped {dropped} rows based on \"\n",
    "                          f\"{len(cols_to_strict_drop)} low-missing columns (≤ {nan_filter_percentage*100:.1f}%)\")\n",
    "                    print(f\"[nan_filter] {season}: low-missing columns: {cols_to_strict_drop}\")\n",
    "            else:\n",
    "                if helper_debug:\n",
    "                    print(f\"[nan_filter] {season}: no columns below threshold; no rows dropped\")\n",
    "        else:\n",
    "            # Legacy behavior: drop any row with any NaN\n",
    "            before = len(merged)\n",
    "            merged = merged.dropna()\n",
    "            if helper_debug:\n",
    "                print(f\"[nan_filter] {season}: legacy dropna - dropped {before - len(merged)} rows with any NaN\")\n",
    "        # ── end filters ──────────────────────────────────────────────────\n",
    "\n",
    "        # Key‐column sanity\n",
    "        dups = merged.duplicated(subset=[\"Season\",\"Player\"], keep=False)\n",
    "        if dups.any():\n",
    "            sample = merged.loc[dups, [\"Season\",\"Player\",\"Team\",\"MP\"]]\n",
    "            print(f\"[update_data][ERROR] Duplicate keys in {season}:\\n{sample}\")\n",
    "            raise AssertionError(f\"Duplicate (Season,Player) in {season}\")\n",
    "\n",
    "        # Trim whitespace-only strings → NA\n",
    "        obj_cols = merged.select_dtypes(include=[\"object\"]).columns\n",
    "        for c in obj_cols:\n",
    "            merged[c] = merged[c].replace(r\"^\\s*$\", pd.NA, regex=True)\n",
    "\n",
    "        # Persist per‐season partition\n",
    "        parquet_path = ckpt_dir / \"part.parquet\"\n",
    "        if not overwrite and parquet_path.exists():\n",
    "            from salary_nba_data_pull.main import _season_partition_identical\n",
    "            if _season_partition_identical(season, output_base, merged):\n",
    "                if helper_debug:\n",
    "                    print(f\"[update_data] {season} unchanged, skipping write\")\n",
    "                out_frames.append(merged)\n",
    "                continue\n",
    "        merged.to_parquet(parquet_path, index=False)\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] wrote {parquet_path}\")\n",
    "\n",
    "        out_frames.append(merged)\n",
    "        season_summaries.append(f\"{season}: {len(merged)} rows\")\n",
    "\n",
    "    if small_debug:\n",
    "        print(\"\\n--- Seasons Summaries ---\")\n",
    "        print(\"\\n\".join(season_summaries))\n",
    "        print(\"-------------------------\\n\")\n",
    "\n",
    "    return pd.concat(out_frames, ignore_index=True) if out_frames else pd.DataFrame()\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Return a filesystem-safe timestamp string.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def remove_old_logs(log_dir, days_to_keep=7):\n",
    "    current_time = datetime.now()\n",
    "    for log_file in glob.glob(os.path.join(log_dir, 'stat_pull_log_*.txt')):\n",
    "        file_modified_time = datetime.fromtimestamp(os.path.getmtime(log_file))\n",
    "        if current_time - file_modified_time > timedelta(days=days_to_keep):\n",
    "            os.remove(log_file)\n",
    "\n",
    "def persist_final_dataset(new_data: pd.DataFrame, seasons_loaded: list[str],\n",
    "                          *, output_base: Path, debug: bool = False,\n",
    "                          numeric_atol: float = 1e-6, numeric_rtol: float = 1e-9,\n",
    "                          max_print: int = 15, mean_tol_pct: float = 0.001) -> None:\n",
    "    from salary_nba_data_pull.data_utils import prune_end_columns\n",
    "\n",
    "    final_parquet = output_base / \"nba_player_data_final_inflated.parquet\"\n",
    "    join_keys = [\"Season\", \"Player\"]\n",
    "\n",
    "    # -- NEW: prune end-only columns BEFORE diffing/writing\n",
    "    new_data = prune_end_columns(new_data, debug=debug)\n",
    "\n",
    "    old_master = (pd.read_parquet(final_parquet)\n",
    "                  if final_parquet.exists() else\n",
    "                  pd.DataFrame(columns=new_data.columns))\n",
    "\n",
    "    # -- NEW: also prune any legacy columns in the old master for a fair diff\n",
    "    if not old_master.empty:\n",
    "        old_master = prune_end_columns(old_master, debug=debug)\n",
    "\n",
    "    for df in (old_master, new_data):\n",
    "        for k in join_keys:\n",
    "            if k in df.columns:\n",
    "                df[k] = df[k].astype(str).str.strip()\n",
    "\n",
    "    old_slice = old_master.merge(\n",
    "        pd.DataFrame({\"Season\": seasons_loaded}).drop_duplicates(),\n",
    "        on=\"Season\", how=\"inner\").reset_index(drop=True)\n",
    "    new_slice = new_data.reset_index(drop=True)\n",
    "\n",
    "    equal, summary, diff_cells, cols_add, cols_rem, mean_diffs = \\\n",
    "        _diff_report(old_slice, new_slice, key_cols=join_keys,\n",
    "                     numeric_atol=numeric_atol, numeric_rtol=numeric_rtol)\n",
    "\n",
    "    # Special case: if old_slice is empty but new_slice has data, we should write\n",
    "    if len(old_slice) == 0 and len(new_slice) > 0:\n",
    "        equal = False\n",
    "        if debug:\n",
    "            print(\"[persist] Creating new master parquet with fresh data\")\n",
    "\n",
    "    if equal:\n",
    "        if debug:\n",
    "            print(\"[persist] No changes detected – master Parquet left untouched\")\n",
    "        return\n",
    "\n",
    "    audits = output_base / \"audits\"; audits.mkdir(parents=True, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    if cols_add or cols_rem:\n",
    "        pd.DataFrame({\"added\": [cols_add], \"removed\": [cols_rem]}\n",
    "                     ).to_csv(audits / f\"column_changes_{ts}.csv\", index=False)\n",
    "    if not mean_diffs.empty:\n",
    "        mean_diffs.to_csv(audits / f\"mean_diffs_{ts}.csv\", index=False)\n",
    "    if not diff_cells.empty:\n",
    "        diff_cells.to_csv(audits / f\"value_diffs_{ts}.csv\", index=False)\n",
    "\n",
    "    # ----- rewrite master -----\n",
    "    union_cols = sorted(set(old_master.columns) | set(new_data.columns))\n",
    "    remover = old_master.merge(\n",
    "        pd.DataFrame({\"Season\": seasons_loaded}), on=\"Season\",\n",
    "        how=\"left\", indicator=True)\n",
    "    remover = remover[remover[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
    "    remover = remover.reindex(columns=union_cols)\n",
    "    new_slice = new_slice.reindex(columns=union_cols)\n",
    "\n",
    "    updated_master = pd.concat([remover, new_slice], ignore_index=True)\\\n",
    "                       .sort_values(join_keys).reset_index(drop=True)\n",
    "    updated_master.to_parquet(final_parquet, index=False)\n",
    "    if debug: print(f\"[persist] Master Parquet updated – {summary}\")\n",
    "\n",
    "def main(start_year: int,\n",
    "         end_year: int,\n",
    "         player_filter: str = \"all\",\n",
    "         min_avg_minutes: float = 10,    # NEW default: 10 minutes\n",
    "         min_shot_attempts: int = 50,    # NEW: filter on shot attempts\n",
    "         nan_filter: bool = False,       # NEW: enable threshold-aware NaN filtering\n",
    "         nan_filter_percentage: float = 0.01,  # NEW: threshold for low-missing columns\n",
    "         debug: bool = False,\n",
    "         small_debug: bool = False,      # --- NEW\n",
    "         workers: int = 8,\n",
    "         overwrite: bool = False,\n",
    "         output_base: str | Path = DATA_PROCESSED_DIR) -> None:\n",
    "    \"\"\"\n",
    "    Entry point for NBA data processing pipeline.\n",
    "    \n",
    "    NaN Filtering Options:\n",
    "    - nan_filter=False (default): Legacy behavior - drop any row with any NaN\n",
    "    - nan_filter=True: Threshold-aware filtering - only drop rows for columns with \n",
    "      NaN rate ≤ nan_filter_percentage\n",
    "    \n",
    "    Debug Options:\n",
    "    - small_debug=True: Print only high-signal info\n",
    "    - debug=True: Full verbose output\n",
    "    - If both debug and small_debug are True, debug wins (full noise)\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    output_base = Path(output_base)\n",
    "\n",
    "\n",
    "    log_dir = output_base.parent / \"stat_pull_output\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remove_old_logs(log_dir)\n",
    "\n",
    "    log_file = log_dir / f\"stat_pull_log_{get_timestamp()}.txt\"\n",
    "    logging.basicConfig(filename=log_file,\n",
    "                        level=logging.DEBUG if debug else logging.INFO,\n",
    "                        format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    updated = update_data(None, start_year, end_year,\n",
    "                          player_filter=player_filter,\n",
    "                          min_avg_minutes=min_avg_minutes,\n",
    "                          min_shot_attempts=min_shot_attempts,  # NEW: pass shot attempts filter\n",
    "                          nan_filter=nan_filter,              # NEW: pass NaN filter flag\n",
    "                          nan_filter_percentage=nan_filter_percentage,  # NEW: pass NaN filter threshold\n",
    "                          debug=debug,\n",
    "                          small_debug=small_debug,          # --- NEW\n",
    "                          max_workers=workers,\n",
    "                          output_base=str(output_base),\n",
    "                          overwrite=overwrite)\n",
    "\n",
    "    if not small_debug:  # keep your old prints in full/quiet modes\n",
    "        print(f\"✔ Completed pull: {len(updated):,} rows added\")\n",
    "\n",
    "    if not updated.empty:\n",
    "        # — Skip salary‐cap entirely —\n",
    "        # Validate only core columns (Season,Player,Team)\n",
    "        from salary_nba_data_pull.data_utils import validate_data\n",
    "        updated = validate_data(updated, name=\"player_dataset\", save_reports=True)\n",
    "\n",
    "        # Persist master\n",
    "        seasons_this_run = sorted(updated[\"Season\"].unique().tolist())\n",
    "        persist_final_dataset(\n",
    "            updated,\n",
    "            seasons_loaded=seasons_this_run,\n",
    "            output_base=output_base,\n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "    if not small_debug:\n",
    "        print(f\"Process finished in {time.time() - t0:.1f} s — log: {log_file}\")\n",
    "    else:\n",
    "        # minimal closing line\n",
    "        print(f\"Done in {time.time() - t0:.1f}s. Log: {log_file}\")\n",
    "        \n",
    "# ----------------------------------------------------------------------\n",
    "# argparse snippet\n",
    "if __name__ == \"__main__\":\n",
    "    cur = datetime.now().year\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--start_year\", type=int, default=cur-1)\n",
    "    p.add_argument(\"--end_year\",   type=int, default=cur)\n",
    "    p.add_argument(\"--player_filter\", default=\"all\")\n",
    "    p.add_argument(\"--min_avg_minutes\", type=float, default=10,\n",
    "                   help=\"Filter out players averaging < this many minutes per game\")\n",
    "    p.add_argument(\"--min_shot_attempts\", type=int, default=50,\n",
    "                   help=\"Filter out players with fewer than this many total shot attempts (FGA+FTA)\")\n",
    "    p.add_argument(\"--nan_filter\", action=\"store_true\",\n",
    "                   help=\"Enable threshold-aware NaN filtering (instead of dropping all rows with any NaN)\")\n",
    "    p.add_argument(\"--nan_filter_percentage\", type=float, default=0.01,\n",
    "                   help=\"Threshold for low-missing columns when nan_filter=True (default 1%%)\")\n",
    "    p.add_argument(\"--debug\", action=\"store_true\")\n",
    "    p.add_argument(\"--small_debug\", action=\"store_true\")   # --- NEW\n",
    "    p.add_argument(\"--workers\", type=int, default=8)\n",
    "    p.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "    p.add_argument(\"--output_base\",\n",
    "                   default=str(DATA_PROCESSED_DIR),\n",
    "                   help=\"Destination root for parquet + csv outputs\")\n",
    "    args = p.parse_args()\n",
    "    main(**vars(args))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[notebook_helper] sys.path[0:3]=['C:\\\\docker_projects\\\\coach_analysis', 'C:\\\\Users\\\\ghadf\\\\AppData\\\\Roaming\\\\uv\\\\python\\\\cpython-3.10.17-windows-x86_64-none\\\\python310.zip', 'C:\\\\Users\\\\ghadf\\\\AppData\\\\Roaming\\\\uv\\\\python\\\\cpython-3.10.17-windows-x86_64-none\\\\DLLs']\n",
      "✅ salary_nba_data_pull imported successfully\n",
      "start_year      default=<class 'inspect._empty'>  kind=POSITIONAL_OR_KEYWORD\n",
      "end_year        default=<class 'inspect._empty'>  kind=POSITIONAL_OR_KEYWORD\n",
      "player_filter   default='all'  kind=POSITIONAL_OR_KEYWORD\n",
      "min_avg_minutes default=10  kind=POSITIONAL_OR_KEYWORD\n",
      "min_shot_attempts default=50  kind=POSITIONAL_OR_KEYWORD\n",
      "nan_filter      default=False  kind=POSITIONAL_OR_KEYWORD\n",
      "nan_filter_percentage default=0.01  kind=POSITIONAL_OR_KEYWORD\n",
      "debug           default=False  kind=POSITIONAL_OR_KEYWORD\n",
      "small_debug     default=False  kind=POSITIONAL_OR_KEYWORD\n",
      "workers         default=8  kind=POSITIONAL_OR_KEYWORD\n",
      "overwrite       default=False  kind=POSITIONAL_OR_KEYWORD\n",
      "output_base     default=WindowsPath('C:/docker_projects/coach_analysis/data/new_processed')  kind=POSITIONAL_OR_KEYWORD\n",
      "[historical_pull] 2010-2024  nan_filter=True nan_filter_percentage=0.02  other_kwargs={'workers': 6, 'min_avg_minutes': 10, 'min_shot_attempts': 50, 'overwrite': True, 'debug': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[update_data] Starting season 2010-11\n",
      "[fetch_season_players] 488 players for 2010-11\n",
      "[update_data] fetched 488 players for 2010-11\n",
      "[update_data] processing 488 players after filter\n",
      "[update_data][WARN] no data for player 'blake ahearn' in 2010-11\n",
      "[update_data][WARN] no data for player 'morris almond' in 2010-11\n",
      "[update_data][WARN] no data for player 'alan anderson' in 2010-11\n",
      "[update_data][WARN] no data for player 'jeff ayres' in 2010-11\n",
      "[update_data][WARN] no data for player 'kelenna azubuike' in 2010-11\n",
      "[update_data][WARN] no data for player 'marqus blakely' in 2010-11\n",
      "[update_data][WARN] no data for player 'bobby brown' in 2010-11\n",
      "[update_data][WARN] no data for player 'da'sean butler' in 2010-11\n",
      "[update_data][WARN] no data for player 'will conroy' in 2010-11\n",
      "[update_data][WARN] no data for player 'eddy curry' in 2010-11\n",
      "[update_data][WARN] no data for player 'josh davis' in 2010-11\n",
      "[update_data][WARN] no data for player 'andre emmett' in 2010-11\n",
      "[update_data][WARN] no data for player 'mickael gelabale' in 2010-11\n",
      "[update_data][WARN] no data for player 'gerald green' in 2010-11\n",
      "[update_data][WARN] no data for player 'larry hughes' in 2010-11\n",
      "[update_data][WARN] no data for player 'mike james' in 2010-11\n",
      "[update_data][WARN] no data for player 'jonas jerebko' in 2010-11\n",
      "[update_data][WARN] no data for player 'dwayne jones' in 2010-11\n",
      "[update_data][WARN] no data for player 'mikki moore' in 2010-11\n",
      "[update_data][WARN] no data for player 'greg oden' in 2010-11\n",
      "[update_data][WARN] no data for player 'daniel orton' in 2010-11\n",
      "[update_data][WARN] no data for player 'jannero pargo' in 2010-11\n",
      "[update_data][WARN] no data for player 'shavlik randolph' in 2010-11\n",
      "[update_data][WARN] no data for player 'magnum rolle' in 2010-11\n",
      "[update_data][WARN] no data for player 'sergio rodriguez' in 2010-11\n",
      "[update_data][WARN] no data for player 'james singleton' in 2010-11\n",
      "[update_data][WARN] no data for player 'greg stiemsma' in 2010-11\n",
      "[update_data][WARN] no data for player 'p.j. tucker' in 2010-11\n",
      "[update_data][WARN] no data for player 'jamaal tinsley' in 2010-11\n",
      "[update_data][WARN] no data for player 'robert vaden' in 2010-11\n",
      "[update_data][WARN] no data for player 'rasheed wallace' in 2010-11\n",
      "[update_data][WARN] no data for player 'darryl watkins' in 2010-11\n",
      "[update_data][WARN] no data for player 'terrico white' in 2010-11\n",
      "[update_data][WARN] no data for player 'james white' in 2010-11\n",
      "[update_data][WARN] no data for player 'elliot williams' in 2010-11\n",
      "[update_data][WARN] no data for player 'sean williams' in 2010-11\n",
      "[update_data] 2010-11 → DataFrame with 452 rows\n",
      "[filter-early] 2010-11: MP ≥ 10  → 452→448 rows\n",
      "[attach_wins_losses] 2010-11 W/L null% = 0.00\n",
      "[update_data] 2010-11 before derived metrics: 32 columns\n",
      "[update_data] 2010-11 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 68\n",
      "[calculate_percentages] FTA==0 count: 12\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2011_advanced.html\n",
      "[validate_name_encoding] 2010-11: Found 0/4 expected names\n",
      "  Missing: ['Luka Dončić', 'Nikola Jokić', 'Dennis Schröder', 'Bojan Bogdanović']\n",
      "  Sample actual names: ['Monta Ellis', 'LaMarcus Aldridge', 'Luol Deng', 'Dorell Wright', 'Blake Griffin', 'LeBron James', 'Kevin Durant', 'Pau Gasol', 'Derrick Rose', 'Al Jefferson']\n",
      "[adv] WARNING: [validate_name_encoding] 2010-11: Critical encoding issues detected. Missing 4 expected Unicode names: ['Luka Dončić', 'Nikola Jokić', 'Dennis Schröder', 'Bojan Bogdanović']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:   7%|▋         | 1/15 [00:05<01:22,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2010-11: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2010-11: matched 444/448 players (99.1 %)\n",
      "  unmatched sample: ['OMER ASIK', 'POOH JETER', 'HAMADY NDIAYE', 'JR SMITH']\n",
      "[chk:2010-11:post-consolidate] rows=448  cols=77\n",
      "[chk:2010-11:post-derived] rows=448  cols=77\n",
      "           Player  Season Team  Age  GP  GS    MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID      Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%    player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "    ALEXIS AJINCA 2010-11  TOR 23.0  24   0 265.0  114  47  101   9   27  11   15   61    8    8   14   17  60   12   49    201582 1610612761        Center                 8  38   74 50.99 51.35    22      60 46.53     False 33.33     False 73.33 52.97       15.49        1.09        8.29    6541    1915    1100   3051     947    2272    3219 19173.0    1717    8483.60       124.60  1.47            1.27             0.20               0.03         1.50             1.56 alexis ajinca 12.2 0.256 0.135   5.9  21.5  13.6   4.8   1.7   4.2  11.3  0.0  0.4 0.4  0.052  -3.8   0.0 -3.8  -0.2\n",
      "     MAURICE AGER 2010-11  MIN 27.0   4   0  29.0   15   6   11   3    4   0    0    2    1    1    0    4   4    0    2    200772 1610612750 Forward-Guard                 4   3    7 68.18 42.86    17      65 54.55     False 75.00      True  0.00 68.18       18.62        1.24        2.48    6992    1967    1350   3083    1084    2551    3635 19713.0    1644    9207.48        15.00  0.16            0.12             0.04               0.00         0.16             0.17  maurice ager 10.1 0.364 0.000   0.0   7.9   3.8   6.0   1.7   0.0  26.7  0.0  0.0 0.0 -0.015  -0.9   0.2 -0.7   0.0\n",
      "      JEFF ADRIEN 2010-11  GSW 25.0  23   0 196.0   57  23   54   0    0  11   19   58   10    4    5    9  28   23   35    202399 1610612744                               6  23   54 42.59 42.59    36      46 42.59      True  0.00     False 57.89 45.70       10.47        1.84       10.65    6943    1660    1137   3200     945    2323    3268 19517.0    1836    8810.40        71.36  0.81            0.71             0.10               0.04         0.85             0.92   jeff adrien 11.3 0.000 0.352  13.0  20.7  16.8   7.3   1.0   1.9  12.6  0.1  0.1 0.2  0.051  -3.3  -2.0 -5.3  -0.2\n",
      "[guard_advanced_null_regress] 2010-11: all-advanced-NA rows: new=0/448  prev=0\n",
      "[diagnose_advanced_nulls] 2010-11: rows with ALL BBR-only adv cols NA = 4/448\n",
      "  head sample (5): ['OMER ASIK', 'POOH JETER', 'HAMADY NDIAYE', 'JR SMITH']\n",
      "  per-col nulls (BBR-only): {'PER': 4, '3PAr': 4, 'BPM': 4, 'DBPM': 4, 'OBPM': 4, 'WS/48': 4, 'WS': 4, 'DWS': 4, 'OWS': 4, 'TOV%': 4, 'BLK%': 4, 'STL%': 4, 'AST%': 4, 'TRB%': 4, 'DRB%': 4, 'ORB%': 4, 'FTr': 4, 'VORP': 4}\n",
      "[update_data] 2010-11 after derived metrics: 77 columns\n",
      "[update_data] 2010-11 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2010-11: MP ≥ 10  → 448→448 rows\n",
      "[filter-shots] 2010-11: ≥50 attempts → 448→399 rows\n",
      "[nan_filter] 2010-11: dropped 3 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2010-11: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2010-11\\part.parquet\n",
      "[update_data] Starting season 2011-12\n",
      "[fetch_season_players] 507 players for 2011-12\n",
      "[update_data] fetched 507 players for 2011-12\n",
      "[update_data] processing 507 players after filter\n",
      "[update_data][WARN] no data for player 'alexis ajinca' in 2011-12\n",
      "[update_data][WARN] no data for player 'hilton armstrong' in 2011-12\n",
      "[update_data][WARN] no data for player 'darrell arthur' in 2011-12\n",
      "[update_data][WARN] no data for player 'aaron brooks' in 2011-12\n",
      "[update_data][WARN] no data for player 'bobby brown' in 2011-12\n",
      "[update_data][WARN] no data for player 'will conroy' in 2011-12\n",
      "[update_data][WARN] no data for player 'joey dorsey' in 2011-12\n",
      "[update_data][WARN] no data for player 'chris douglas-roberts' in 2011-12\n",
      "[update_data][WARN] no data for player 'melvin ely' in 2011-12\n",
      "[update_data][WARN] no data for player 'mickael gelabale' in 2011-12\n",
      "[update_data][WARN] no data for player 'jeff green' in 2011-12\n",
      "[update_data][WARN] no data for player 'mike harris' in 2011-12\n",
      "[update_data][WARN] no data for player 'othyus jeffers' in 2011-12\n",
      "[update_data][WARN] no data for player 'dwayne jones' in 2011-12\n",
      "[update_data][WARN] no data for player 'andrei kirilenko' in 2011-12\n",
      "[update_data][WARN] no data for player 'greg oden' in 2011-12\n",
      "[update_data][WARN] no data for player 'josh powell' in 2011-12\n",
      "[update_data][WARN] no data for player 'chris quinn' in 2011-12\n",
      "[update_data][WARN] no data for player 'shavlik randolph' in 2011-12\n",
      "[update_data][WARN] no data for player 'sergio rodriguez' in 2011-12\n",
      "[update_data][WARN] no data for player 'brandon roy' in 2011-12\n",
      "[update_data][WARN] no data for player 'mustafa shakur' in 2011-12\n",
      "[update_data][WARN] no data for player 'diamon simpson' in 2011-12\n",
      "[update_data][WARN] no data for player 'garrett temple' in 2011-12\n",
      "[update_data][WARN] no data for player 'p.j. tucker' in 2011-12\n",
      "[update_data][WARN] no data for player 'sasha vujacic' in 2011-12\n",
      "[update_data][WARN] no data for player 'rasheed wallace' in 2011-12\n",
      "[update_data][WARN] no data for player 'sonny weems' in 2011-12\n",
      "[update_data][WARN] no data for player 'james white' in 2011-12\n",
      "[update_data] 2011-12 → DataFrame with 478 rows\n",
      "[filter-early] 2011-12: MP ≥ 10  → 478→474 rows\n",
      "[attach_wins_losses] 2011-12 W/L null% = 0.00\n",
      "[update_data] 2011-12 before derived metrics: 32 columns\n",
      "[update_data] 2011-12 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 76\n",
      "[calculate_percentages] FTA==0 count: 11\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2012_advanced.html\n",
      "[validate_name_encoding] 2011-12: Found 0/4 expected names\n",
      "  Missing: ['Luka Dončić', 'Nikola Jokić', 'Dennis Schröder', 'Bojan Bogdanović']\n",
      "  Sample actual names: ['Kevin Durant', 'Pau Gasol', 'Rudy Gay', 'Blake Griffin', 'John Wall', 'Marc Gasol', 'Brandon Jennings', 'Russell Westbrook', 'Josh Smith', 'LeBron James']\n",
      "[adv] WARNING: [validate_name_encoding] 2011-12: Critical encoding issues detected. Missing 4 expected Unicode names: ['Luka Dončić', 'Nikola Jokić', 'Dennis Schröder', 'Bojan Bogdanović']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  13%|█▎        | 2/15 [00:10<01:03,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2011-12: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2011-12: matched 472/474 players (99.6 %)\n",
      "  unmatched sample: ['OMER ASIK', 'JR SMITH']\n",
      "[chk:2011-12:post-consolidate] rows=474  cols=77\n",
      "[chk:2011-12:post-derived] rows=474  cols=77\n",
      "           Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%    player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS   WS  WS/48  OBPM  DBPM   BPM  VORP\n",
      "      JEFF ADRIEN 2011-12  HOU 26.0   8   0   63.0   21   7   16   0    0   7   12   22    1    0    2    2  13    5   17    202399 1610612745                          6   7   16 43.75 43.75    34      32 43.75      True  0.00     False 58.33 49.34       12.00        0.57       12.57    5316    1249     884   2393     713    1851    2564 15268.0    1334    6749.56        23.28  0.34            0.32             0.03               0.01         0.36             0.36   jeff adrien 11.2 0.000 0.750   9.1  31.6  20.3   2.4   0.0   2.4   8.6  0.1  0.1  0.1  0.106  -4.0  -0.2  -4.3   0.0\n",
      "    ARRON AFFLALO 2011-12  DEN 26.0  62  62 2086.0  943 329  699  88  221 197  247  197  149   36   13   85 134   40  157    201167 1610612743    Guard                12 241  478 53.36 50.42    38      28 47.07     False 39.82     False 79.76 58.38       16.27        2.57        3.40    5246    1709     966   2473     702    2023    2725 15608.0    1572    6963.96       892.68 12.82           11.60             1.22               0.79        13.61            14.96 arron afflalo 14.7 0.316 0.353   2.3   8.5   5.5  11.1   0.9   0.5   9.5  4.7  0.6  5.3  0.121   1.4  -1.2   0.1   1.1\n",
      "     BLAKE AHEARN 2011-12  UTA 28.0   4   0   30.0   10   4   14   2    9   0    0    2    1    0    0    5   4    0    2    201336 1610612762    Guard                 4   2    5 35.71 40.00    36      30 28.57     False 22.22      True  0.00 35.71       12.00        1.20        2.40    5531    1668     896   2523     861    2055    2916 16166.0    1439    7160.92        19.00  0.27            0.20             0.07               0.01         0.28             0.28  blake ahearn -7.3 0.643 0.000   0.0   7.7   3.8   5.2   0.0   0.0  26.3 -0.2  0.0 -0.2 -0.315 -12.7  -6.8 -19.5  -0.1\n",
      "[guard_advanced_null_regress] 2011-12: all-advanced-NA rows: new=0/474  prev=0\n",
      "[diagnose_advanced_nulls] 2011-12: rows with ALL BBR-only adv cols NA = 2/474\n",
      "  head sample (5): ['OMER ASIK', 'JR SMITH']\n",
      "  per-col nulls (BBR-only): {'PER': 2, '3PAr': 2, 'BPM': 2, 'DBPM': 2, 'OBPM': 2, 'WS/48': 2, 'WS': 2, 'DWS': 2, 'OWS': 2, 'TOV%': 2, 'BLK%': 2, 'STL%': 2, 'AST%': 2, 'TRB%': 2, 'DRB%': 2, 'ORB%': 2, 'FTr': 2, 'VORP': 2}\n",
      "[update_data] 2011-12 after derived metrics: 77 columns\n",
      "[update_data] 2011-12 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2011-12: MP ≥ 10  → 474→474 rows\n",
      "[filter-shots] 2011-12: ≥50 attempts → 474→415 rows\n",
      "[nan_filter] 2011-12: dropped 2 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2011-12: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2011-12\\part.parquet\n",
      "[update_data] Starting season 2012-13\n",
      "[fetch_season_players] 503 players for 2012-13\n",
      "[update_data] fetched 503 players for 2012-13\n",
      "[update_data] processing 503 players after filter\n",
      "[update_data][WARN] no data for player 'alexis ajinca' in 2012-13\n",
      "[update_data][WARN] no data for player 'hilton armstrong' in 2012-13\n",
      "[update_data][WARN] no data for player 'bobby brown' in 2012-13\n",
      "[update_data][WARN] no data for player 'rasual butler' in 2012-13\n",
      "[update_data][WARN] no data for player 'andrew bynum' in 2012-13\n",
      "[update_data][WARN] no data for player 'joey dorsey' in 2012-13\n",
      "[update_data][WARN] no data for player 'melvin ely' in 2012-13\n",
      "[update_data][WARN] no data for player 'jordan farmar' in 2012-13\n",
      "[update_data][WARN] no data for player 'channing frye' in 2012-13\n",
      "[update_data][WARN] no data for player 'ryan gomes' in 2012-13\n",
      "[update_data][WARN] no data for player 'donte greene' in 2012-13\n",
      "[update_data][WARN] no data for player 'justin harper' in 2012-13\n",
      "[update_data][WARN] no data for player 'manny harris' in 2012-13\n",
      "[update_data][WARN] no data for player 'mike harris' in 2012-13\n",
      "[update_data][WARN] no data for player 'lester hudson' in 2012-13\n",
      "[update_data][WARN] no data for player 'othyus jeffers' in 2012-13\n",
      "[update_data][WARN] no data for player 'dwayne jones' in 2012-13\n",
      "[update_data][WARN] no data for player 'jerome jordan' in 2012-13\n",
      "[update_data][WARN] no data for player 'tracy mcgrady' in 2012-13\n",
      "[update_data][WARN] no data for player 'jerel mcneal' in 2012-13\n",
      "[update_data][WARN] no data for player 'hamady ndiaye' in 2012-13\n",
      "[update_data][WARN] no data for player 'greg oden' in 2012-13\n",
      "[update_data][WARN] no data for player 'josh powell' in 2012-13\n",
      "[update_data][WARN] no data for player 'sergio rodriguez' in 2012-13\n",
      "[update_data][WARN] no data for player 'derrick rose' in 2012-13\n",
      "[update_data][WARN] no data for player 'willie reed' in 2012-13\n",
      "[update_data][WARN] no data for player 'mustafa shakur' in 2012-13\n",
      "[update_data][WARN] no data for player 'xavier silas' in 2012-13\n",
      "[update_data][WARN] no data for player 'sasha vujacic' in 2012-13\n",
      "[update_data][WARN] no data for player 'henry walker' in 2012-13\n",
      "[update_data][WARN] no data for player 'sonny weems' in 2012-13\n",
      "[update_data][WARN] no data for player 'hassan whiteside' in 2012-13\n",
      "[update_data][WARN] no data for player 'elliot williams' in 2012-13\n",
      "[update_data][WARN] no data for player 'royce white' in 2012-13\n",
      "[update_data][WARN] no data for player 'shawne williams' in 2012-13\n",
      "[update_data] 2012-13 → DataFrame with 468 rows\n",
      "[filter-early] 2012-13: MP ≥ 10  → 468→458 rows\n",
      "[attach_wins_losses] 2012-13 W/L null% = 0.00\n",
      "[update_data] 2012-13 before derived metrics: 32 columns\n",
      "[update_data] 2012-13 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 57\n",
      "[calculate_percentages] FTA==0 count: 4\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2013_advanced.html\n",
      "[validate_name_encoding] 2012-13: Found 0/4 expected names\n",
      "  Missing: ['Luka Dončić', 'Nikola Jokić', 'Dennis Schröder', 'Bojan Bogdanović']\n",
      "  Sample actual names: ['Damian Lillard', 'Kevin Durant', 'Monta Ellis', 'DeMar DeRozan', 'Kobe Bryant', 'James Harden', 'Stephen Curry', 'Paul George', 'Klay Thompson', 'Jrue Holiday']\n",
      "[adv] WARNING: [validate_name_encoding] 2012-13: Critical encoding issues detected. Missing 4 expected Unicode names: ['Luka Dončić', 'Nikola Jokić', 'Dennis Schröder', 'Bojan Bogdanović']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  20%|██        | 3/15 [00:13<00:49,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2012-13: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2012-13: matched 455/458 players (99.3 %)\n",
      "  unmatched sample: ['OMER ASIK', 'JR SMITH', 'JEFFERY TAYLOR']\n",
      "[chk:2012-13:post-consolidate] rows=458  cols=77\n",
      "[chk:2012-13:post-derived] rows=458  cols=77\n",
      "           Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero  3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%    player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "      JEFF ADRIEN 2012-13  CHA 27.0  52   5  713.0  209  72  168   0    2  65  100  196   36   18   27   32  80   68  128    202399 1610612766                          6  72  166 42.86 43.37    21      61 42.86     False  0.0     False 65.00 49.29       10.55        1.82        9.90    6459    2005    1036   2727     865    2253    3118 18997.0    1515    8377.20       244.00  2.91            2.53             0.38               0.16         3.07             3.34   jeff adrien 13.4 0.012 0.595  10.6  21.2  15.7   8.3   1.3   3.1  13.1  0.5  0.4 1.0  0.064  -2.1  -0.9 -3.0  -0.2\n",
      "    ARRON AFFLALO 2012-13  ORL 27.0  64  64 2307.0 1057 397  905  72  240 191  223  239  206   40   11  138 137   29  210    201167 1610612753    Guard                12 325  665 47.85 48.87    20      62 43.87     False 30.0     False 85.65 52.69       16.49        3.21        3.73    6269    1178    1044   2816     817    2392    3209 18067.0    1651    7831.32      1141.12 14.57           12.81             1.76               0.97        15.54            17.20 arron afflalo 13.0 0.265 0.246   1.4  10.3   5.8  14.6   0.9   0.4  12.1  1.5  0.5 2.0  0.042  -1.1  -1.8 -2.9  -0.5\n",
      "     COLE ALDRICH 2012-13  HOU 24.0  30   0  213.0   50  23   43   0    0   4    9   57    6    3    9   14  41   12   45    202332 1610612745   Center                 9  23   43 53.49 53.49    45      37 53.49      True  0.0     False 44.44 53.24        8.45        1.01        9.63    6537    2039    1253   3023     872    2569    3441 19012.0    1856    8687.16        60.96  0.70            0.54             0.16               0.03         0.73             0.77  cole aldrich 11.1 0.000 0.250   8.7  26.7  17.7   3.4   0.7   4.6  20.6  0.1  0.4 0.6  0.070  -3.5   0.6 -2.9  -0.1\n",
      "[guard_advanced_null_regress] 2012-13: all-advanced-NA rows: new=0/458  prev=0\n",
      "[diagnose_advanced_nulls] 2012-13: rows with ALL BBR-only adv cols NA = 3/458\n",
      "  head sample (5): ['OMER ASIK', 'JR SMITH', 'JEFFERY TAYLOR']\n",
      "  per-col nulls (BBR-only): {'PER': 3, '3PAr': 3, 'BPM': 3, 'DBPM': 3, 'OBPM': 3, 'WS/48': 3, 'WS': 3, 'DWS': 3, 'OWS': 3, 'TOV%': 3, 'BLK%': 3, 'STL%': 3, 'AST%': 3, 'TRB%': 3, 'DRB%': 3, 'ORB%': 3, 'FTr': 3, 'VORP': 3}\n",
      "[update_data] 2012-13 after derived metrics: 77 columns\n",
      "[update_data] 2012-13 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2012-13: MP ≥ 10  → 458→458 rows\n",
      "[filter-shots] 2012-13: ≥50 attempts → 458→414 rows\n",
      "[nan_filter] 2012-13: dropped 3 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2012-13: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2012-13\\part.parquet\n",
      "[update_data] Starting season 2013-14\n",
      "[fetch_season_players] 512 players for 2013-14\n",
      "[update_data] fetched 512 players for 2013-14\n",
      "[update_data] processing 512 players after filter\n",
      "[update_data][WARN] no data for player 'earl barron' in 2013-14\n",
      "[update_data][WARN] no data for player 'bobby brown' in 2013-14\n",
      "[update_data][WARN] no data for player 'carlos delfino' in 2013-14\n",
      "[update_data][WARN] no data for player 'joey dorsey' in 2013-14\n",
      "[update_data][WARN] no data for player 'festus ezeli' in 2013-14\n",
      "[update_data][WARN] no data for player 'danilo gallinari' in 2013-14\n",
      "[update_data][WARN] no data for player 'andrew goudelock' in 2013-14\n",
      "[update_data][WARN] no data for player 'justin harper' in 2013-14\n",
      "[update_data][WARN] no data for player 'brendan haywood' in 2013-14\n",
      "[update_data][WARN] no data for player 'justin holiday' in 2013-14\n",
      "[update_data][WARN] no data for player 'lester hudson' in 2013-14\n",
      "[update_data][WARN] no data for player 'dahntay jones' in 2013-14\n",
      "[update_data][WARN] no data for player 'grant jerrett' in 2013-14\n",
      "[update_data][WARN] no data for player 'jerome jordan' in 2013-14\n",
      "[update_data][WARN] no data for player 'malcolm lee' in 2013-14\n",
      "[update_data][WARN] no data for player 'scott machado' in 2013-14\n",
      "[update_data][WARN] no data for player 'jerel mcneal' in 2013-14\n",
      "[update_data][WARN] no data for player 'lamar odom' in 2013-14\n",
      "[update_data][WARN] no data for player 'emeka okafor' in 2013-14\n",
      "[update_data][WARN] no data for player 'nerlens noel' in 2013-14\n",
      "[update_data][WARN] no data for player 'jeremy pargo' in 2013-14\n",
      "[update_data][WARN] no data for player 'willie reed' in 2013-14\n",
      "[update_data][WARN] no data for player 'jason richardson' in 2013-14\n",
      "[update_data][WARN] no data for player 'sergio rodriguez' in 2013-14\n",
      "[update_data][WARN] no data for player 'xavier silas' in 2013-14\n",
      "[update_data][WARN] no data for player 'sebastian telfair' in 2013-14\n",
      "[update_data][WARN] no data for player 'tyrus thomas' in 2013-14\n",
      "[update_data][WARN] no data for player 'henry walker' in 2013-14\n",
      "[update_data][WARN] no data for player 'sonny weems' in 2013-14\n",
      "[update_data][WARN] no data for player 'hassan whiteside' in 2013-14\n",
      "[update_data][WARN] no data for player 'damien wilkins' in 2013-14\n",
      "[update_data] 2013-14 → DataFrame with 481 rows\n",
      "[filter-early] 2013-14: MP ≥ 10  → 481→473 rows\n",
      "[attach_wins_losses] 2013-14 W/L null% = 0.00\n",
      "[update_data] 2013-14 before derived metrics: 32 columns\n",
      "[update_data] 2013-14 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 63\n",
      "[calculate_percentages] FTA==0 count: 15\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2014_advanced.html\n",
      "[validate_name_encoding] 2013-14: Found 1/4 expected names\n",
      "  Missing: ['Luka Dončić', 'Nikola Jokić', 'Bojan Bogdanović']\n",
      "  Sample actual names: ['Kevin Durant', 'Monta Ellis', 'DeMar DeRozan', 'Carmelo Anthony', 'John Wall', 'Nicolas Batum', 'Damian Lillard', 'LeBron James', 'Paul George', 'DeAndre Jordan']\n",
      "[adv] WARNING: [validate_name_encoding] 2013-14: Critical encoding issues detected. Missing 3 expected Unicode names: ['Luka Dončić', 'Nikola Jokić', 'Bojan Bogdanović']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  27%|██▋       | 4/15 [00:16<00:42,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2013-14: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2013-14: matched 468/473 players (98.9 %)\n",
      "  unmatched sample: ['OMER ASIK', 'VITOR FAVERANI', 'HAMADY NDIAYE', 'JR SMITH', 'JEFFERY TAYLOR']\n",
      "[chk:2013-14:post-consolidate] rows=473  cols=77\n",
      "[chk:2013-14:post-derived] rows=473  cols=77\n",
      "          Player  Season Team  Age  GP  GS    MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero  3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%   player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "      QUINCY ACY 2013-14  SAC 23.0  56   0 786.0  152  60  127   2   10  30   45  201   24   19   23   28 114   67  134    203112 1610612758  Forward                 7  58  117 48.03 49.57    28      54 47.24     False 20.0     False 66.67 51.77        6.96        1.10        9.21    6204    2093    1104   2790     850    2354    3204 17571.0    1352    8228.92       174.80  2.12            1.78             0.34               0.11         2.23             2.42   quincy acy 10.1 0.106 0.376   9.5  19.5  14.4   4.9   1.4   2.5  15.4  0.6  0.9 1.5  0.086  -2.5   0.8 -1.7   0.1\n",
      "     JEFF ADRIEN 2013-14  MIL 28.0  28  12 705.0  305 121  235   0    0  63   94  218   31   17   21   31  73   69  149    202399 1610612749                          6 121  235 51.49 51.49    15      67 51.49      True  0.0     False 67.02 55.18       15.57        1.58       11.13    6428    1673    1117   2809     954    2329    3283 18961.0    1625    8281.12       307.36  3.71            3.34             0.37               0.14         3.85             4.09  jeff adrien 17.4 0.000 0.433  11.8  24.8  18.2   6.8   1.3   3.0  10.6  1.6  1.1 2.7  0.134   0.0  -0.9 -0.9   0.3\n",
      "    COLE ALDRICH 2013-14  NYK 25.0  46   2 330.0   92  33   61   0    0  26   30  129   14    8   30   18  40   37   92    202332 1610612752   Center                 9  33   61 54.10 54.10    37      45 54.10      True  0.0     False 86.67 61.99       10.04        1.53       14.07    6718    1660    1016   3020     868    2423    3291 19782.0    1639    8464.40        92.20  1.09            0.88             0.21               0.06         1.15             1.25 cole aldrich 19.1 0.000 0.492  12.8  33.8  23.0   6.4   1.3   8.1  19.5  0.6  0.6 1.2  0.178  -0.1   1.6  1.5   0.3\n",
      "[guard_advanced_null_regress] 2013-14: all-advanced-NA rows: new=0/473  prev=0\n",
      "[diagnose_advanced_nulls] 2013-14: rows with ALL BBR-only adv cols NA = 5/473\n",
      "  head sample (5): ['OMER ASIK', 'VITOR FAVERANI', 'HAMADY NDIAYE', 'JR SMITH', 'JEFFERY TAYLOR']\n",
      "  per-col nulls (BBR-only): {'FTr': 6, '3PAr': 6, 'PER': 5, 'OWS': 5, 'BPM': 5, 'DBPM': 5, 'OBPM': 5, 'WS/48': 5, 'WS': 5, 'DWS': 5, 'TOV%': 5, 'BLK%': 5, 'STL%': 5, 'AST%': 5, 'TRB%': 5, 'DRB%': 5, 'ORB%': 5, 'VORP': 5}\n",
      "[update_data] 2013-14 after derived metrics: 77 columns\n",
      "[update_data] 2013-14 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2013-14: MP ≥ 10  → 473→473 rows\n",
      "[filter-shots] 2013-14: ≥50 attempts → 473→412 rows\n",
      "[nan_filter] 2013-14: dropped 4 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2013-14: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2013-14\\part.parquet\n",
      "[update_data] Starting season 2014-15\n",
      "[fetch_season_players] 519 players for 2014-15\n",
      "[update_data] fetched 519 players for 2014-15\n",
      "[update_data] processing 519 players after filter\n",
      "[update_data][WARN] no data for player 'james anderson' in 2014-15\n",
      "[update_data][WARN] no data for player 'chris babb' in 2014-15\n",
      "[update_data][WARN] no data for player 'bobby brown' in 2014-15\n",
      "[update_data][WARN] no data for player 'marshon brooks' in 2014-15\n",
      "[update_data][WARN] no data for player 'jordan crawford' in 2014-15\n",
      "[update_data][WARN] no data for player 'joel embiid' in 2014-15\n",
      "[update_data][WARN] no data for player 'andrew goudelock' in 2014-15\n",
      "[update_data][WARN] no data for player 'justin harper' in 2014-15\n",
      "[update_data][WARN] no data for player 'manny harris' in 2014-15\n",
      "[update_data][WARN] no data for player 'scotty hopson' in 2014-15\n",
      "[update_data][WARN] no data for player 'damien inglis' in 2014-15\n",
      "[update_data][WARN] no data for player 'orlando johnson' in 2014-15\n",
      "[update_data][WARN] no data for player 'deandre liggins' in 2014-15\n",
      "[update_data][WARN] no data for player 'scott machado' in 2014-15\n",
      "[update_data][WARN] no data for player 'james nunnally' in 2014-15\n",
      "[update_data][WARN] no data for player 'emeka okafor' in 2014-15\n",
      "[update_data][WARN] no data for player 'jeremy pargo' in 2014-15\n",
      "[update_data][WARN] no data for player 'willie reed' in 2014-15\n",
      "[update_data][WARN] no data for player 'sergio rodriguez' in 2014-15\n",
      "[update_data][WARN] no data for player 'xavier silas' in 2014-15\n",
      "[update_data][WARN] no data for player 'dj stephens' in 2014-15\n",
      "[update_data][WARN] no data for player 'julyan stone' in 2014-15\n",
      "[update_data][WARN] no data for player 'marquis teague' in 2014-15\n",
      "[update_data][WARN] no data for player 'sasha vujacic' in 2014-15\n",
      "[update_data][WARN] no data for player 'sonny weems' in 2014-15\n",
      "[update_data][WARN] no data for player 'damien wilkins' in 2014-15\n",
      "[update_data][WARN] no data for player 'metta world peace' in 2014-15\n",
      "[update_data] 2014-15 → DataFrame with 492 rows\n",
      "[filter-early] 2014-15: MP ≥ 10  → 492→484 rows\n",
      "[attach_wins_losses] 2014-15 W/L null% = 0.00\n",
      "[update_data] 2014-15 before derived metrics: 32 columns\n",
      "[update_data] 2014-15 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 64\n",
      "[calculate_percentages] FTA==0 count: 11\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2015_advanced.html\n",
      "[validate_name_encoding] 2014-15: Found 2/4 expected names\n",
      "  Missing: ['Luka Dončić', 'Nikola Jokić']\n",
      "  Sample actual names: ['James Harden', 'Andrew Wiggins', 'Trevor Ariza', 'Damian Lillard', 'Chris Paul', 'John Wall', 'DeAndre Jordan', 'Eric Bledsoe', 'Joe Johnson', 'Kyrie Irving']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  33%|███▎      | 5/15 [00:20<00:36,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2014-15: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2014-15: matched 481/484 players (99.4 %)\n",
      "  unmatched sample: ['OMER ASIK', 'JR SMITH', 'JEFFERY TAYLOR']\n",
      "[chk:2014-15:post-consolidate] rows=484  cols=77\n",
      "[chk:2014-15:post-derived] rows=484  cols=77\n",
      "           Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%    player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "       QUINCY ACY 2014-15  NYK 24.0  68  22 1287.0  398 152  331  18   60  76   97  301   68   27   22   60 147   79  222    203112 1610612752  Forward                 7 134  271 48.64 49.45    17      65 45.92     False 30.00     False 78.35 53.25       11.13        1.90        8.42    6079    1417    1017   2620     827    2271    3098 18188.0    1530    7719.48       433.68  5.62            4.84             0.78               0.33         5.95             6.50    quincy acy 11.9 0.181 0.293   6.9  20.5  13.5   8.7   1.1   1.4  13.8  1.0  0.7 1.7  0.063  -2.1  -1.0 -3.1  -0.3\n",
      "     STEVEN ADAMS 2014-15  OKC 21.0  70  67 1771.0  537 217  399   0    2 103  205  523   66   38   86   99 222  199  324    203500 1610612760   Center                11 217  397 54.39 54.66    45      37 54.39     False  0.00     False 50.24 54.89       10.92        1.34       10.63    6344    1819    1031   2836     876    2536    3412 17352.0    1521    8175.36       588.20  7.19            5.98             1.21               0.30         7.49             8.00  steven adams 14.1 0.005 0.514  12.2  19.3  15.8   5.5   1.1   3.8  16.8  1.9  2.2 4.1  0.111  -1.5   0.2 -1.3   0.3\n",
      "    ARRON AFFLALO 2014-15  DEN 29.0  53  53 1750.0  771 281  657  82  243 127  151  180  101   32    5   83 108   21  159    201167 1610612743    Guard                12 199  414 49.01 48.07    30      52 42.77     False 33.74     False 84.11 53.29       15.86        2.08        3.70    6662    1822    1040   2866     898    2364    3262 18270.0    1716    8503.68       806.44  9.48            8.51             0.98               0.44         9.93            10.67 arron afflalo 10.7 0.377 0.224   1.1   9.7   5.3   8.2   0.8   0.2  10.7  1.6  1.0 2.6  0.050  -1.4  -1.1 -2.5  -0.3\n",
      "[guard_advanced_null_regress] 2014-15: all-advanced-NA rows: new=0/484  prev=0\n",
      "[diagnose_advanced_nulls] 2014-15: rows with ALL BBR-only adv cols NA = 3/484\n",
      "  head sample (5): ['OMER ASIK', 'JR SMITH', 'JEFFERY TAYLOR']\n",
      "  per-col nulls (BBR-only): {'TOV%': 4, 'FTr': 4, '3PAr': 4, 'OWS': 3, 'BPM': 3, 'DBPM': 3, 'OBPM': 3, 'WS/48': 3, 'WS': 3, 'DWS': 3, 'PER': 3, 'BLK%': 3, 'STL%': 3, 'AST%': 3, 'TRB%': 3, 'DRB%': 3, 'ORB%': 3, 'VORP': 3}\n",
      "[update_data] 2014-15 after derived metrics: 77 columns\n",
      "[update_data] 2014-15 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2014-15: MP ≥ 10  → 484→484 rows\n",
      "[filter-shots] 2014-15: ≥50 attempts → 484→428 rows\n",
      "[nan_filter] 2014-15: dropped 3 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2014-15: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2014-15\\part.parquet\n",
      "[update_data] Starting season 2015-16\n",
      "[fetch_season_players] 513 players for 2015-16\n",
      "[update_data] fetched 513 players for 2015-16\n",
      "[update_data] processing 513 players after filter\n",
      "[update_data][WARN] no data for player 'vander blue' in 2015-16\n",
      "[update_data][WARN] no data for player 'bobby brown' in 2015-16\n",
      "[update_data][WARN] no data for player 'marshon brooks' in 2015-16\n",
      "[update_data][WARN] no data for player 'dwight buycks' in 2015-16\n",
      "[update_data][WARN] no data for player 'wilson chandler' in 2015-16\n",
      "[update_data][WARN] no data for player 'jack cooley' in 2015-16\n",
      "[update_data][WARN] no data for player 'jordan crawford' in 2015-16\n",
      "[update_data][WARN] no data for player 'larry drew ii' in 2015-16\n",
      "[update_data][WARN] no data for player 'joel embiid' in 2015-16\n",
      "[update_data][WARN] no data for player 'dante exum' in 2015-16\n",
      "[update_data][WARN] no data for player 'justin hamilton' in 2015-16\n",
      "[update_data][WARN] no data for player 'manny harris' in 2015-16\n",
      "[update_data][WARN] no data for player 'scotty hopson' in 2015-16\n",
      "[update_data][WARN] no data for player 'john holland' in 2015-16\n",
      "[update_data][WARN] no data for player 'deandre liggins' in 2015-16\n",
      "[update_data][WARN] no data for player 'john lucas iii' in 2015-16\n",
      "[update_data][WARN] no data for player 'kalin lucas' in 2015-16\n",
      "[update_data][WARN] no data for player 'scott machado' in 2015-16\n",
      "[update_data][WARN] no data for player 'darius miller' in 2015-16\n",
      "[update_data][WARN] no data for player 'james nunnally' in 2015-16\n",
      "[update_data][WARN] no data for player 'emeka okafor' in 2015-16\n",
      "[update_data][WARN] no data for player 'arinze onuaku' in 2015-16\n",
      "[update_data][WARN] no data for player 'jeremy pargo' in 2015-16\n",
      "[update_data][WARN] no data for player 'quincy pondexter' in 2015-16\n",
      "[update_data][WARN] no data for player 'sergio rodriguez' in 2015-16\n",
      "[update_data][WARN] no data for player 'larry sanders' in 2015-16\n",
      "[update_data][WARN] no data for player 'xavier silas' in 2015-16\n",
      "[update_data][WARN] no data for player 'dj stephens' in 2015-16\n",
      "[update_data][WARN] no data for player 'julyan stone' in 2015-16\n",
      "[update_data][WARN] no data for player 'david stockton' in 2015-16\n",
      "[update_data][WARN] no data for player 'marquis teague' in 2015-16\n",
      "[update_data][WARN] no data for player 'ekpe udoh' in 2015-16\n",
      "[update_data][WARN] no data for player 'travis wear' in 2015-16\n",
      "[update_data][WARN] no data for player 'damien wilkins' in 2015-16\n",
      "[update_data][WARN] no data for player 'reggie williams' in 2015-16\n",
      "[update_data][WARN] no data for player 'nate wolters' in 2015-16\n",
      "[update_data][WARN] no data for player 'dorell wright' in 2015-16\n",
      "[update_data] 2015-16 → DataFrame with 476 rows\n",
      "[filter-early] 2015-16: MP ≥ 10  → 476→470 rows\n",
      "[attach_wins_losses] 2015-16 W/L null% = 0.00\n",
      "[update_data] 2015-16 before derived metrics: 32 columns\n",
      "[update_data] 2015-16 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 42\n",
      "[calculate_percentages] FTA==0 count: 10\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2016_advanced.html\n",
      "[validate_name_encoding] 2015-16: Found 3/4 expected names\n",
      "  Missing: ['Luka Dončić']\n",
      "  Sample actual names: ['James Harden', 'Gordon Hayward', 'Kemba Walker', 'Trevor Ariza', 'Marcus Morris', 'Khris Middleton', 'Kyle Lowry', 'Andrew Wiggins', 'Giannis Antetokounmpo', 'Paul George']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  40%|████      | 6/15 [00:23<00:32,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2015-16: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2015-16: matched 467/470 players (99.4 %)\n",
      "  unmatched sample: ['OMER ASIK', 'TIBOR PLEISS', 'JR SMITH']\n",
      "[chk:2015-16:post-consolidate] rows=470  cols=77\n",
      "[chk:2015-16:post-derived] rows=470  cols=77\n",
      "          Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%   player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "      QUINCY ACY 2015-16  SAC 25.0  59  29  876.0  307 119  214  19   49  50   68  188   27   29   24   27 103   65  123    203112 1610612758  Forward                 7 100  165 60.05 60.61    33      49 55.61     False 38.78     False 73.53 62.93       12.62        1.11        7.73    7083    2089    1274   3283     868    2760    3628 19807.0    2009    9276.16       270.92  2.92            2.63             0.29               0.11         3.03             3.21   quincy acy 14.7 0.229 0.318   8.1  15.1  11.6   4.4   1.6   2.2  10.0  1.8  0.7 2.5  0.137  -0.2   0.2 -0.1   0.4\n",
      "    STEVEN ADAMS 2015-16  OKC 22.0  80  80 2014.0  636 261  426   0    0 114  196  533   62   42   89   84 223  219  314    203500 1610612760   Center                11 261  426 61.27 61.27    55      27 61.27      True  0.00     False 58.16 62.08       11.37        1.11        9.53    6933    2040    1241   3320    1063    2873    3936 19257.0    1834    9071.60       596.24  6.57            5.65             0.93               0.25         6.83             7.26 steven adams 15.5 0.000 0.460  12.5  16.1  14.4   4.3   1.0   3.3  14.1  4.2  2.3 6.5  0.155   0.0   0.2  0.2   1.1\n",
      "    JORDAN ADAMS 2015-16  MEM 21.0   2   0   15.0    7   2    6   0    1   3    5    2    3    3    0    2   2    0    2    203919 1610612763    Guard                 3   2    5 33.33 40.00    42      40 33.33     False  0.00     False 60.00 42.68       16.80        7.20        4.80    6292    1913     954   2777     883    2342    3225 18413.0    1562    8087.72        10.20  0.13            0.10             0.02               0.01         0.13             0.16 jordan adams 17.3 0.167 0.833   0.0  15.9   7.6  31.9  10.3   0.0  19.6  0.0  0.0 0.0  0.015  -2.5   9.4  6.9   0.0\n",
      "[guard_advanced_null_regress] 2015-16: all-advanced-NA rows: new=0/470  prev=0\n",
      "[diagnose_advanced_nulls] 2015-16: rows with ALL BBR-only adv cols NA = 3/470\n",
      "  head sample (5): ['OMER ASIK', 'TIBOR PLEISS', 'JR SMITH']\n",
      "  per-col nulls (BBR-only): {'PER': 3, '3PAr': 3, 'BPM': 3, 'DBPM': 3, 'OBPM': 3, 'WS/48': 3, 'WS': 3, 'DWS': 3, 'OWS': 3, 'TOV%': 3, 'BLK%': 3, 'STL%': 3, 'AST%': 3, 'TRB%': 3, 'DRB%': 3, 'ORB%': 3, 'FTr': 3, 'VORP': 3}\n",
      "[update_data] 2015-16 after derived metrics: 77 columns\n",
      "[update_data] 2015-16 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2015-16: MP ≥ 10  → 470→470 rows\n",
      "[filter-shots] 2015-16: ≥50 attempts → 470→422 rows\n",
      "[nan_filter] 2015-16: dropped 2 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2015-16: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2015-16\\part.parquet\n",
      "[update_data] Starting season 2016-17\n",
      "[fetch_season_players] 526 players for 2016-17\n",
      "[update_data] fetched 526 players for 2016-17\n",
      "[update_data] processing 526 players after filter\n",
      "[update_data][WARN] no data for player 'thanasis antetokounmpo' in 2016-17\n",
      "[update_data][WARN] no data for player 'vander blue' in 2016-17\n",
      "[update_data][WARN] no data for player 'chris bosh' in 2016-17\n",
      "[update_data][WARN] no data for player 'marshon brooks' in 2016-17\n",
      "[update_data][WARN] no data for player 'lorenzo brown' in 2016-17\n",
      "[update_data][WARN] no data for player 'markel brown' in 2016-17\n",
      "[update_data][WARN] no data for player 'dwight buycks' in 2016-17\n",
      "[update_data][WARN] no data for player 'mario chalmers' in 2016-17\n",
      "[update_data][WARN] no data for player 'jack cooley' in 2016-17\n",
      "[update_data][WARN] no data for player 'larry drew ii' in 2016-17\n",
      "[update_data][WARN] no data for player 'jeremy evans' in 2016-17\n",
      "[update_data][WARN] no data for player 'festus ezeli' in 2016-17\n",
      "[update_data][WARN] no data for player 'jimmer fredette' in 2016-17\n",
      "[update_data][WARN] no data for player 'john holland' in 2016-17\n",
      "[update_data][WARN] no data for player 'scotty hopson' in 2016-17\n",
      "[update_data][WARN] no data for player 'shane larkin' in 2016-17\n",
      "[update_data][WARN] no data for player 'kalin lucas' in 2016-17\n",
      "[update_data][WARN] no data for player 'scott machado' in 2016-17\n",
      "[update_data][WARN] no data for player 'darius miller' in 2016-17\n",
      "[update_data][WARN] no data for player 'luis montero' in 2016-17\n",
      "[update_data][WARN] no data for player 'eric moreland' in 2016-17\n",
      "[update_data][WARN] no data for player 'xavier munford' in 2016-17\n",
      "[update_data][WARN] no data for player 'james nunnally' in 2016-17\n",
      "[update_data][WARN] no data for player 'emeka okafor' in 2016-17\n",
      "[update_data][WARN] no data for player 'jeremy pargo' in 2016-17\n",
      "[update_data][WARN] no data for player 'nikola pekovic' in 2016-17\n",
      "[update_data][WARN] no data for player 'kendrick perkins' in 2016-17\n",
      "[update_data][WARN] no data for player 'quincy pondexter' in 2016-17\n",
      "[update_data][WARN] no data for player 'jakarr sampson' in 2016-17\n",
      "[update_data][WARN] no data for player 'xavier silas' in 2016-17\n",
      "[update_data][WARN] no data for player 'ben simmons' in 2016-17\n",
      "[update_data][WARN] no data for player 'josh smith' in 2016-17\n",
      "[update_data][WARN] no data for player 'dj stephens' in 2016-17\n",
      "[update_data][WARN] no data for player 'david stockton' in 2016-17\n",
      "[update_data][WARN] no data for player 'julyan stone' in 2016-17\n",
      "[update_data][WARN] no data for player 'marquis teague' in 2016-17\n",
      "[update_data][WARN] no data for player 'ekpe udoh' in 2016-17\n",
      "[update_data][WARN] no data for player 'travis wear' in 2016-17\n",
      "[update_data][WARN] no data for player 'damien wilkins' in 2016-17\n",
      "[update_data][WARN] no data for player 'nate wolters' in 2016-17\n",
      "[update_data] 2016-17 → DataFrame with 486 rows\n",
      "[filter-early] 2016-17: MP ≥ 10  → 486→482 rows\n",
      "[attach_wins_losses] 2016-17 W/L null% = 0.00\n",
      "[update_data] 2016-17 before derived metrics: 32 columns\n",
      "[update_data] 2016-17 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 34\n",
      "[calculate_percentages] FTA==0 count: 12\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2017_advanced.html\n",
      "[validate_name_encoding] 2016-17: Found 3/4 expected names\n",
      "  Missing: ['Luka Dončić']\n",
      "  Sample actual names: ['Andrew Wiggins', 'Karl-Anthony Towns', 'James Harden', 'Giannis Antetokounmpo', 'John Wall', 'Jimmy Butler', 'Harrison Barnes', 'Russell Westbrook', 'CJ McCollum', 'LeBron James']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  47%|████▋     | 7/15 [00:27<00:29,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2016-17: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2016-17: matched 480/482 players (99.6 %)\n",
      "  unmatched sample: ['OMER ASIK', 'JR SMITH']\n",
      "[chk:2016-17:post-consolidate] rows=482  cols=77\n",
      "[chk:2016-17:post-derived] rows=482  cols=77\n",
      "           Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%    player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "       QUINCY ACY 2016-17  BKN 26.0  32   1  510.0  209  65  153  36   83  43   57  107   18   14   15   19  58   18   89    203112 1610612751  Forward                 7  29   70 54.25 41.43    20      62 42.48     False 43.37     False 75.44 58.68       14.75        1.27        7.55    6803    1974    1268   3025     704    2805    3509 19199.0    1727    8939.56       197.08  2.20            1.99             0.21               0.07         2.27             2.41    quincy acy 11.8 0.529 0.353   3.9  18.0  11.0   4.9   1.2   2.0   9.7  0.5  0.5 0.9  0.082  -1.5  -0.6 -2.1   0.0\n",
      "     STEVEN ADAMS 2016-17  OKC 23.0  80  80 2389.0  905 374  655   0    1 157  257  613   86   89   78  146 195  281  332    203500 1610612760   Center                11 374  654 57.10 57.19    47      35 57.10     False  0.00     False 61.09 58.91       13.64        1.30        9.24    6850    2057    1161   3086     956    2699    3655 18827.0    1694    8916.08       914.08 10.25            8.61             1.64               0.36        10.61            11.22  steven adams 16.5 0.002 0.392  13.0  15.4  14.2   5.4   1.8   2.6  16.0  3.3  3.1 6.5  0.130  -0.2   0.0 -0.2   1.1\n",
      "    ARRON AFFLALO 2016-17  SAC 31.0  61  45 1580.0  515 185  420  62  151  83   93  125   78   21    6   42 104    9  116    201167 1610612758    Guard                12 123  269 51.43 45.72    32      50 44.05     False 41.06     False 89.25 55.87       11.73        1.78        2.85    6191    1824    1064   2862     685    2493    3178 18463.0    1739    8057.56       502.92  6.24            5.72             0.52               0.36         6.60             7.21 arron afflalo  8.9 0.360 0.221   0.7   8.4   4.6   7.4   0.7   0.3   8.4  1.2  0.2 1.4  0.043  -2.1  -1.5 -3.6  -0.7\n",
      "[guard_advanced_null_regress] 2016-17: all-advanced-NA rows: new=0/482  prev=0\n",
      "[diagnose_advanced_nulls] 2016-17: rows with ALL BBR-only adv cols NA = 2/482\n",
      "  head sample (5): ['OMER ASIK', 'JR SMITH']\n",
      "  per-col nulls (BBR-only): {'PER': 2, '3PAr': 2, 'BPM': 2, 'DBPM': 2, 'OBPM': 2, 'WS/48': 2, 'WS': 2, 'DWS': 2, 'OWS': 2, 'TOV%': 2, 'BLK%': 2, 'STL%': 2, 'AST%': 2, 'TRB%': 2, 'DRB%': 2, 'ORB%': 2, 'FTr': 2, 'VORP': 2}\n",
      "[update_data] 2016-17 after derived metrics: 77 columns\n",
      "[update_data] 2016-17 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2016-17: MP ≥ 10  → 482→482 rows\n",
      "[filter-shots] 2016-17: ≥50 attempts → 482→422 rows\n",
      "[nan_filter] 2016-17: dropped 2 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2016-17: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2016-17\\part.parquet\n",
      "[update_data] Starting season 2017-18\n",
      "[fetch_season_players] 564 players for 2017-18\n",
      "[update_data] fetched 564 players for 2017-18\n",
      "[update_data] processing 564 players after filter\n",
      "[update_data][WARN] no data for player 'alexis ajinca' in 2017-18\n",
      "[update_data][WARN] no data for player 'thanasis antetokounmpo' in 2017-18\n",
      "[update_data][WARN] no data for player 'seth curry' in 2017-18\n",
      "[update_data][WARN] no data for player 'jimmer fredette' in 2017-18\n",
      "[update_data][WARN] no data for player 'harry giles iii' in 2017-18\n",
      "[update_data][WARN] no data for player 'john jenkins' in 2017-18\n",
      "[update_data][WARN] no data for player 'frank jackson' in 2017-18\n",
      "[update_data][WARN] no data for player 'amile jefferson' in 2017-18\n",
      "[update_data][WARN] no data for player 'terrence jones' in 2017-18\n",
      "[update_data][WARN] no data for player 'brandon knight' in 2017-18\n",
      "[update_data][WARN] no data for player 'ty lawson' in 2017-18\n",
      "[update_data][WARN] no data for player 'kalin lucas' in 2017-18\n",
      "[update_data][WARN] no data for player 'scott machado' in 2017-18\n",
      "[update_data][WARN] no data for player 'jordan mcrae' in 2017-18\n",
      "[update_data][WARN] no data for player 'donatas motiejunas' in 2017-18\n",
      "[update_data][WARN] no data for player 'james nunnally' in 2017-18\n",
      "[update_data][WARN] no data for player 'jeremy pargo' in 2017-18\n",
      "[update_data][WARN] no data for player 'chasson randle' in 2017-18\n",
      "[update_data][WARN] no data for player 'dj stephens' in 2017-18\n",
      "[update_data][WARN] no data for player 'axel toupane' in 2017-18\n",
      "[update_data][WARN] no data for player 'jarrod uthoff' in 2017-18\n",
      "[update_data][WARN] no data for player 'anderson varejao' in 2017-18\n",
      "[update_data][WARN] no data for player 'c.j. wilcox' in 2017-18\n",
      "[update_data][WARN] no data for player 'christian wood' in 2017-18\n",
      "[update_data] 2017-18 → DataFrame with 540 rows\n",
      "[filter-early] 2017-18: MP ≥ 10  → 540→517 rows\n",
      "[attach_wins_losses] 2017-18 W/L null% = 0.00\n",
      "[update_data] 2017-18 before derived metrics: 32 columns\n",
      "[update_data] 2017-18 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 39\n",
      "[calculate_percentages] FTA==0 count: 19\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2018_advanced.html\n",
      "[validate_name_encoding] 2017-18: Found 3/4 expected names\n",
      "  Missing: ['Luka Dončić']\n",
      "  Sample actual names: ['LeBron James', 'Khris Middleton', 'Andrew Wiggins', 'Bradley Beal', 'Jrue Holiday', 'CJ McCollum', 'Karl-Anthony Towns', 'Russell Westbrook', 'Paul George', 'Giannis Antetokounmpo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  53%|█████▎    | 8/15 [00:31<00:26,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2017-18: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2017-18: matched 515/517 players (99.6 %)\n",
      "  unmatched sample: ['OMER ASIK', 'JR SMITH']\n",
      "[chk:2017-18:post-consolidate] rows=517  cols=77\n",
      "[chk:2017-18:post-derived] rows=517  cols=77\n",
      "          Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%   player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "    ALEX ABRINES 2017-18  OKC 24.0  75   8 1134.0  353 115  291  84  221  39   46  114   28   38    8   25 124   26   88    203518 1610612760    Guard                 3  31   70 53.95 44.29    48      34 39.52     False 38.01     False 84.78 56.71       11.21        0.89        3.62    7075    1946    1090   3206    1004    2629    3633 19314.0    1726    9021.24       336.24  3.73            3.45             0.28               0.11         3.84             4.04 alex abrines  9.0 0.759 0.158   2.5   8.9   5.6   3.4   1.7   0.6   7.4  1.3  0.9 2.2  0.094  -1.9   0.4 -1.5   0.1\n",
      "    STEVEN ADAMS 2017-18  OKC 24.0  76  76 2487.0 1056 448  712   0    2 160  286  685   88   92   78  128 215  384  301    203500 1610612760   Center                11 448  710 62.92 63.10    48      34 62.92     False  0.00     False 55.94 63.02       15.29        1.27        9.92    7075    1946    1090   3206    1004    2629    3633 19314.0    1726    9021.24       965.84 10.71            9.29             1.42               0.36        11.07            11.68 steven adams 20.6 0.003 0.402  16.6  13.9  15.3   5.5   1.8   2.8  13.3  6.7  3.0 9.7  0.187   1.7  -0.6  1.1   2.0\n",
      "      QUINCY ACY 2017-18  BKN 27.0  70   8 1359.0  411 130  365 102  292  49   60  257   57   33   29   60 149   40  217    203112 1610612751  Forward                 7  28   73 49.59 38.36    28      54 35.62     False 34.93     False 81.67 52.50       10.89        1.51        6.81    6738    1772    1104   2969     725    2660    3385 18828.0    1865    8621.68       451.40  5.24            4.54             0.70               0.24         5.48             5.90   quincy acy  8.2 0.800 0.164   3.1  17.1  10.0   6.0   1.2   1.6  13.3 -0.1  1.1 1.0  0.036  -2.6   0.1 -2.5  -0.2\n",
      "[guard_advanced_null_regress] 2017-18: all-advanced-NA rows: new=0/517  prev=0\n",
      "[diagnose_advanced_nulls] 2017-18: rows with ALL BBR-only adv cols NA = 2/517\n",
      "  head sample (5): ['OMER ASIK', 'JR SMITH']\n",
      "  per-col nulls (BBR-only): {'PER': 2, '3PAr': 2, 'BPM': 2, 'DBPM': 2, 'OBPM': 2, 'WS/48': 2, 'WS': 2, 'DWS': 2, 'OWS': 2, 'TOV%': 2, 'BLK%': 2, 'STL%': 2, 'AST%': 2, 'TRB%': 2, 'DRB%': 2, 'ORB%': 2, 'FTr': 2, 'VORP': 2}\n",
      "[update_data] 2017-18 after derived metrics: 77 columns\n",
      "[update_data] 2017-18 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2017-18: MP ≥ 10  → 517→517 rows\n",
      "[filter-shots] 2017-18: ≥50 attempts → 517→437 rows\n",
      "[nan_filter] 2017-18: dropped 1 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2017-18: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2017-18\\part.parquet\n",
      "[update_data] Starting season 2018-19\n",
      "[fetch_season_players] 557 players for 2018-19\n",
      "[update_data] fetched 557 players for 2018-19\n",
      "[update_data] processing 557 players after filter\n",
      "[update_data][WARN] no data for player 'thanasis antetokounmpo' in 2018-19\n",
      "[update_data][WARN] no data for player 'trevon bluiett' in 2018-19\n",
      "[update_data][WARN] no data for player 'antonius cleveland' in 2018-19\n",
      "[update_data][WARN] no data for player 'michael frazier ii' in 2018-19\n",
      "[update_data][WARN] no data for player 'wenyen gabriel' in 2018-19\n",
      "[update_data][WARN] no data for player 'jonathan gibson' in 2018-19\n",
      "[update_data][WARN] no data for player 'josh gray' in 2018-19\n",
      "[update_data][WARN] no data for player 'nigel hayes-davis' in 2018-19\n",
      "[update_data][WARN] no data for player 'scotty hopson' in 2018-19\n",
      "[update_data][WARN] no data for player 'mike james' in 2018-19\n",
      "[update_data][WARN] no data for player 'joe johnson' in 2018-19\n",
      "[update_data][WARN] no data for player 'josh magette' in 2018-19\n",
      "[update_data][WARN] no data for player 'ben moore' in 2018-19\n",
      "[update_data][WARN] no data for player 'timofey mozgov' in 2018-19\n",
      "[update_data][WARN] no data for player 'dejounte murray' in 2018-19\n",
      "[update_data][WARN] no data for player 'kendrick nunn' in 2018-19\n",
      "[update_data][WARN] no data for player 'jeremy pargo' in 2018-19\n",
      "[update_data][WARN] no data for player 'michael porter jr.' in 2018-19\n",
      "[update_data][WARN] no data for player 'tarik phillip' in 2018-19\n",
      "[update_data][WARN] no data for player 'kristaps porzingis' in 2018-19\n",
      "[update_data][WARN] no data for player 'andre roberson' in 2018-19\n",
      "[update_data][WARN] no data for player 'axel toupane' in 2018-19\n",
      "[update_data][WARN] no data for player 'jarrod uthoff' in 2018-19\n",
      "[update_data][WARN] no data for player 'denzel valentine' in 2018-19\n",
      "[update_data][WARN] no data for player 'anderson varejao' in 2018-19\n",
      "[update_data][WARN] no data for player 'derrick walton jr.' in 2018-19\n",
      "[update_data][WARN] no data for player 'isaiah whitehead' in 2018-19\n",
      "[update_data] 2018-19 → DataFrame with 530 rows\n",
      "[filter-early] 2018-19: MP ≥ 10  → 530→513 rows\n",
      "[attach_wins_losses] 2018-19 W/L null% = 0.00\n",
      "[update_data] 2018-19 before derived metrics: 32 columns\n",
      "[update_data] 2018-19 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 28\n",
      "[calculate_percentages] FTA==0 count: 13\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2019_advanced.html\n",
      "[validate_name_encoding] 2018-19: Found 4/4 expected names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  60%|██████    | 9/15 [00:35<00:22,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2018-19: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2018-19: matched 509/513 players (99.2 %)\n",
      "  unmatched sample: ['MITCHELL CREEK', 'VINCENT EDWARDS', 'CAM REYNOLDS', 'JR SMITH']\n",
      "[chk:2018-19:post-consolidate] rows=513  cols=77\n",
      "[chk:2018-19:post-derived] rows=513  cols=77\n",
      "          Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%   player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS   WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "    ALEX ABRINES 2018-19  OKC 25.0  31   2  588.0  165  56  157  41  127  12   13   48   20   17    6   14  53    5   43    203518 1610612760    Guard                 3  15   30 48.73 50.00    49      33 35.67     False 32.28     False 92.31 50.70       10.10        1.22        2.94    7503    2024    1107   3423    1006    2826    3832 19340.0    1894    9500.56       176.72  1.86            1.71             0.15               0.08         1.94             2.07 alex abrines  6.3 0.809 0.083   0.9   7.8   4.2   4.3   1.3   0.9   7.9  0.1  0.6  0.6  0.053  -3.7   0.4 -3.3  -0.2\n",
      "      QUINCY ACY 2018-19  PHX 28.0  10   0  123.0   17   4   18   2   15   7   10   25    8    1    4    4  24    3   22    203112 1610612756  Forward                 7   2    3 27.78 66.67    19      63 22.22     False 13.33     False 70.00 37.95        4.98        2.34        7.32    6800    1758    1184   3149     710    2361    3071 18492.0    1810    8757.52        26.40  0.30            0.26             0.05               0.03         0.34             0.39   quincy acy  2.9 0.833 0.556   2.7  20.1  11.3   8.2   0.4   2.7  15.2 -0.1  0.0 -0.1 -0.022  -7.6  -0.5 -8.1  -0.2\n",
      "    STEVEN ADAMS 2018-19  OKC 25.0  80  80 2669.0 1108 481  809   0    2 146  292  760  124  117   76  135 204  391  369    203500 1610612760   Center                11 481  807 59.46 59.60    49      33 59.46     False  0.00     False 50.00 59.09       14.94        1.67       10.25    7503    2024    1107   3423    1006    2826    3832 19340.0    1894    9500.56      1072.48 11.29            9.87             1.42               0.48        11.77            12.59 steven adams 18.5 0.002 0.361  14.7  14.8  14.7   6.6   2.0   2.4  12.6  5.1  4.0  9.1  0.163   0.7   0.4  1.1   2.1\n",
      "[guard_advanced_null_regress] 2018-19: all-advanced-NA rows: new=0/513  prev=0\n",
      "[diagnose_advanced_nulls] 2018-19: rows with ALL BBR-only adv cols NA = 4/513\n",
      "  head sample (5): ['MITCHELL CREEK', 'VINCENT EDWARDS', 'CAM REYNOLDS', 'JR SMITH']\n",
      "  per-col nulls (BBR-only): {'PER': 4, '3PAr': 4, 'BPM': 4, 'DBPM': 4, 'OBPM': 4, 'WS/48': 4, 'WS': 4, 'DWS': 4, 'OWS': 4, 'TOV%': 4, 'BLK%': 4, 'STL%': 4, 'AST%': 4, 'TRB%': 4, 'DRB%': 4, 'ORB%': 4, 'FTr': 4, 'VORP': 4}\n",
      "[update_data] 2018-19 after derived metrics: 77 columns\n",
      "[update_data] 2018-19 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2018-19: MP ≥ 10  → 513→513 rows\n",
      "[filter-shots] 2018-19: ≥50 attempts → 513→442 rows\n",
      "[nan_filter] 2018-19: dropped 2 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2018-19: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2018-19\\part.parquet\n",
      "[update_data] Starting season 2019-20\n",
      "[fetch_season_players] 562 players for 2019-20\n",
      "[update_data] fetched 562 players for 2019-20\n",
      "[update_data] processing 562 players after filter\n",
      "[update_data][WARN] no data for player 'jaylen adams' in 2019-20\n",
      "[update_data][WARN] no data for player 'darren collison' in 2019-20\n",
      "[update_data][WARN] no data for player 'demarcus cousins' in 2019-20\n",
      "[update_data][WARN] no data for player 'sam dekker' in 2019-20\n",
      "[update_data][WARN] no data for player 'tyler dorsey' in 2019-20\n",
      "[update_data][WARN] no data for player 'kevin durant' in 2019-20\n",
      "[update_data][WARN] no data for player 'nigel hayes-davis' in 2019-20\n",
      "[update_data][WARN] no data for player 'haywood highsmith' in 2019-20\n",
      "[update_data][WARN] no data for player 'scotty hopson' in 2019-20\n",
      "[update_data][WARN] no data for player 'mike james' in 2019-20\n",
      "[update_data][WARN] no data for player 'joe johnson' in 2019-20\n",
      "[update_data][WARN] no data for player 'jemerrio jones' in 2019-20\n",
      "[update_data][WARN] no data for player 'george king' in 2019-20\n",
      "[update_data][WARN] no data for player 'darius miller' in 2019-20\n",
      "[update_data][WARN] no data for player 'greg monroe' in 2019-20\n",
      "[update_data][WARN] no data for player 'jaylen morris' in 2019-20\n",
      "[update_data][WARN] no data for player 'james nunnally' in 2019-20\n",
      "[update_data][WARN] no data for player 'jontay porter' in 2019-20\n",
      "[update_data][WARN] no data for player 'davon reed' in 2019-20\n",
      "[update_data][WARN] no data for player 'cam reynolds' in 2019-20\n",
      "[update_data][WARN] no data for player 'wayne selden' in 2019-20\n",
      "[update_data][WARN] no data for player 'kobi simmons' in 2019-20\n",
      "[update_data][WARN] no data for player 'ray spalding' in 2019-20\n",
      "[update_data][WARN] no data for player 'lance stephenson' in 2019-20\n",
      "[update_data][WARN] no data for player 'nik stauskas' in 2019-20\n",
      "[update_data][WARN] no data for player 'emanuel terry' in 2019-20\n",
      "[update_data][WARN] no data for player 'klay thompson' in 2019-20\n",
      "[update_data][WARN] no data for player 'axel toupane' in 2019-20\n",
      "[update_data][WARN] no data for player 'anderson varejao' in 2019-20\n",
      "[update_data][WARN] no data for player 'john wall' in 2019-20\n",
      "[update_data][WARN] no data for player 'guerschon yabusele' in 2019-20\n",
      "[update_data][WARN] no data for player 'dylan windler' in 2019-20\n",
      "[update_data][WARN] no data for player 'kenny wooten' in 2019-20\n",
      "[update_data] 2019-20 → DataFrame with 529 rows\n",
      "[filter-early] 2019-20: MP ≥ 10  → 529→521 rows\n",
      "[attach_wins_losses] 2019-20 W/L null% = 0.00\n",
      "[update_data] 2019-20 before derived metrics: 32 columns\n",
      "[update_data] 2019-20 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 25\n",
      "[calculate_percentages] FTA==0 count: 20\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2020_advanced.html\n",
      "[validate_name_encoding] 2019-20: Found 4/4 expected names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  67%|██████▋   | 10/15 [00:38<00:17,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2019-20: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2019-20: matched 520/521 players (99.8 %)\n",
      "  unmatched sample: ['JR SMITH']\n",
      "[chk:2019-20:post-consolidate] rows=521  cols=77\n",
      "[chk:2019-20:post-derived] rows=521  cols=77\n",
      "                      Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID       Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%               player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS   WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "           LAMARCUS ALDRIDGE 2019-20  SAS 34.0  53  53 1754.0 1001 391  793  61  157 158  191  392  129   36   87   74 128  103  289    200746 1610612759 Center-Forward                17 330  636 53.15 51.89    32      39 49.31     False 38.85     False 82.72 57.07       20.55        2.65        8.05    6346    1664     868   2994     637    2529    3166 17211.0    1751    7946.16       951.04 11.97           11.04             0.93               0.60        12.57            13.59        lamarcus aldridge 19.7 0.198 0.241   6.3  17.8  12.0  11.4   1.0   4.4   7.8  3.0  1.4  4.5  0.122   1.8  -0.5  1.4   1.5\n",
      "    NICKEIL ALEXANDER-WALKER 2019-20  NOP 21.0  47   1  591.0  267  98  266  46  133  25   37   84   89   17    8   54  57    9   75   1629638 1610612740          Guard                 6  52  133 45.49 39.10    30      42 36.84     False 34.59     False 67.57 47.29       16.26        5.42        5.12    6598    1687    1143   3065     797    2551    3348 17429.0    1932    8483.28       336.28  3.96            3.33             0.64               0.39         4.36             5.01 nickeil alexander walker  8.9 0.500 0.139   1.6  13.5   7.5  21.1   1.3   1.1  16.1 -0.7  0.4 -0.2 -0.020  -3.2  -1.4 -4.6  -0.4\n",
      "                 BAM ADEBAYO 2019-20  MIA 22.0  72  72 2417.0 1146 440  790   2   14 264  382  735  368   82   93  204 182  176  559   1628389 1610612748 Center-Forward                 8 438  776 55.82 56.44    44      29 55.70     False 14.29     False 69.11 59.81       17.07        5.48       10.95    5925    1793     989   2777     603    2497    3100 16961.0    1843    7702.92      1162.08 15.09           12.44             2.65               1.77        16.86            19.86              bam adebayo 20.3 0.018 0.484   8.5  24.9  17.0  24.2   1.7   3.8  17.6  4.6  3.9  8.5  0.168   1.4   2.0  3.4   3.3\n",
      "[guard_advanced_null_regress] 2019-20: all-advanced-NA rows: new=0/521  prev=0\n",
      "[diagnose_advanced_nulls] 2019-20: rows with ALL BBR-only adv cols NA = 1/521\n",
      "  head sample (5): ['JR SMITH']\n",
      "  per-col nulls (BBR-only): {'PER': 1, '3PAr': 1, 'BPM': 1, 'DBPM': 1, 'OBPM': 1, 'WS/48': 1, 'WS': 1, 'DWS': 1, 'OWS': 1, 'TOV%': 1, 'BLK%': 1, 'STL%': 1, 'AST%': 1, 'TRB%': 1, 'DRB%': 1, 'ORB%': 1, 'FTr': 1, 'VORP': 1}\n",
      "[update_data] 2019-20 after derived metrics: 77 columns\n",
      "[update_data] 2019-20 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2019-20: MP ≥ 10  → 521→521 rows\n",
      "[filter-shots] 2019-20: ≥50 attempts → 521→429 rows\n",
      "[nan_filter] 2019-20: no columns below threshold; no rows dropped\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2019-20\\part.parquet\n",
      "[update_data] Starting season 2020-21\n",
      "[fetch_season_players] 579 players for 2020-21\n",
      "[update_data] fetched 579 players for 2020-21\n",
      "[update_data] processing 579 players after filter\n",
      "[update_data][WARN] no data for player 'justin anderson' in 2020-21\n",
      "[update_data][WARN] no data for player 'deonte burton' in 2020-21\n",
      "[update_data][WARN] no data for player 'zylan cheatham' in 2020-21\n",
      "[update_data][WARN] no data for player 'zach collins' in 2020-21\n",
      "[update_data][WARN] no data for player 'darren collison' in 2020-21\n",
      "[update_data][WARN] no data for player 'sam dekker' in 2020-21\n",
      "[update_data][WARN] no data for player 'cheick diallo' in 2020-21\n",
      "[update_data][WARN] no data for player 'tyler dorsey' in 2020-21\n",
      "[update_data][WARN] no data for player 'melvin frazier jr.' in 2020-21\n",
      "[update_data][WARN] no data for player 'nigel hayes-davis' in 2020-21\n",
      "[update_data][WARN] no data for player 'haywood highsmith' in 2020-21\n",
      "[update_data][WARN] no data for player 'scotty hopson' in 2020-21\n",
      "[update_data][WARN] no data for player 'caleb homesley' in 2020-21\n",
      "[update_data][WARN] no data for player 'jonathan isaac' in 2020-21\n",
      "[update_data][WARN] no data for player 'bj johnson' in 2020-21\n",
      "[update_data][WARN] no data for player 'joe johnson' in 2020-21\n",
      "[update_data][WARN] no data for player 'jemerrio jones' in 2020-21\n",
      "[update_data][WARN] no data for player 'george king' in 2020-21\n",
      "[update_data][WARN] no data for player 'brandon knight' in 2020-21\n",
      "[update_data][WARN] no data for player 'skal labissiere' in 2020-21\n",
      "[update_data][WARN] no data for player 'cj miles' in 2020-21\n",
      "[update_data][WARN] no data for player 'greg monroe' in 2020-21\n",
      "[update_data][WARN] no data for player 'matt mooney' in 2020-21\n",
      "[update_data][WARN] no data for player 'jaylen morris' in 2020-21\n",
      "[update_data][WARN] no data for player 'emmanuel mudiay' in 2020-21\n",
      "[update_data][WARN] no data for player 'malik newman' in 2020-21\n",
      "[update_data][WARN] no data for player 'davon reed' in 2020-21\n",
      "[update_data][WARN] no data for player 'admiral schofield' in 2020-21\n",
      "[update_data][WARN] no data for player 'wayne selden' in 2020-21\n",
      "[update_data][WARN] no data for player 'kobi simmons' in 2020-21\n",
      "[update_data][WARN] no data for player 'nik stauskas' in 2020-21\n",
      "[update_data][WARN] no data for player 'lance stephenson' in 2020-21\n",
      "[update_data][WARN] no data for player 'emanuel terry' in 2020-21\n",
      "[update_data][WARN] no data for player 'klay thompson' in 2020-21\n",
      "[update_data][WARN] no data for player 'derrick walton jr.' in 2020-21\n",
      "[update_data][WARN] no data for player 'tyrone wallace' in 2020-21\n",
      "[update_data][WARN] no data for player 'luca vildoza' in 2020-21\n",
      "[update_data][WARN] no data for player 'guerschon yabusele' in 2020-21\n",
      "[update_data][WARN] no data for player 'omer yurtseven' in 2020-21\n",
      "[update_data] 2020-21 → DataFrame with 540 rows\n",
      "[filter-early] 2020-21: MP ≥ 10  → 540→535 rows\n",
      "[attach_wins_losses] 2020-21 W/L null% = 0.00\n",
      "[update_data] 2020-21 before derived metrics: 32 columns\n",
      "[update_data] 2020-21 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 18\n",
      "[calculate_percentages] FTA==0 count: 14\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2021_advanced.html\n",
      "[validate_name_encoding] 2020-21: Found 4/4 expected names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  73%|███████▎  | 11/15 [00:42<00:14,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2020-21: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2020-21: matched 535/535 players (100.0 %)\n",
      "[chk:2020-21:post-consolidate] rows=535  cols=77\n",
      "[chk:2020-21:post-derived] rows=535  cols=77\n",
      "               Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID       Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero  3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%        player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "     PRECIOUS ACHIUWA 2020-21  MIA 21.0  61   4  737.0  304 124  228   0    1  56  110  208   29   20   28   43  91   73  135   1630173 1610612748        Forward                 5 124  227 54.39 54.63    40      32 54.39     False  0.0     False 50.91 54.99       14.85        1.42       10.16    5859    1482     925   2750     567    2348    2915 16777.0    1840    7436.08       319.40  4.30            3.72             0.58               0.14         4.44             4.69  precious achiuwa 14.2 0.004 0.482  11.5  20.6  16.1   6.1   1.3   4.0  13.5  0.3  1.0 1.3  0.085  -3.6  -0.5 -4.1  -0.4\n",
      "         STEVEN ADAMS 2020-21  NOP 27.0  58  58 1605.0  438 189  308   0    3  60  135  514  111   54   38   78 113  213  301    203500 1610612740         Center                11 189  305 61.36 61.97    31      41 61.36     False  0.0     False 44.44 59.61        9.82        2.49       11.53    6120    1807     954   2949     806    2413    3219 16396.0    1791    7869.08       445.40  5.66            4.67             0.99               0.52         6.18             7.07      steven adams 15.1 0.010 0.438  14.4  20.4  17.4   9.1   1.6   2.2  17.5  2.3  1.7 4.0  0.119  -0.4   0.1 -0.3   0.7\n",
      "    LAMARCUS ALDRIDGE 2020-21  SAS 35.0  21  18  544.0  288 115  248  27   75  31   37   94   36    8   18   20  36   17   77    200746 1610612759 Center-Forward                17  88  173 51.81 50.87    33      39 46.37     False 36.0     False 83.78 54.49       19.06        2.38        6.22    6461    1560     781   2984     657    2459    3116 17293.0    1740    7928.40       284.28  3.59            3.33             0.25               0.17         3.75             4.04 lamarcus aldridge 15.7 0.270 0.159   3.0  15.8   9.4  11.0   0.8   3.7   7.9  0.5  0.6 1.1  0.080  -0.2  -0.2 -0.3   0.3\n",
      "[guard_advanced_null_regress] 2020-21: all-advanced-NA rows: new=0/535  prev=0\n",
      "[diagnose_advanced_nulls] 2020-21: rows with ALL BBR-only adv cols NA = 0/535\n",
      "  per-col nulls (BBR-only): {'PER': 0, '3PAr': 0, 'BPM': 0, 'DBPM': 0, 'OBPM': 0, 'WS/48': 0, 'WS': 0, 'DWS': 0, 'OWS': 0, 'TOV%': 0, 'BLK%': 0, 'STL%': 0, 'AST%': 0, 'TRB%': 0, 'DRB%': 0, 'ORB%': 0, 'FTr': 0, 'VORP': 0}\n",
      "[update_data] 2020-21 after derived metrics: 77 columns\n",
      "[update_data] 2020-21 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2020-21: MP ≥ 10  → 535→535 rows\n",
      "[filter-shots] 2020-21: ≥50 attempts → 535→457 rows\n",
      "[nan_filter] 2020-21: no columns below threshold; no rows dropped\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2020-21\\part.parquet\n",
      "[update_data] Starting season 2021-22\n",
      "[fetch_season_players] 639 players for 2021-22\n",
      "[update_data] fetched 639 players for 2021-22\n",
      "[update_data] processing 639 players after filter\n",
      "[update_data][WARN] no data for player 'marques bolden' in 2021-22\n",
      "[update_data][WARN] no data for player 'jarrell brantley' in 2021-22\n",
      "[update_data][WARN] no data for player 'deonte burton' in 2021-22\n",
      "[update_data][WARN] no data for player 'michael carter-williams' in 2021-22\n",
      "[update_data][WARN] no data for player 'matthew dellavedova' in 2021-22\n",
      "[update_data][WARN] no data for player 'tyler dorsey' in 2021-22\n",
      "[update_data][WARN] no data for player 'dante exum' in 2021-22\n",
      "[update_data][WARN] no data for player 'harry giles iii' in 2021-22\n",
      "[update_data][WARN] no data for player 'ashton hagans' in 2021-22\n",
      "[update_data][WARN] no data for player 'nigel hayes-davis' in 2021-22\n",
      "[update_data][WARN] no data for player 'jonathan isaac' in 2021-22\n",
      "[update_data][WARN] no data for player 'mfiondu kabengele' in 2021-22\n",
      "[update_data][WARN] no data for player 'skal labissiere' in 2021-22\n",
      "[update_data][WARN] no data for player 'meyers leonard' in 2021-22\n",
      "[update_data][WARN] no data for player 'kawhi leonard' in 2021-22\n",
      "[update_data][WARN] no data for player 'jamal murray' in 2021-22\n",
      "[update_data][WARN] no data for player 'kendrick nunn' in 2021-22\n",
      "[update_data][WARN] no data for player 'jahlil okafor' in 2021-22\n",
      "[update_data][WARN] no data for player 'jontay porter' in 2021-22\n",
      "[update_data][WARN] no data for player 'jason preston' in 2021-22\n",
      "[update_data][WARN] no data for player 'jerome robinson' in 2021-22\n",
      "[update_data][WARN] no data for player 'luka samanic' in 2021-22\n",
      "[update_data][WARN] no data for player 'ben simmons' in 2021-22\n",
      "[update_data][WARN] no data for player 'kobi simmons' in 2021-22\n",
      "[update_data][WARN] no data for player 'edmond sumner' in 2021-22\n",
      "[update_data][WARN] no data for player 'dj stewart' in 2021-22\n",
      "[update_data][WARN] no data for player 'luca vildoza' in 2021-22\n",
      "[update_data][WARN] no data for player 'noah vonleh' in 2021-22\n",
      "[update_data][WARN] no data for player 'john wall' in 2021-22\n",
      "[update_data][WARN] no data for player 't.j. warren' in 2021-22\n",
      "[update_data][WARN] no data for player 'zion williamson' in 2021-22\n",
      "[update_data][WARN] no data for player 'james wiseman' in 2021-22\n",
      "[update_data][WARN] no data for player 'guerschon yabusele' in 2021-22\n",
      "[update_data][WARN] no data for player 'dario saric' in 2021-22\n",
      "[update_data] 2021-22 → DataFrame with 605 rows\n",
      "[filter-early] 2021-22: MP ≥ 10  → 605→577 rows\n",
      "[attach_wins_losses] 2021-22 W/L null% = 0.00\n",
      "[update_data] 2021-22 before derived metrics: 32 columns\n",
      "[update_data] 2021-22 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 28\n",
      "[calculate_percentages] FTA==0 count: 36\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2022_advanced.html\n",
      "[validate_name_encoding] 2021-22: Found 4/4 expected names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  80%|████████  | 12/15 [00:46<00:11,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2021-22: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2021-22: matched 575/577 players (99.7 %)\n",
      "  unmatched sample: ['RUBEN NEMBHARD JR.', 'TREVON SCOTT']\n",
      "[chk:2021-22:post-consolidate] rows=577  cols=77\n",
      "[chk:2021-22:post-derived] rows=577  cols=77\n",
      "                      Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%               player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "            PRECIOUS ACHIUWA 2021-22  TOR 22.0  73  28 1725.0  664 265  603  56  156  78  131  473   82   37   41   84 151  146  327   1630173 1610612761  Forward                 5 209  447 48.59 46.76    48      34 43.95     False 35.90     False 59.54 50.25       13.86        1.71        9.87    7416    1737     950   3308    1085    2596    3681 19608.0    1787    9130.28       744.64  8.16            7.24             0.92               0.33         8.49             9.05         precious achiuwa 12.7 0.259 0.217   8.7  21.7  14.9   6.9   1.1   2.3  11.3  0.4  2.1 2.5  0.070  -2.0  -0.6 -2.6  -0.2\n",
      "                STEVEN ADAMS 2021-22  MEM 28.0  76  75 1999.0  528 210  384   0    1 108  199  760  256   65   60  115 153  349  411    203500 1610612763   Center                11 210  383 54.69 54.83    56      26 54.69     False  0.00     False 54.27 55.98        9.51        4.61       13.69    7733    1896    1023   3569    1158    2868    4026 19754.0    2129    9590.24       586.56  6.12            4.92             1.20               0.99         7.11             8.79             steven adams 17.6 0.003 0.518  17.9  22.0  19.9  16.1   1.6   2.7  19.6  3.8  3.0 6.8  0.163   1.0   1.0  2.0   2.0\n",
      "    NICKEIL ALEXANDER-WALKER 2021-22  NOP 23.0  50  19 1317.0  639 237  632  95  305  70   97  164  139   41   19   85  88   36  128   1629638 1610612740    Guard                 6 142  327 45.02 43.43    36      46 37.50     False 31.15     False 72.16 47.36       17.47        3.80        4.48    6577    1788    1013   2985     934    2572    3506 18384.0    1877    8376.72       759.68  9.07            8.05             1.01               0.61         9.67            10.73 nickeil alexander walker 10.5 0.497 0.160   2.7  11.5   7.1  16.1   1.5   1.5  11.3 -1.1  1.1 0.1  0.003  -1.8  -1.1 -2.9  -0.3\n",
      "[guard_advanced_null_regress] 2021-22: all-advanced-NA rows: new=0/577  prev=0\n",
      "[diagnose_advanced_nulls] 2021-22: rows with ALL BBR-only adv cols NA = 2/577\n",
      "  head sample (5): ['RUBEN NEMBHARD JR.', 'TREVON SCOTT']\n",
      "  per-col nulls (BBR-only): {'PER': 2, '3PAr': 2, 'BPM': 2, 'DBPM': 2, 'OBPM': 2, 'WS/48': 2, 'WS': 2, 'DWS': 2, 'OWS': 2, 'TOV%': 2, 'BLK%': 2, 'STL%': 2, 'AST%': 2, 'TRB%': 2, 'DRB%': 2, 'ORB%': 2, 'FTr': 2, 'VORP': 2}\n",
      "[update_data] 2021-22 after derived metrics: 77 columns\n",
      "[update_data] 2021-22 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2021-22: MP ≥ 10  → 577→577 rows\n",
      "[filter-shots] 2021-22: ≥50 attempts → 577→471 rows\n",
      "[nan_filter] 2021-22: no columns below threshold; no rows dropped\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2021-22\\part.parquet\n",
      "[update_data] Starting season 2022-23\n",
      "[fetch_season_players] 574 players for 2022-23\n",
      "[update_data] fetched 574 players for 2022-23\n",
      "[update_data] processing 574 players after filter\n",
      "[update_data][WARN] no data for player 'd.j. augustin' in 2022-23\n",
      "[update_data][WARN] no data for player 'lonzo ball' in 2022-23\n",
      "[update_data][WARN] no data for player 'ibou badji' in 2022-23\n",
      "[update_data][WARN] no data for player 'marques bolden' in 2022-23\n",
      "[update_data][WARN] no data for player 'miles bridges' in 2022-23\n",
      "[update_data][WARN] no data for player 'armoni brooks' in 2022-23\n",
      "[update_data][WARN] no data for player 'charlie brown jr.' in 2022-23\n",
      "[update_data][WARN] no data for player 'willie cauley-stein' in 2022-23\n",
      "[update_data][WARN] no data for player 'dante exum' in 2022-23\n",
      "[update_data][WARN] no data for player 'danilo gallinari' in 2022-23\n",
      "[update_data][WARN] no data for player 'marcus garrett' in 2022-23\n",
      "[update_data][WARN] no data for player 'harry giles iii' in 2022-23\n",
      "[update_data][WARN] no data for player 'collin gillespie' in 2022-23\n",
      "[update_data][WARN] no data for player 'ashton hagans' in 2022-23\n",
      "[update_data][WARN] no data for player 'nigel hayes-davis' in 2022-23\n",
      "[update_data][WARN] no data for player 'nate hinton' in 2022-23\n",
      "[update_data][WARN] no data for player 'chet holmgren' in 2022-23\n",
      "[update_data][WARN] no data for player 'daquan jeffries' in 2022-23\n",
      "[update_data][WARN] no data for player 'dejon jarreau' in 2022-23\n",
      "[update_data][WARN] no data for player 'mason jones' in 2022-23\n",
      "[update_data][WARN] no data for player 'skal labissiere' in 2022-23\n",
      "[update_data][WARN] no data for player 'e.j. liddell' in 2022-23\n",
      "[update_data][WARN] no data for player 'justin lewis' in 2022-23\n",
      "[update_data][WARN] no data for player 'jahlil okafor' in 2022-23\n",
      "[update_data][WARN] no data for player 'elfrid payton' in 2022-23\n",
      "[update_data][WARN] no data for player 'jontay porter' in 2022-23\n",
      "[update_data][WARN] no data for player 'jahmi'us ramsey' in 2022-23\n",
      "[update_data][WARN] no data for player 'jerome robinson' in 2022-23\n",
      "[update_data][WARN] no data for player 'zavier simpson' in 2022-23\n",
      "[update_data][WARN] no data for player 'javonte smart' in 2022-23\n",
      "[update_data][WARN] no data for player 'isaiah thomas' in 2022-23\n",
      "[update_data][WARN] no data for player 'tristan thompson' in 2022-23\n",
      "[update_data][WARN] no data for player 'brandon williams' in 2022-23\n",
      "[update_data][WARN] no data for player 'd.j. wilson' in 2022-23\n",
      "[update_data][WARN] no data for player 'guerschon yabusele' in 2022-23\n",
      "[update_data] 2022-23 → DataFrame with 539 rows\n",
      "[filter-early] 2022-23: MP ≥ 10  → 539→526 rows\n",
      "[attach_wins_losses] 2022-23 W/L null% = 0.00\n",
      "[update_data] 2022-23 before derived metrics: 32 columns\n",
      "[update_data] 2022-23 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 12\n",
      "[calculate_percentages] FTA==0 count: 15\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2023_advanced.html\n",
      "[validate_name_encoding] 2022-23: Found 4/4 expected names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  87%|████████▋ | 13/15 [00:50<00:07,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2022-23: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2022-23: matched 525/526 players (99.8 %)\n",
      "  unmatched sample: ['NATE WILLIAMS']\n",
      "[chk:2022-23:post-consolidate] rows=526  cols=77\n",
      "[chk:2022-23:post-derived] rows=526  cols=77\n",
      "                      Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%               player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "                OCHAI AGBAJI 2022-23  UTA 23.0  59  22 1209.0  467 165  386  81  228  56   69  121   67   16   15   41  99   43   78   1630534 1610612762    Guard                 3  84  158 53.24 53.16    37      45 42.75     False 35.53     False 81.16 56.08       13.91        2.00        3.60    7236    1937    1191   3419     929    2701    3630 19162.0    2077    9279.28       457.36  4.93            4.49             0.44               0.27         5.20             5.65             ochai agbaji  9.5 0.591 0.179   3.9   6.9   5.4   7.5   0.6   1.0   9.0  0.9  0.4 1.3  0.053  -1.7  -1.4 -3.0  -0.3\n",
      "            PRECIOUS ACHIUWA 2022-23  TOR 23.0  55  12 1141.0  508 196  404  29  108  87  124  328   50   31   30   59 102  100  228   1630173 1610612761  Forward                 5 167  296 52.10 56.42    41      41 48.51     False 26.85     False 70.16 55.39       16.03        1.58       10.35    7180    1848     888   3256     954    2311    3265 18890.0    1888    8881.12       517.56  5.83            5.16             0.66               0.21         6.03             6.39         precious achiuwa 15.2 0.267 0.307   9.3  24.4  16.3   6.3   1.3   2.6  11.4  0.8  1.4 2.2  0.093  -1.4  -0.8 -2.3  -0.1\n",
      "    NICKEIL ALEXANDER-WALKER 2022-23  UTA 24.0  36   3  528.0  228  83  170  35   87  27   39   59   76   24   14   45  58    8   51   1629638 1610612762    Guard                 6  48   83 59.12 57.83    37      45 48.82     False 40.23     False 69.23 60.91       15.55        5.18        4.02    7236    1937    1191   3419     929    2701    3630 19162.0    2077    9279.28       232.16  2.50            2.02             0.48               0.30         2.80             3.32 nickeil alexander walker 11.6 0.539 0.203   1.9  10.5   6.3  16.7   1.7   2.0  14.6  0.3  0.8 1.1  0.062  -1.4   0.4 -0.9   0.2\n",
      "[guard_advanced_null_regress] 2022-23: all-advanced-NA rows: new=0/526  prev=0\n",
      "[diagnose_advanced_nulls] 2022-23: rows with ALL BBR-only adv cols NA = 1/526\n",
      "  head sample (5): ['NATE WILLIAMS']\n",
      "  per-col nulls (BBR-only): {'PER': 1, '3PAr': 1, 'BPM': 1, 'DBPM': 1, 'OBPM': 1, 'WS/48': 1, 'WS': 1, 'DWS': 1, 'OWS': 1, 'TOV%': 1, 'BLK%': 1, 'STL%': 1, 'AST%': 1, 'TRB%': 1, 'DRB%': 1, 'ORB%': 1, 'FTr': 1, 'VORP': 1}\n",
      "[update_data] 2022-23 after derived metrics: 77 columns\n",
      "[update_data] 2022-23 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2022-23: MP ≥ 10  → 526→526 rows\n",
      "[filter-shots] 2022-23: ≥50 attempts → 526→446 rows\n",
      "[nan_filter] 2022-23: no columns below threshold; no rows dropped\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2022-23\\part.parquet\n",
      "[update_data] Starting season 2023-24\n",
      "[fetch_season_players] 596 players for 2023-24\n",
      "[update_data] fetched 596 players for 2023-24\n",
      "[update_data] processing 596 players after filter\n",
      "[update_data][WARN] no data for player 'steven adams' in 2023-24\n",
      "[update_data][WARN] no data for player 'lonzo ball' in 2023-24\n",
      "[update_data][WARN] no data for player 'tony bradley' in 2023-24\n",
      "[update_data][WARN] no data for player 'josh christopher' in 2023-24\n",
      "[update_data][WARN] no data for player 'jaylen clark' in 2023-24\n",
      "[update_data][WARN] no data for player 'terence davis' in 2023-24\n",
      "[update_data][WARN] no data for player 'pj dozier' in 2023-24\n",
      "[update_data][WARN] no data for player 'marcus garrett' in 2023-24\n",
      "[update_data][WARN] no data for player 'jaylin galloway' in 2023-24\n",
      "[update_data][WARN] no data for player 'nigel hayes-davis' in 2023-24\n",
      "[update_data][WARN] no data for player 'malcolm hill' in 2023-24\n",
      "[update_data][WARN] no data for player 'kai jones' in 2023-24\n",
      "[update_data][WARN] no data for player 'christian koloko' in 2023-24\n",
      "[update_data][WARN] no data for player 'skal labissiere' in 2023-24\n",
      "[update_data][WARN] no data for player 'damion lee' in 2023-24\n",
      "[update_data][WARN] no data for player 'jaylen martin' in 2023-24\n",
      "[update_data][WARN] no data for player 'tyrese martin' in 2023-24\n",
      "[update_data][WARN] no data for player 'mac mcclung' in 2023-24\n",
      "[update_data][WARN] no data for player 'jahlil okafor' in 2023-24\n",
      "[update_data][WARN] no data for player 'elfrid payton' in 2023-24\n",
      "[update_data][WARN] no data for player 'kevin porter jr.' in 2023-24\n",
      "[update_data][WARN] no data for player 'duane washington jr.' in 2023-24\n",
      "[update_data][WARN] no data for player 'guerschon yabusele' in 2023-24\n",
      "[update_data][WARN] no data for player 'vlatko cancar' in 2023-24\n",
      "[update_data] 2023-24 → DataFrame with 572 rows\n",
      "[filter-early] 2023-24: MP ≥ 10  → 572→557 rows\n",
      "[attach_wins_losses] 2023-24 W/L null% = 0.00\n",
      "[update_data] 2023-24 before derived metrics: 32 columns\n",
      "[update_data] 2023-24 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 23\n",
      "[calculate_percentages] FTA==0 count: 24\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2024_advanced.html\n",
      "[validate_name_encoding] 2023-24: Found 4/4 expected names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:  93%|█████████▎| 14/15 [00:54<00:03,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2023-24: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2023-24: matched 555/557 players (99.6 %)\n",
      "  unmatched sample: ['MATT HURT', 'NATE WILLIAMS']\n",
      "[chk:2023-24:post-consolidate] rows=557  cols=77\n",
      "[chk:2023-24:post-derived] rows=557  cols=77\n",
      "              Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID       Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%       player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "    PRECIOUS ACHIUWA 2023-24  NYK 24.0  49  18 1187.0  372 157  299  13   50  45   70  351   53   30   56   54 103  141  210   1630173 1610612752        Forward                 5 144  249 54.68 57.83    50      32 52.51     False 26.00     False 64.29 56.40       11.28        1.61       10.65    6074    1480     866   2876     984    2372    3356 16985.0    1792    7591.20       383.80  5.06            4.34             0.71               0.26         5.31             5.75 precious achiuwa 14.6 0.207 0.239  13.0  20.5  16.7   8.4   1.4   3.9  13.8  1.2  2.2 3.4  0.102  -1.7   0.3 -1.4   0.2\n",
      "        SANTI ALDAMA 2023-24  MEM 23.0  61  35 1618.0  654 247  568 106  304  54   87  352  138   43   54   69  89   72  280   1630583 1610612763 Forward-Center                 4 141  264 52.82 53.41    27      55 43.49     False 34.87     False 62.07 53.94       14.55        3.07        7.83    7005    1690    1135   3068     840    2493    3333 19122.0    1937    8883.60       675.28  7.60            6.82             0.78               0.57         8.17             9.15     santi aldama 13.1 0.535 0.153   4.7  19.7  11.9  13.3   1.3   3.1  10.2  0.4  2.0 2.4  0.070  -0.7   0.4 -0.2   0.7\n",
      "        OCHAI AGBAJI 2023-24  UTA 24.0  51  10 1003.0  274 106  249  47  142  15   20  126   47   27   29   34  66   35   91   1630534 1610612762          Guard                 3  59  107 52.01 55.14    31      51 42.57     False 33.10     False 75.00 53.14        9.83        1.69        4.52    7296    1833    1216   3404     987    2708    3695 19594.0    2194    9318.52       291.80  3.13            2.77             0.36               0.19         3.32             3.64     ochai agbaji  7.7 0.487 0.129   4.9   9.6   7.2   6.6   1.4   2.4  12.3 -0.5  0.6 0.1  0.002  -3.5  -0.9 -4.4  -1.0\n",
      "[guard_advanced_null_regress] 2023-24: all-advanced-NA rows: new=0/557  prev=0\n",
      "[diagnose_advanced_nulls] 2023-24: rows with ALL BBR-only adv cols NA = 2/557\n",
      "  head sample (5): ['MATT HURT', 'NATE WILLIAMS']\n",
      "  per-col nulls (BBR-only): {'PER': 2, '3PAr': 2, 'BPM': 2, 'DBPM': 2, 'OBPM': 2, 'WS/48': 2, 'WS': 2, 'DWS': 2, 'OWS': 2, 'TOV%': 2, 'BLK%': 2, 'STL%': 2, 'AST%': 2, 'TRB%': 2, 'DRB%': 2, 'ORB%': 2, 'FTr': 2, 'VORP': 2}\n",
      "[update_data] 2023-24 after derived metrics: 77 columns\n",
      "[update_data] 2023-24 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2023-24: MP ≥ 10  → 557→557 rows\n",
      "[filter-shots] 2023-24: ≥50 attempts → 557→451 rows\n",
      "[nan_filter] 2023-24: dropped 1 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2023-24: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2023-24\\part.parquet\n",
      "[update_data] Starting season 2024-25\n",
      "[update_data] fetched 578 players for 2024-25\n",
      "[update_data] processing 578 players after filter\n",
      "[update_data][WARN] no data for player 'taran armstrong' in 2024-25\n",
      "[update_data][WARN] no data for player 'saddiq bey' in 2024-25\n",
      "[update_data][WARN] no data for player 'kevon harris' in 2024-25\n",
      "[update_data][WARN] no data for player 'nigel hayes-davis' in 2024-25\n",
      "[update_data][WARN] no data for player 'daron holmes ii' in 2024-25\n",
      "[update_data][WARN] no data for player 'isaiah livers' in 2024-25\n",
      "[update_data][WARN] no data for player 'seth lundy' in 2024-25\n",
      "[update_data][WARN] no data for player 'ethan thompson' in 2024-25\n",
      "[update_data][WARN] no data for player 'nikola topic' in 2024-25\n",
      "[update_data] 2024-25 → DataFrame with 569 rows\n",
      "[filter-early] 2024-25: MP ≥ 10  → 569→558 rows\n",
      "[attach_wins_losses] 2024-25 W/L null% = 0.00\n",
      "[update_data] 2024-25 before derived metrics: 32 columns\n",
      "[update_data] 2024-25 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'PlayerID', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 23\n",
      "[calculate_percentages] FTA==0 count: 14\n",
      "Percentage calculations completed with zero-denominator handling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons: 100%|██████████| 15/15 [00:55<00:00,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2024-25: attached 18 cols – ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%']…\n",
      "[merge_advanced_metrics] 2024-25: matched 556/558 players (99.6 %)\n",
      "  unmatched sample: ['RONALD HOLLAND II', 'NATE WILLIAMS']\n",
      "[chk:2024-25:post-consolidate] rows=558  cols=77\n",
      "[chk:2024-25:post-derived] rows=558  cols=77\n",
      "              Player  Season Team  Age  GP  GS     MP  PTS  FG  FGA  3P  3PA  FT  FTA  TRB  AST  STL  BLK  TOV  PF  ORB  DRB  PlayerID     TeamID       Position  Years_of_Service  2P  2PA  eFG%   2P%  Wins  Losses   FG%  3PA_zero   3P%  FTA_zero   FT%   TS%  PTS_per_36  AST_per_36  TRB_per_36  Tm_FGA  Tm_FTA  Tm_TOV  Tm_FG  Tm_ORB  Tm_DRB  Tm_TRB   Tm_MP  Tm_AST  Team_Poss  Player_Poss  USG%  Scoring_Usage%  Turnover_Usage%  Playmaking_Usage%  True_Usage%  Offensive_Load%       player_key  PER  3PAr   FTr  ORB%  DRB%  TRB%  AST%  STL%  BLK%  TOV%  OWS  DWS  WS  WS/48  OBPM  DBPM  BPM  VORP\n",
      "    PRECIOUS ACHIUWA 2024-25  NYK 25.0  57  10 1170.0  379 164  327  10   36  41   69  317   55   47   42   45  81  101  216   1630173 1610612752        Forward                 5 154  291 51.68 52.92    51      31 50.15     False 27.78     False 59.42 53.03       11.66        1.69        9.75    7255    1683     990   3525     876    2592    3468 19634.0    2224    8985.52       402.36  4.48            3.98             0.50               0.23         4.71             5.09 precious achiuwa 13.6 0.110 0.211  10.0  21.0  15.5   6.2   2.0   3.3  11.2  0.7  1.7 2.3  0.095  -2.1   0.2 -1.9   0.0\n",
      "        STEVEN ADAMS 2024-25  HOU 31.0  58   3  794.0  225  91  167   0    2  43   93  327   66   22   28   54  60  166  161    203500 1610612745         Center                11  91  165 54.49 55.15    52      30 54.49     False  0.00     False 46.24 54.11       10.20        2.99       14.83    7646    1847    1068   3479    1200    2769    3969 19744.0    1912    9526.68       261.92  2.75            2.18             0.57               0.26         3.01             3.44     steven adams 16.6 0.012 0.557  21.8  22.0  21.9  10.9   1.3   3.2  20.6  1.0  1.3 2.3  0.137   0.0   0.3  0.3   0.5\n",
      "         BAM ADEBAYO 2024-25  MIA 27.0  78  78 2674.0 1410 540 1113  79  221 251  328  749  337   98   53  161 162  185  564   1628389 1610612748 Center-Forward                 8 461  892 52.07 51.68    37      45 48.52     False 35.75     False 76.52 56.07       18.98        4.54       10.08    6228    1319     894   2866     669    2474    3143 17088.0    1762    7702.36      1418.32 18.41           16.32             2.09               1.62        20.03            22.79      bam adebayo 18.9 0.199 0.295   7.7  23.1  15.5  20.0   1.8   1.9  11.4  3.5  4.0 7.5  0.135   1.0   1.0  2.0   2.7\n",
      "[guard_advanced_null_regress] 2024-25: all-advanced-NA rows: new=0/558  prev=0\n",
      "[diagnose_advanced_nulls] 2024-25: rows with ALL BBR-only adv cols NA = 2/558\n",
      "  head sample (5): ['RONALD HOLLAND II', 'NATE WILLIAMS']\n",
      "  per-col nulls (BBR-only): {'PER': 2, '3PAr': 2, 'BPM': 2, 'DBPM': 2, 'OBPM': 2, 'WS/48': 2, 'WS': 2, 'DWS': 2, 'OWS': 2, 'TOV%': 2, 'BLK%': 2, 'STL%': 2, 'AST%': 2, 'TRB%': 2, 'DRB%': 2, 'ORB%': 2, 'FTr': 2, 'VORP': 2}\n",
      "[update_data] 2024-25 after derived metrics: 77 columns\n",
      "[update_data] 2024-25 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n",
      "[filter-late] 2024-25: MP ≥ 10  → 558→558 rows\n",
      "[filter-shots] 2024-25: ≥50 attempts → 558→475 rows\n",
      "[nan_filter] 2024-25: dropped 2 rows based on 18 low-missing columns (≤ 2.0%)\n",
      "[nan_filter] 2024-25: low-missing columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[update_data] wrote C:\\docker_projects\\coach_analysis\\data\\new_processed\\season=2024-25\\part.parquet\n",
      "✔ Completed pull: 6,495 rows added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[persist] Master Parquet updated – cells:12044  col+:0  col-:0  meanΔ:1\n",
      "Process finished in 57.0 s — log: C:\\docker_projects\\coach_analysis\\data\\stat_pull_output\\stat_pull_log_2025-07-31_12-02-54.txt\n",
      "[check_existing_data] found 26 seasons in C:\\docker_projects\\coach_analysis\\data\\new_processed\n",
      "[load_parquet_data] loading 1 files from C:\\docker_projects\\coach_analysis\\data\\new_processed\n",
      "[validate_season_coverage] expected: ['2023-24']\n",
      "[validate_season_coverage] actual:   ['2023-24']\n",
      "[validate_season_coverage] ✅ coverage OK\n",
      "[report_nulls]\n",
      "           column  null_count  total_rows  null_pct\n",
      "           Player           0         450       0.0\n",
      "           Tm_AST           0         450       0.0\n",
      "      True_Usage%           0         450       0.0\n",
      "Playmaking_Usage%           0         450       0.0\n",
      "  Turnover_Usage%           0         450       0.0\n",
      "   Scoring_Usage%           0         450       0.0\n",
      "             USG%           0         450       0.0\n",
      "      Player_Poss           0         450       0.0\n",
      "        Team_Poss           0         450       0.0\n",
      "            Tm_MP           0         450       0.0\n",
      "       AST_per_36           0         450       0.0\n",
      "           Tm_TRB           0         450       0.0\n",
      "           Tm_DRB           0         450       0.0\n",
      "           Tm_ORB           0         450       0.0\n",
      "            Tm_FG           0         450       0.0\n",
      "           Tm_TOV           0         450       0.0\n",
      "           Tm_FTA           0         450       0.0\n",
      "           Tm_FGA           0         450       0.0\n",
      "  Offensive_Load%           0         450       0.0\n",
      "       player_key           0         450       0.0\n",
      "              PER           0         450       0.0\n",
      "             3PAr           0         450       0.0\n",
      "              BPM           0         450       0.0\n",
      "             DBPM           0         450       0.0\n",
      "             OBPM           0         450       0.0\n",
      "            WS/48           0         450       0.0\n",
      "               WS           0         450       0.0\n",
      "              DWS           0         450       0.0\n",
      "              OWS           0         450       0.0\n",
      "             TOV%           0         450       0.0\n",
      "             BLK%           0         450       0.0\n",
      "             STL%           0         450       0.0\n",
      "             AST%           0         450       0.0\n",
      "             TRB%           0         450       0.0\n",
      "             DRB%           0         450       0.0\n",
      "             ORB%           0         450       0.0\n",
      "              FTr           0         450       0.0\n",
      "       TRB_per_36           0         450       0.0\n",
      "       PTS_per_36           0         450       0.0\n",
      "           Season           0         450       0.0\n",
      "               3P           0         450       0.0\n",
      "              BLK           0         450       0.0\n",
      "              STL           0         450       0.0\n",
      "              AST           0         450       0.0\n",
      "              TRB           0         450       0.0\n",
      "              FTA           0         450       0.0\n",
      "               FT           0         450       0.0\n",
      "              3PA           0         450       0.0\n",
      "              FGA           0         450       0.0\n",
      "              TS%           0         450       0.0\n",
      "               FG           0         450       0.0\n",
      "              PTS           0         450       0.0\n",
      "               MP           0         450       0.0\n",
      "               GS           0         450       0.0\n",
      "               GP           0         450       0.0\n",
      "              Age           0         450       0.0\n",
      "             Team           0         450       0.0\n",
      "              TOV           0         450       0.0\n",
      "               PF           0         450       0.0\n",
      "              ORB           0         450       0.0\n",
      "              DRB           0         450       0.0\n",
      "              FT%           0         450       0.0\n",
      "         FTA_zero           0         450       0.0\n",
      "              3P%           0         450       0.0\n",
      "         3PA_zero           0         450       0.0\n",
      "              FG%           0         450       0.0\n",
      "           Losses           0         450       0.0\n",
      "             Wins           0         450       0.0\n",
      "              2P%           0         450       0.0\n",
      "             eFG%           0         450       0.0\n",
      "              2PA           0         450       0.0\n",
      "               2P           0         450       0.0\n",
      " Years_of_Service           0         450       0.0\n",
      "         Position           0         450       0.0\n",
      "           TeamID           0         450       0.0\n",
      "         PlayerID           0         450       0.0\n",
      "             VORP           0         450       0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\docker_projects\\coach_analysis\\src\\salary_nba_data_pull\\main.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[True True True ... True True True]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  eq_result[non_na_mask] = a[non_na_mask].eq(b[non_na_mask])\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ../src/salary_nba_data_pull/notebook_helper.py\n",
    "\"\"\"\n",
    "Notebook/REPL helper utilities for salary_nba_data_pull.\n",
    "\n",
    "Goals\n",
    "-----\n",
    "• Work no matter where the notebook is opened (absolute paths).\n",
    "• Avoid NameError on __file__.\n",
    "• Keep hot‑reload for iterative dev.\n",
    "• Forward arbitrary args to main() so we can test all scenarios.\n",
    "• Support NaN filtering with configurable thresholds.\n",
    "\n",
    "Use:\n",
    ">>> import salary_nba_data_pull.notebook_helper as nb\n",
    ">>> nb.quick_pull(2024, workers=12, debug=True)\n",
    ">>> nb.quick_pull(2024, nan_filter=True, nan_filter_percentage=0.02)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import sys, importlib, inspect, os\n",
    "from pathlib import Path\n",
    "import requests_cache\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "def _find_repo_root(start: Path | None = None) -> Path:\n",
    "    \"\"\"Find the repository root by looking for pyproject.toml or .git.\"\"\"\n",
    "    markers = {\"pyproject.toml\", \".git\"}\n",
    "    here = (start or Path.cwd()).resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "# Ensure project root & src are on sys.path (defensive)\n",
    "ROOT = _find_repo_root()\n",
    "SRC  = ROOT / \"src\"\n",
    "for p in (ROOT, SRC):\n",
    "    if p.is_dir() and str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "\n",
    "# Sanity print (can be silenced)\n",
    "if __name__ == \"__main__\" or \"JPY_PARENT_PID\" in os.environ:\n",
    "    print(f\"[notebook_helper] sys.path[0:3]={sys.path[:3]}\")\n",
    "\n",
    "# Import after path fix\n",
    "try:\n",
    "    from salary_nba_data_pull import main as nba_main\n",
    "    from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "    from salary_nba_data_pull.fetch_utils import clear_cache as _cc\n",
    "    print(\"✅ salary_nba_data_pull imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import salary_nba_data_pull: {e}\")\n",
    "    print(f\"   ROOT={ROOT}\")\n",
    "    print(f\"   SRC={SRC}\")\n",
    "    print(f\"   sys.path[0:3]={sys.path[:3]}\")\n",
    "    raise\n",
    "    \n",
    "    \n",
    "def _reload():\n",
    "    \"\"\"Reload the main module so code edits are picked up.\"\"\"\n",
    "    importlib.reload(nba_main)\n",
    "\n",
    "def quick_pull(season: int, **kwargs):\n",
    "    \"\"\"\n",
    "    Pull data for a single season with optional NaN filtering.\n",
    "    \n",
    "    Args:\n",
    "        season: Year to pull (e.g., 2024 for 2024-25 season)\n",
    "        **kwargs: Additional arguments passed to main()\n",
    "        \n",
    "    NaN Filtering:\n",
    "        nan_filter: If True, use threshold-aware NaN filtering (default: False)\n",
    "        nan_filter_percentage: Threshold for low-missing columns (default: 0.01 = 1%)\n",
    "        \n",
    "    Examples:\n",
    "        >>> quick_pull(2024, debug=True)  # Legacy behavior\n",
    "        >>> quick_pull(2024, nan_filter=True, nan_filter_percentage=0.02)  # 2% threshold\n",
    "    \"\"\"\n",
    "    _reload()\n",
    "    # Explicitly support nan_filter and its threshold:\n",
    "    nan_filter = kwargs.pop(\"nan_filter\", False)\n",
    "    nan_filter_percentage = kwargs.pop(\"nan_filter_percentage\", 0.01)\n",
    "    print(f\"[quick_pull] season={season}  nan_filter={nan_filter} \"\n",
    "          f\"nan_filter_percentage={nan_filter_percentage}  other_kwargs={kwargs}\")\n",
    "    nba_main.main(\n",
    "        start_year=season,\n",
    "        end_year=season,\n",
    "        nan_filter=nan_filter,\n",
    "        nan_filter_percentage=nan_filter_percentage,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def historical_pull(start_year: int, end_year: int, **kwargs):\n",
    "    \"\"\"\n",
    "    Pull data for multiple seasons with optional NaN filtering.\n",
    "    \n",
    "    Args:\n",
    "        start_year: First year to pull (inclusive)\n",
    "        end_year: Last year to pull (inclusive)\n",
    "        **kwargs: Additional arguments passed to main()\n",
    "        \n",
    "    NaN Filtering:\n",
    "        nan_filter: If True, use threshold-aware NaN filtering (default: False)\n",
    "        nan_filter_percentage: Threshold for low-missing columns (default: 0.01 = 1%)\n",
    "        \n",
    "    Examples:\n",
    "        >>> historical_pull(2022, 2024, debug=True)  # Legacy behavior\n",
    "        >>> historical_pull(2022, 2024, nan_filter=True, nan_filter_percentage=0.02)  # 2% threshold\n",
    "    \"\"\"\n",
    "    _reload()\n",
    "    # Explicitly support nan_filter and its threshold:\n",
    "    nan_filter = kwargs.pop(\"nan_filter\", False)\n",
    "    nan_filter_percentage = kwargs.pop(\"nan_filter_percentage\", 0.01)\n",
    "    print(f\"[historical_pull] {start_year}-{end_year}  nan_filter={nan_filter} \"\n",
    "          f\"nan_filter_percentage={nan_filter_percentage}  other_kwargs={kwargs}\")\n",
    "    nba_main.main(\n",
    "        start_year=start_year,\n",
    "        end_year=end_year,\n",
    "        nan_filter=nan_filter,\n",
    "        nan_filter_percentage=nan_filter_percentage,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def check_existing_data(base: Path | str | None = None) -> list[str]:\n",
    "    base = Path(base) if base else DATA_PROCESSED_DIR\n",
    "    seasons = sorted(d.name.split(\"=\", 1)[-1] for d in base.glob(\"season=*\") if d.is_dir())\n",
    "    print(f\"[check_existing_data] found {len(seasons)} seasons in {base}\")\n",
    "    return seasons\n",
    "\n",
    "def load_parquet_data(season: str | None = None, *, base: Path | str | None = None):\n",
    "    import pandas as pd\n",
    "    base = Path(base) if base else DATA_PROCESSED_DIR\n",
    "    files = list(base.glob(f\"season={season}/part.parquet\")) if season else list(base.glob(\"season=*/part.parquet\"))\n",
    "    if not files:\n",
    "        print(\"[load_parquet_data] No parquet files found.\")\n",
    "        return pd.DataFrame()\n",
    "    print(f\"[load_parquet_data] loading {len(files)} files from {base}\")\n",
    "    return pd.concat((pd.read_parquet(f) for f in files), ignore_index=True)\n",
    "\n",
    "def clear_all_caches():\n",
    "    requests_cache.clear()\n",
    "    _cc()\n",
    "    print(\"✅ caches cleared\")\n",
    "\n",
    "def print_args():\n",
    "    sig = inspect.signature(nba_main.main)\n",
    "    for name, param in sig.parameters.items():\n",
    "        print(f\"{name:<15} default={param.default!r}  kind={param.kind}\")\n",
    "\n",
    "def query_data(sql: str, db: str | None = None):\n",
    "    \"\"\"\n",
    "    Run arbitrary SQL against the DuckDB lake. Example:\n",
    "        query_data(\"SELECT COUNT(*) FROM parquet_scan('data/new_processed/season=*/part.parquet')\")\n",
    "    \"\"\"\n",
    "    import duckdb, pandas as pd\n",
    "    db = db or (DATA_PROCESSED_DIR.parent / \"nba_stats.duckdb\")\n",
    "    with duckdb.connect(str(db), read_only=True) as con:\n",
    "        return con.execute(sql).fetchdf()\n",
    "\n",
    "\n",
    "# ── NEW VALIDATORS ──────────────────────────────────────────────────────────\n",
    "\n",
    "def validate_season_coverage(df: pd.DataFrame,\n",
    "                             expected_seasons: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Check that df['Season'] covers exactly the expected seasons.\n",
    "    Prints missing and extra seasons.\n",
    "    \"\"\"\n",
    "    if \"Season\" not in df.columns:\n",
    "        print(\"[validate_season_coverage] ERROR: no 'Season' column\")\n",
    "        return\n",
    "\n",
    "    actual = sorted(df[\"Season\"].dropna().unique().tolist())\n",
    "    missing = [s for s in expected_seasons if s not in actual]\n",
    "    extra   = [s for s in actual if s not in expected_seasons]\n",
    "\n",
    "    print(f\"[validate_season_coverage] expected: {expected_seasons}\")\n",
    "    print(f\"[validate_season_coverage] actual:   {actual}\")\n",
    "    if missing:\n",
    "        print(f\"[validate_season_coverage] MISSING seasons: {missing}\")\n",
    "    if extra:\n",
    "        print(f\"[validate_season_coverage] EXTRA seasons:   {extra}\")\n",
    "    if not missing and not extra:\n",
    "        print(\"[validate_season_coverage] ✅ coverage OK\")\n",
    "\n",
    "def report_nulls(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise null counts & percentages for each column in df.\n",
    "    Returns a DataFrame with columns: column, null_count, total_rows, null_pct.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    stats = []\n",
    "    for col in df.columns:\n",
    "        nulls = int(df[col].isna().sum())\n",
    "        pct   = 100 * nulls / total if total else 0\n",
    "        stats.append({\n",
    "            \"column\": col,\n",
    "            \"null_count\": nulls,\n",
    "            \"total_rows\": total,\n",
    "            \"null_pct\": round(pct, 2)\n",
    "        })\n",
    "    report = pd.DataFrame(stats).sort_values(\"null_pct\", ascending=False)\n",
    "    print(\"[report_nulls]\")\n",
    "    print(report.to_string(index=False))\n",
    "    return report\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_args()\n",
    "    # quick_pull(2023, workers=4, debug=True)\n",
    "\n",
    "    historical_pull(2010, 2024,        # multi‑season, 2012, 2024,\n",
    "                    workers=6,\n",
    "                    min_avg_minutes=10,\n",
    "                    min_shot_attempts=50,\n",
    "                    overwrite=True,\n",
    "                    debug=True,\n",
    "                    nan_filter=True,\n",
    "                    nan_filter_percentage=0.02)\n",
    "    check_existing_data()              # see which seasons are cached\n",
    "    df = load_parquet_data(\"2023-24\")  # inspect a single season\n",
    "\n",
    "    # Suppose you want exactly that one season:\n",
    "    validate_season_coverage(df, [\"2023-24\"])\n",
    "    # Check nulls:\n",
    "    null_report = report_nulls(df)\n",
    "    # Examine the top 5 columns by null_pct\n",
    "    null_report.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Data Pipeline DAG Architecture\n",
    "\n",
    "## 🏗️ Architecture Overview\n",
    "\n",
    "This document details the **simplified DAG architecture** that focuses on core data sources while removing salary scraping complexity.\n",
    "\n",
    "## 📊 DAG Comparison\n",
    "\n",
    "| Aspect | Monolithic DAG | Split DAGs | Benefit |\n",
    "|--------|----------------|------------|---------|\n",
    "| **Failure Isolation** | One failure blocks all | Isolated failures | ✅ Higher reliability |\n",
    "| **Scheduling** | Single cadence for all | Source-specific cadences | ✅ Optimized resource usage |\n",
    "| **Maintenance** | All-or-nothing updates | Independent iteration | ✅ Faster development |\n",
    "| **Monitoring** | Single SLA for everything | Granular SLAs | ✅ Better observability |\n",
    "| **Parsing Speed** | Large file slows DagBag | Smaller files | ✅ Faster Airflow startup |\n",
    "\n",
    "## 🗓️ Current DAG Set\n",
    "\n",
    "| # | DAG file | Purpose | Schedule | SLA | Retries |\n",
    "|---|----------|---------|----------|-----|---------|\n",
    "| 1 | `nba_advanced_ingest.py` | Advanced metrics (Basketball‑Reference) | `@daily` | 1 h | 2 |\n",
    "| 2 | `injury_etl.py`          | Injury CSV processing | `@monthly` | 1 h | 1 |\n",
    "| 3 | `nba_data_loader.py`     | Load all sources into DuckDB | `@daily` | 3 h | 2 |\n",
    "\n",
    "> **Salary cap**: the yearly cap/parquet is committed by the build pipeline\n",
    "> and version‑controlled; no Airflow DAG is required.\n",
    "\n",
    "### Dependency graph\n",
    "\n",
    "```\n",
    "nba_advanced_ingest ┐\n",
    "injury_etl ├──► nba_data_loader\n",
    "```\n",
    "\n",
    "## 🗓️ DAG Scheduling Strategy\n",
    "\n",
    "### 1. `nba_advanced_ingest` - Daily\n",
    "**Rationale**: Advanced stats update daily\n",
    "- **Schedule**: `@daily`\n",
    "- **SLA**: 1 hour\n",
    "- **Retries**: 2 with 5-minute delays\n",
    "- **Sources**: Basketball-Reference\n",
    "\n",
    "### 2. `injury_etl` - Monthly\n",
    "**Rationale**: Injury data updates monthly\n",
    "- **Schedule**: `@monthly`\n",
    "- **SLA**: 1 hour\n",
    "- **Retries**: 1 with 5-minute delays\n",
    "- **Sources**: Local CSV files\n",
    "\n",
    "### 3. `nba_data_loader` - Daily\n",
    "**Rationale**: Loads all data into DuckDB daily\n",
    "- **Schedule**: `@daily`\n",
    "- **SLA**: 3 hours\n",
    "- **Dependencies**: Advanced metrics and injury ETL via ExternalTaskSensor\n",
    "\n",
    "## 🔗 Dependency Management\n",
    "\n",
    "### ExternalTaskSensor Configuration\n",
    "\n",
    "```python\n",
    "# Wait for advanced metrics\n",
    "wait_advanced = ExternalTaskSensor(\n",
    "    task_id=\"wait_advanced_ingest\",\n",
    "    external_dag_id=\"nba_advanced_ingest\",\n",
    "    external_task_id=\"scrape_advanced_metrics\",\n",
    "    timeout=3600,                     # 1 hour timeout\n",
    "    mode=\"reschedule\",\n",
    "    poke_interval=300,                # Check every 5 minutes\n",
    ")\n",
    "```\n",
    "\n",
    "### Timeout Strategy\n",
    "\n",
    "| DAG | Timeout | Rationale |\n",
    "|-----|---------|-----------|\n",
    "| Daily DAGs | 1 hour | Normal operation time |\n",
    "| Monthly DAGs | 2 hours | Allow for monthly task completion |\n",
    "\n",
    "## 📈 Performance Metrics\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "| Metric | Target | Measurement |\n",
    "|--------|--------|-------------|\n",
    "| **Ingest Success Rate** | >95% | Successful DAG runs / Total runs |\n",
    "| **Data Quality** | >99% | Valid rows / Total rows |\n",
    "| **SLA Compliance** | >90% | On-time completions / Total runs |\n",
    "\n",
    "### Monitoring Dashboard\n",
    "\n",
    "```sql\n",
    "-- DAG Performance Query\n",
    "SELECT \n",
    "    dag_id,\n",
    "    COUNT(*) as total_runs,\n",
    "    AVG(CASE WHEN state = 'success' THEN 1 ELSE 0 END) as success_rate,\n",
    "    AVG(duration) as avg_duration_minutes\n",
    "FROM airflow.task_instance \n",
    "WHERE start_date >= CURRENT_DATE - 30\n",
    "GROUP BY dag_id;\n",
    "```\n",
    "\n",
    "## 🔄 Removed Components\n",
    "\n",
    "### Salary Scraping (Removed)\n",
    "- ❌ `nba_salary_ingest.py` - Player & team salary scraping\n",
    "- ❌ `salary_cap_snapshot.py` - Yearly salary cap scraping\n",
    "- ❌ ESPN/HoopsHype scrapers in `scrape_utils.py`\n",
    "\n",
    "### Salary Cap Handling (Updated)\n",
    "- ✅ **Build pipeline**: Yearly cap data committed to version control\n",
    "- ✅ **No DAG required**: Parquet files pre-baked by build process\n",
    "- ✅ **Loader compatibility**: Still loads cap data if available\n",
    "\n",
    "## 🛠️ Implementation Details\n",
    "\n",
    "### Error Handling Strategy\n",
    "\n",
    "1. **Primary Source Failure**: Graceful degradation when data unavailable\n",
    "2. **Rate Limiting**: Exponential backoff with jitter\n",
    "3. **Data Validation**: Quality gates before loading to DuckDB\n",
    "4. **Alerting**: Email notifications for critical failures\n",
    "\n",
    "### Retry Configuration\n",
    "\n",
    "```python\n",
    "default_args = dict(\n",
    "    retries=2,                           # Standard retries\n",
    "    retry_delay=timedelta(minutes=5),    # Standard delays\n",
    "    sla=timedelta(hours=1),              # Standard SLA\n",
    ")\n",
    "```\n",
    "\n",
    "### Data Quality Gates\n",
    "\n",
    "```python\n",
    "# Quality checks before loading\n",
    "if len(df) == 0:\n",
    "    raise ValueError(f\"No data found for season {season}\")\n",
    "\n",
    "required_cols = [\"Season\", \"Player\", \"Team\"]\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "```\n",
    "\n",
    "## 📊 Cost-Benefit Analysis\n",
    "\n",
    "### Pros of Simplified Architecture\n",
    "\n",
    "| Benefit | Impact | Metric |\n",
    "|---------|--------|--------|\n",
    "| **Reliability** | High | 95%+ uptime per source |\n",
    "| **Maintainability** | High | Independent development cycles |\n",
    "| **Simplicity** | High | Fewer DAGs to manage |\n",
    "| **Monitoring** | High | Granular observability |\n",
    "\n",
    "### Cons of Simplified Architecture\n",
    "\n",
    "| Drawback | Mitigation | Status |\n",
    "|----------|------------|--------|\n",
    "| **Less data sources** | External salary data | ✅ Addressed |\n",
    "| **Reduced functionality** | Core metrics preserved | ✅ Minimized |\n",
    "\n",
    "## 🚀 Deployment Checklist\n",
    "\n",
    "### Pre-Deployment\n",
    "- [x] All DAG files created and tested\n",
    "- [x] Salary scraping removed and stubbed\n",
    "- [x] ExternalTaskSensor dependencies configured\n",
    "- [x] Data quality gates implemented\n",
    "- [x] Monitoring and alerting configured\n",
    "\n",
    "### Deployment\n",
    "- [x] Deploy new DAGs to Airflow\n",
    "- [x] Disable old monolithic DAG\n",
    "- [x] Verify all DAGs are running\n",
    "- [x] Check data flow end-to-end\n",
    "- [x] Monitor for 24 hours\n",
    "\n",
    "### Post-Deployment\n",
    "- [x] Compare performance metrics\n",
    "- [x] Validate data quality\n",
    "- [x] Update documentation\n",
    "- [x] Train team on new architecture\n",
    "\n",
    "## 📚 References\n",
    "\n",
    "- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n",
    "- [ExternalTaskSensor Guide](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html)\n",
    "- [DAG Design Patterns](https://medium.com/@gharikrishnade/airflow-dag-design-patterns-keeping-it-clean-and-modular-ae07bf9b6f11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_api_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_api_ingest.py\n",
    "# dags/nba_api_ingest.py\n",
    "\"\"\"\n",
    "Pulls roster + box‑score data from nba_api once per hour and writes Parquet\n",
    "partitions under data/new_processed/season=<YYYY-YY>/part.parquet.\n",
    "\n",
    "Why hourly?\n",
    "• The NBA Stats endpoints update within minutes after a game ends.\n",
    "• Hourly keeps your lake near‑real‑time without hammering the API.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys, pathlib\n",
    "\n",
    "# Allow `salary_nba_data_pull` imports\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.main import main as pull_main\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,      # explicit\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_api_ingest\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@hourly\",            # unified scheduling API (Airflow ≥ 2.4)\n",
    "    catchup=False,\n",
    "    default_args=default_args,\n",
    "    max_active_runs=1,             # avoid overlapping pulls\n",
    "    tags=[\"nba\", \"api\", \"ingest\"],\n",
    "    params={\"season\": \"2024-25\"},  # visible & overridable in the UI\n",
    ") as dag:\n",
    "\n",
    "    def pull_season(**context):\n",
    "        season = context[\"params\"][\"season\"]\n",
    "        start_year = int(season[:4])\n",
    "        pull_main(\n",
    "            start_year=start_year,\n",
    "            end_year=start_year,\n",
    "            small_debug=True,\n",
    "            workers=8,\n",
    "            overwrite=False,\n",
    "        )\n",
    "\n",
    "    PythonOperator(\n",
    "        task_id=\"scrape_season_data\",\n",
    "        python_callable=pull_season,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_advanced_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_advanced_ingest.py\n",
    "# dags/nba_advanced_ingest.py\n",
    "\"\"\"\n",
    "Daily scrape of Basketball‑Reference season‑level advanced metrics.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.scrape_utils import _season_advanced_df\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_advanced_ingest\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    tags=[\"nba\", \"advanced\", \"ingest\"],\n",
    "    params={\"season\": \"2024-25\"},\n",
    ") as dag:\n",
    "\n",
    "    def scrape_adv(**ctx):\n",
    "        season = ctx[\"params\"][\"season\"]\n",
    "        df = _season_advanced_df(season)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"No advanced data for {season}\")\n",
    "        out_dir = Path(\"/workspace/data/new_processed/advanced_metrics\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_parquet(out_dir / f\"advanced_{season}.parquet\", index=False)\n",
    "\n",
    "    PythonOperator(\n",
    "        task_id=\"scrape_advanced_metrics\",\n",
    "        python_callable=scrape_adv,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_data_loader.py\n",
    "# dags/nba_data_loader.py\n",
    "\"\"\"\n",
    "Fan‑in loader: waits for api_ingest + advanced_ingest + injury_etl,\n",
    "then materialises season tables and a joined view in DuckDB.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import sys, os, duckdb, pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.data_utils import validate_data\n",
    "\n",
    "DATA_ROOT = Path(\"/workspace/data\")\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=3),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_data_loader\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    tags=[\"nba\", \"loader\", \"duckdb\"],\n",
    "    params={\"season\": \"2024-25\"},\n",
    ") as dag:\n",
    "\n",
    "    # ─── sensors (one per upstream DAG) ────────────────────────────────\n",
    "    sensor_args = dict(\n",
    "        poke_interval=300,\n",
    "        mode=\"reschedule\",   # avoids tying up a worker slot\n",
    "    )\n",
    "    wait_api = ExternalTaskSensor(\n",
    "        task_id=\"wait_api_ingest\",\n",
    "        external_dag_id=\"nba_api_ingest\",\n",
    "        external_task_id=\"scrape_season_data\",\n",
    "        timeout=3600,\n",
    "        **sensor_args,\n",
    "    )\n",
    "    wait_adv = ExternalTaskSensor(\n",
    "        task_id=\"wait_advanced_ingest\",\n",
    "        external_dag_id=\"nba_advanced_ingest\",\n",
    "        external_task_id=\"scrape_advanced_metrics\",\n",
    "        timeout=3600,\n",
    "        **sensor_args,\n",
    "    )\n",
    "    wait_injury = ExternalTaskSensor(\n",
    "        task_id=\"wait_injury_etl\",\n",
    "        external_dag_id=\"injury_etl\",\n",
    "        external_task_id=\"process_injury_data\",\n",
    "        timeout=7200,\n",
    "        poke_interval=600,\n",
    "        mode=\"reschedule\",\n",
    "    )\n",
    "\n",
    "    # ─── loader task ───────────────────────────────────────────────────\n",
    "    def load_to_duckdb(**ctx):\n",
    "        season = ctx[\"params\"][\"season\"]\n",
    "        db = DATA_ROOT / \"nba_stats.duckdb\"\n",
    "        con = duckdb.connect(db)\n",
    "        sources = {\n",
    "            f\"player_{season}\": DATA_ROOT / f\"new_processed/season={season}/part.parquet\",\n",
    "            f\"advanced_{season}\": DATA_ROOT / f\"new_processed/advanced_metrics/advanced_{season}.parquet\",\n",
    "            \"injury_master\": DATA_ROOT / \"new_processed/injury_reports/injury_master.parquet\",\n",
    "        }\n",
    "\n",
    "        for alias, path in sources.items():\n",
    "            if path.exists():\n",
    "                if alias.startswith(\"player\"):\n",
    "                    df = pd.read_parquet(path)\n",
    "                    validate_data(df, name=alias, save_reports=True)\n",
    "                con.execute(\n",
    "                    f\"CREATE OR REPLACE TABLE {alias.replace('-', '_')} AS \"\n",
    "                    f\"SELECT * FROM read_parquet('{path}')\"\n",
    "                )\n",
    "\n",
    "        # materialised view – wildcard parquet scan is fine too\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE VIEW v_player_full_{season.replace('-', '_')} AS\n",
    "            SELECT *\n",
    "            FROM player_{season.replace('-', '_')} p\n",
    "            LEFT JOIN advanced_{season.replace('-', '_')} a USING(player, season)\n",
    "            LEFT JOIN injury_master i USING(player, season)\n",
    "        \"\"\")\n",
    "        con.close()\n",
    "\n",
    "    loader = PythonOperator(\n",
    "        task_id=\"validate_and_load\",\n",
    "        python_callable=load_to_duckdb,\n",
    "    )\n",
    "\n",
    "    [wait_api, wait_adv, wait_injury] >> loader "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
