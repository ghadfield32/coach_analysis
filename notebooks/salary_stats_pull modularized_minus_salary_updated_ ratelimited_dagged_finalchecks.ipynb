{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pyproject.toml\n",
    "[build-system]\n",
    "requires = [\"setuptools>=70\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[project]\n",
    "name = \"data_science\"\n",
    "version = \"0.0.1\"\n",
    "description = \"General data science/ML environment\"\n",
    "authors = [{ name = \"Geoffrey Hadfield\" }]\n",
    "requires-python = \">=3.10,<3.12\"   # stay on 3.10/3.11; 3.12 still shaky for some wheels\n",
    "\n",
    "dependencies = [\n",
    "  \"numpy>=1.26\",              # keep modern NumPy\n",
    "  \"pandas>=2.2\",\n",
    "  \"scikit-learn>=1.5\",\n",
    "  \"joblib\",\n",
    "  \"matplotlib\",\n",
    "  \"seaborn\",\n",
    "  \"jupyterlab<5.0\",\n",
    "  \"ipykernel<6.30\",\n",
    "  \"dash\",\n",
    "  \"dash-bootstrap-components\",\n",
    "  \"plotly\",\n",
    "  \"opencv-python-headless\",\n",
    "  \"pillow\",\n",
    "  \"tqdm\",\n",
    "  \"statsmodels\",\n",
    "  \"streamlit\",\n",
    "  \"xgboost\",\n",
    "  \"lightgbm\",\n",
    "  \"requests\",\n",
    "  \"IPython\",\n",
    "  \"tabulate\",\n",
    "  \"pyarrow>=10.0.0\",\n",
    "  \"requests-cache\",\n",
    "  \"diskcache\",\n",
    "  \"unidecode\",\n",
    "  \"cpi>=2.0.0\",\n",
    "  \"lxml\",\n",
    "  \"duckdb>=0.10.0\",\n",
    "  \"apache-airflow>=2.9.0\",\n",
    "  # ---- Explainability stack ----\n",
    "  \"shap>=0.46.0\",             # supports NumPy 2, so fine with 1.26+\n",
    "  \"numba>=0.58.1,<0.61\",      # 0.58.1 adds NumPy 1.26 support; 0.60 adds NumPy2\n",
    "  # llvmlite will be pulled transitively with the correct version\n",
    "  # ---- NBA tooling ----\n",
    "  \"nba_api<=1.4.1\",\n",
    "  \"beautifulsoup4\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "spark = [\n",
    "  \"pyspark\",\n",
    "  \"install-jdk>=1.1.0\",\n",
    "]\n",
    "dev = [\n",
    "  \"pytest\",\n",
    "  \"black\",\n",
    "  \"flake8\",\n",
    "  \"mypy\",\n",
    "]\n",
    "\n",
    "[tool.black]\n",
    "line-length = 88\n",
    "target-version = [\"py310\"]\n",
    "\n",
    "[tool.flake8]\n",
    "max-line-length = 88\n",
    "extend-ignore = [\"E203\"]\n",
    "\n",
    "[tool.mypy]\n",
    "python_version = \"3.10\"\n",
    "ignore_missing_imports = true\n",
    "strict_optional = true\n",
    "\n",
    "[tool.setuptools.packages.find]\n",
    "where = [\"src\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/__init__.py\n",
    "\"\"\"\n",
    "NBA Data Pull Package\n",
    "\n",
    "A comprehensive package for fetching, processing, and analyzing NBA player data\n",
    "including salaries, statistics, and advanced metrics.\n",
    "\"\"\"\n",
    "\n",
    "__version__ = \"0.1.0\"\n",
    "__all__ = [\n",
    "    \"main\",\n",
    "    \"fetch_utils\", \n",
    "    \"process_utils\",\n",
    "    \"scrape_utils\",\n",
    "    \"data_utils\",\n",
    "    \"settings\",\n",
    "    \"notebook_helper\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/settings.py\n",
    "# src/salary_nba_data_pull/settings.py\n",
    "from pathlib import Path\n",
    "import os\n",
    "import typing as _t\n",
    "\n",
    "# ðŸ—‚ï¸  Central data directory (override via env if needed)\n",
    "DATA_PROCESSED_DIR = Path(\n",
    "    (Path(__file__).resolve().parent.parent.parent)  # project root\n",
    "    / \"data\"\n",
    "    / \"new_processed\"\n",
    ")\n",
    "\n",
    "# optional: allow `DATA_PROCESSED_DIR=/tmp/demo python main.py â€¦`\n",
    "ENV_OVERRIDE: _t.Optional[str] = os.getenv(\"DATA_PROCESSED_DIR\")\n",
    "if ENV_OVERRIDE:\n",
    "    DATA_PROCESSED_DIR = Path(ENV_OVERRIDE).expanduser().resolve()\n",
    "\n",
    "# Legacy path for backward compatibility\n",
    "LEGACY_DATA_PROCESSED_DIR = Path(\n",
    "    (Path(__file__).resolve().parent.parent.parent)  # project root\n",
    "    / \"data\"\n",
    "    / \"processed\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/fetch_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/fetch_utils.py\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache, wraps\n",
    "from http import HTTPStatus\n",
    "from typing import Callable\n",
    "import requests\n",
    "from nba_api.stats.endpoints import commonallplayers, commonplayerinfo, playercareerstats, leaguestandings\n",
    "from requests.exceptions import RequestException\n",
    "from json.decoder import JSONDecodeError\n",
    "from joblib import Memory\n",
    "from unidecode import unidecode\n",
    "from tenacity import (\n",
    "    retry, retry_if_exception, wait_random_exponential,\n",
    "    stop_after_attempt, before_log\n",
    ")\n",
    "\n",
    "REQUESTS_PER_MIN = 8   # â†“ a bit safer for long pulls (NBA suggests â‰¤10)\n",
    "_SEM = threading.BoundedSemaphore(REQUESTS_PER_MIN)\n",
    "\n",
    "# Set up joblib memory for caching API responses\n",
    "cache_dir = os.path.join(os.path.dirname(__file__), '../../data/cache/nba_api')\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "\n",
    "def _throttle():\n",
    "    \"\"\"Global semaphore + sleep to stay under REQUESTS_PER_MIN.\"\"\"\n",
    "    _SEM.acquire()\n",
    "    time.sleep(60 / REQUESTS_PER_MIN)\n",
    "    _SEM.release()\n",
    "\n",
    "def _needs_retry(exc: Exception) -> bool:\n",
    "    \"\"\"Return True if we should retry.\"\"\"\n",
    "    if isinstance(exc, requests.HTTPError) and exc.response is not None:\n",
    "        code = exc.response.status_code\n",
    "        if code in (HTTPStatus.TOO_MANY_REQUESTS, HTTPStatus.SERVICE_UNAVAILABLE):\n",
    "            return True\n",
    "    return isinstance(exc, (requests.ConnectionError, requests.Timeout))\n",
    "\n",
    "def _respect_retry_after(resp: requests.Response):\n",
    "    \"\"\"Sleep for serverâ€‘suggested time if header present.\"\"\"\n",
    "    if resp is not None and 'Retry-After' in resp.headers:\n",
    "        try:\n",
    "            sleep = int(resp.headers['Retry-After'])\n",
    "            logging.warning(\"â†º server asked to wait %ss\", sleep)\n",
    "            time.sleep(sleep)\n",
    "        except ValueError:\n",
    "            pass   # header unparsable, ignore\n",
    "\n",
    "def _make_retry(fn: Callable) -> Callable:\n",
    "    \"\"\"Decorator to add tenacity retry with jitter + respect Retry-After.\"\"\"\n",
    "    @retry(\n",
    "        retry=retry_if_exception(_needs_retry),\n",
    "        wait=wait_random_exponential(multiplier=2, max=60),\n",
    "        stop=stop_after_attempt(5),\n",
    "        before_sleep=before_log(logging.getLogger(__name__), logging.WARNING),\n",
    "        reraise=True,\n",
    "    )\n",
    "    @wraps(fn)\n",
    "    def _wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except requests.HTTPError as exc:\n",
    "            _respect_retry_after(exc.response)\n",
    "            raise\n",
    "    return _wrapper\n",
    "\n",
    "@memory.cache\n",
    "@_make_retry\n",
    "def fetch_with_retry(endpoint, *, timeout=90, debug=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Threadâ€‘safe, rateâ€‘limited, cached NBAâ€‘Stats call with adaptive backâ€‘off.\n",
    "    \"\"\"\n",
    "    _throttle()\n",
    "    start = time.perf_counter()\n",
    "    resp = endpoint(timeout=timeout, **kwargs)\n",
    "    df = resp.get_data_frames()[0]\n",
    "    if debug:\n",
    "        logging.debug(\"âœ“ %s in %.1fs %s\", endpoint.__name__,\n",
    "                      time.perf_counter() - start, kwargs)\n",
    "    return df\n",
    "\n",
    "@memory.cache\n",
    "def fetch_all_players(season: str, debug: bool = False) -> dict[str, dict]:\n",
    "    \"\"\"Return {clean_name: {'player_id':â€¦, 'team_id':â€¦}} for *active* roster.\"\"\"\n",
    "    roster_df = fetch_with_retry(\n",
    "        commonallplayers.CommonAllPlayers,\n",
    "        season=season,\n",
    "        is_only_current_season=1,        # <â€‘â€‘ key fix\n",
    "        league_id=\"00\",\n",
    "        debug=debug,\n",
    "    )\n",
    "    players: dict[str, dict] = {}\n",
    "    if roster_df is not None:\n",
    "        for _, row in roster_df.iterrows():\n",
    "            clean = unidecode(row[\"DISPLAY_FIRST_LAST\"]).strip().lower()\n",
    "            players[clean] = {\n",
    "                \"player_id\": int(row[\"PERSON_ID\"]),\n",
    "                \"team_id\": int(row[\"TEAM_ID\"]),\n",
    "            }\n",
    "    if debug:\n",
    "        print(f\"[fetch_all_players] {len(players)} active players for {season}\")\n",
    "    return players\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def fetch_season_players(season: str, debug: bool = False) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Return {clean_name: {'player_id':â€¦, 'team_id':â€¦}} for *everyone who was\n",
    "    on a roster at any time during the given season*.\n",
    "    \"\"\"\n",
    "    # call once for the whole database (not \"currentâ€‘season only\")\n",
    "    df = fetch_with_retry(\n",
    "        commonallplayers.CommonAllPlayers,\n",
    "        season=season,\n",
    "        is_only_current_season=0,         # <-- key change\n",
    "        league_id=\"00\",\n",
    "        debug=debug,\n",
    "    )\n",
    "    players: dict[str, dict] = {}\n",
    "    if df is not None:\n",
    "        yr = int(season[:4])\n",
    "        # keep rows whose career window encloses this season\n",
    "        df = df[(df.FROM_YEAR.astype(int) <= yr) & (df.TO_YEAR.astype(int) >= yr)]\n",
    "        for _, row in df.iterrows():\n",
    "            clean = unidecode(row[\"DISPLAY_FIRST_LAST\"]).strip().lower()\n",
    "            players[clean] = {\n",
    "                \"player_id\": int(row[\"PERSON_ID\"]),\n",
    "                \"team_id\": int(row[\"TEAM_ID\"]),\n",
    "            }\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[fetch_season_players] {len(players)} players for {season}\")\n",
    "    return players\n",
    "\n",
    "@memory.cache\n",
    "def fetch_player_info(player_id, debug=False):\n",
    "    return fetch_with_retry(commonplayerinfo.CommonPlayerInfo, player_id=player_id, debug=debug)\n",
    "\n",
    "@memory.cache\n",
    "def fetch_career_stats(player_id, debug=False):\n",
    "    return fetch_with_retry(playercareerstats.PlayerCareerStats, player_id=player_id, debug=debug)\n",
    "\n",
    "@memory.cache\n",
    "def fetch_league_standings(season, debug=False):\n",
    "    return fetch_with_retry(leaguestandings.LeagueStandings, season=season, debug=debug)\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear the joblib memory cache.\"\"\"\n",
    "    memory.clear()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    debug = True\n",
    "    season = \"2022-23\"\n",
    "    sample_player_name = \"LeBron James\"\n",
    "\n",
    "    # Fetch all players\n",
    "    all_players = fetch_all_players(season, debug=debug)\n",
    "    print(f\"Total players fetched: {len(all_players)}\")\n",
    "\n",
    "    # Fetch player info for a sample player\n",
    "    if sample_player_name.lower() in all_players:\n",
    "        sample_player_id = all_players[sample_player_name.lower()]['player_id']\n",
    "        player_info = fetch_player_info(sample_player_id, debug=debug)\n",
    "        print(f\"Sample player info for {sample_player_name}:\")\n",
    "        print(player_info)\n",
    "\n",
    "        # Fetch career stats for the sample player\n",
    "        career_stats = fetch_career_stats(sample_player_id, debug=debug)\n",
    "        print(f\"Sample player career stats for {sample_player_name}:\")\n",
    "        print(career_stats)\n",
    "    else:\n",
    "        print(f\"Player {sample_player_name} not found in the {season} season data.\")\n",
    "\n",
    "    # Fetch league standings\n",
    "    standings = fetch_league_standings(season, debug=debug)\n",
    "    print(\"League standings:\")\n",
    "    print(standings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/scrape_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/scrape_utils.py\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from typing import Optional\n",
    "import os\n",
    "import requests_cache\n",
    "from unidecode import unidecode\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "from functools import lru_cache\n",
    "import threading\n",
    "_ADV_LOCK   = threading.Lock()\n",
    "_ADV_CACHE: dict[str, pd.DataFrame] = {}   # season -> DataFrame\n",
    "\n",
    "# Install cache for all requests\n",
    "requests_cache.install_cache('nba_scraping', expire_after=86400)  # 24 hours\n",
    "\n",
    "# Create cached session with stale-if-error capability\n",
    "session = requests_cache.CachedSession(\n",
    "    'nba_scraping',\n",
    "    expire_after=86400,\n",
    "    stale_if_error=True       # <-- NEW: serve expired cache if remote 429s\n",
    ")\n",
    "\n",
    "def scrape_salary_cap_history(*, debug: bool = False) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Robust pull of historical cap / tax / apron lines.\n",
    "\n",
    "    Strategy:\n",
    "    1. Try RealGM (live HTML).\n",
    "    2. If the selector fails, look for an existing CSV in DATA_PROCESSED_DIR.\n",
    "    3. As a lastâ€‘chance fallback, hit NBA.com / Reuters bulletins for the\n",
    "       current season only (so we still merge *something*).\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "    url = \"https://basketball.realgm.com/nba/info/salary_cap\"\n",
    "\n",
    "    try:\n",
    "        html = requests.get(url, timeout=30).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # -------- 1ï¸âƒ£  RealGM table (new markup) --------------------\n",
    "        blk = soup.find(\"pre\")                      # new 2025 layout\n",
    "        if blk:                                     # parse fixedâ€‘width block\n",
    "            rows = [r.strip().split() for r in blk.text.strip().splitlines()]\n",
    "            header = rows[0]\n",
    "            data = rows[1:]\n",
    "            df = pd.DataFrame(data, columns=header)\n",
    "        else:\n",
    "            # Legacy table path (kept for safety)\n",
    "            tbl = soup.select_one(\"table\")\n",
    "            if not tbl:\n",
    "                raise ValueError(\"salary_cap table not found\")\n",
    "            df = pd.read_html(str(tbl))[0]\n",
    "\n",
    "        # ---- normalise ----\n",
    "        df[\"Season\"] = df[\"Season\"].str.extract(r\"(\\d{4}-\\d{4})\")\n",
    "        money_cols = [c for c in df.columns if c != \"Season\"]\n",
    "        for c in money_cols:\n",
    "            df[c] = (\n",
    "                df[c]\n",
    "                .astype(str)\n",
    "                .str.replace(r\"[$,]\", \"\", regex=True)\n",
    "                .replace(\"\", pd.NA)\n",
    "                .astype(float)\n",
    "            )\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[salaryâ€‘cap] scraped {len(df)} rows from RealGM\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as exc:\n",
    "        if debug:\n",
    "            print(f\"[salaryâ€‘cap] primary scrape failed â†’ {exc!s}\")\n",
    "\n",
    "        # -------- 2ï¸âƒ£  local cached CSV ----------------------------\n",
    "        fallback = DATA_PROCESSED_DIR / \"salary_cap_history_inflated.csv\"\n",
    "        if fallback.exists():\n",
    "            if debug:\n",
    "                print(f\"[salaryâ€‘cap] using cached CSV at {fallback}\")\n",
    "            return pd.read_csv(fallback)\n",
    "\n",
    "        # -------- 3ï¸âƒ£  NBA.com / Reuters oneâ€‘liner -----------------\n",
    "        try:\n",
    "            # Latest season only\n",
    "            # For now, create a minimal fallback with current season data\n",
    "            year = datetime.now().year\n",
    "            cap = 140.588  # 2024-25 cap as fallback\n",
    "            df = pd.DataFrame(\n",
    "                {\"Season\": [f\"{year}-{str(year+1)[-2:]}\"],\n",
    "                 \"Salary Cap\": [cap * 1_000_000]}\n",
    "            )\n",
    "            if debug:\n",
    "                print(\"[salaryâ€‘cap] built minimal oneâ€‘row DataFrame \"\n",
    "                      \"from fallback values\")\n",
    "            return df\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if debug:\n",
    "        print(\"[salaryâ€‘cap] giving up â€“ no data available\")\n",
    "    return None\n",
    "\n",
    "# User-Agent header to avoid Cloudflare blocks\n",
    "UA = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/126.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 3  # seconds\n",
    "\n",
    "# Define column templates to guarantee DataFrame structure\n",
    "PLAYER_COLS = [\"Player\", \"Salary\", \"Season\"]\n",
    "TEAM_COLS = [\"Team\", \"Team_Salary\", \"Season\"]\n",
    "\n",
    "# Salary parsing pattern\n",
    "_salary_pat = re.compile(r\"\\$?\\d[\\d,]*\")\n",
    "\n",
    "def _clean_salary(text: str) -> int | None:\n",
    "    \"\"\"Return salary as int or None when text has no digits.\"\"\"\n",
    "    m = _salary_pat.search(text)\n",
    "    return int(m.group(0).replace(\",\", \"\").replace(\"$\", \"\")) if m else None\n",
    "\n",
    "# Name normalization pattern with unidecode\n",
    "def _normalise_name(raw: str) -> str:\n",
    "    \"\"\"ASCIIâ€‘fold, trim, lower.\"\"\"\n",
    "    return unidecode(raw).split(\",\")[0].split(\"(\")[0].strip().lower()\n",
    "\n",
    "\n",
    "# ------- INTERNAL HELPER --------\n",
    "def _get_hoopshype_soup(url: str, debug: bool = False) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Hit HoopsHype once with a realistic UA.  \n",
    "    Return BeautifulSoup if the page looks OK, else None.\n",
    "    \"\"\"\n",
    "    for attempt in range(2):\n",
    "        try:\n",
    "            if debug:\n",
    "                print(f\"[fetch] {url} (attempt {attempt+1})\")\n",
    "            resp = requests.get(url, headers=UA, timeout=30)\n",
    "            if resp.status_code != 200:\n",
    "                if debug:\n",
    "                    print(f\"  -> HTTP {resp.status_code}, skipping.\")\n",
    "                return None\n",
    "            html = resp.text\n",
    "            # crude Cloudflare challenge check\n",
    "            if (\"Access denied\" in html) or (\"cf-chl\" in html):\n",
    "                if debug:\n",
    "                    print(\"  -> Cloudflare challenge detected; giving up.\")\n",
    "                return None\n",
    "            return BeautifulSoup(html, \"html.parser\")\n",
    "        except requests.RequestException as e:\n",
    "            if debug:\n",
    "                print(f\"  -> network error {e}, retryingâ€¦\")\n",
    "            time.sleep(2 ** attempt + random.random())\n",
    "    return None\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _espn_salary_url(year: int, page: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Build the new ESPN salary URL. Examples:\n",
    "      page 1 â†’ https://www.espn.com/nba/salaries/_/year/2024/seasontype/4\n",
    "      page 3 â†’ https://www.espn.com/nba/salaries/_/year/2024/page/3/seasontype/4\n",
    "    \"\"\"\n",
    "    base = f\"https://www.espn.com/nba/salaries/_/year/{year}\"\n",
    "    return f\"{base}/seasontype/4\" if page == 1 else f\"{base}/page/{page}/seasontype/4\"\n",
    "\n",
    "\n",
    "def _scrape_espn_player_salaries(season_start: int, debug: bool = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Salary scraping was removed â€“ consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Salary scraping was removed â€“ consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def scrape_player_salary_data(start_season: int, end_season: int,\n",
    "                              player_filter: str | None = None,\n",
    "                              debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Salary scraping was removed â€“ consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Salary scraping was removed â€“ consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _scrape_espn_team_salaries(season: str, debug: bool = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Team salary scraping was removed â€“ consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Team salary scraping was removed â€“ consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def scrape_team_salary_data(season: str, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Team salary scraping was removed â€“ consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Team salary scraping was removed â€“ consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "# --- Seasonâ€‘level advanced stats --------------------------------------------\n",
    "ADV_METRIC_COLS = [\n",
    "    \"PER\", \"TS%\", \"3PAr\", \"FTr\", \"ORB%\", \"DRB%\", \"TRB%\", \"AST%\", \"STL%\", \"BLK%\",\n",
    "    \"TOV%\", \"USG%\", \"OWS\", \"DWS\", \"WS\", \"WS/48\", \"OBPM\", \"DBPM\", \"BPM\", \"VORP\",\n",
    "    \"ORtg\", \"DRtg\",  # extra goodies if you want them\n",
    "]\n",
    "\n",
    "def _season_advanced_df(season: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Threadâ€‘safe, memoised download of the *seasonâ€‘wide* advancedâ€‘stats table.\n",
    "\n",
    "    The first thread to request a given season does the HTTP work while holding\n",
    "    a lock; all others simply wait for the result instead of firing duplicate\n",
    "    requests. The DataFrame is cached inâ€‘process for the life of the run.\n",
    "    \"\"\"\n",
    "    if season in _ADV_CACHE:            # fast path, no lock\n",
    "        return _ADV_CACHE[season]\n",
    "\n",
    "    with _ADV_LOCK:                     # only one thread may enter the block\n",
    "        if season in _ADV_CACHE:        # doubleâ€‘checked locking\n",
    "            return _ADV_CACHE[season]\n",
    "\n",
    "        end_year = int(season[:4]) + 1\n",
    "        url = f\"https://www.basketball-reference.com/leagues/NBA_{end_year}_advanced.html\"\n",
    "        print(f\"[adv] fetching {url}\")\n",
    "        resp = session.get(url, headers=UA, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        df = pd.read_html(StringIO(resp.text), header=0)[0]\n",
    "        df = df[df.Player != \"Player\"]          # drop repeated header rows\n",
    "        df[\"player_key\"] = df.Player.map(_normalise_name)\n",
    "\n",
    "        avail = [c for c in ADV_METRIC_COLS if c in df.columns]\n",
    "        if avail:\n",
    "            df[avail] = df[avail].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "        _ADV_CACHE[season] = df                # memoise\n",
    "        time.sleep(random.uniform(1.5, 2.5))   # be polite\n",
    "        return df\n",
    "\n",
    "def scrape_advanced_metrics(player_name: str,\n",
    "                            season: str,\n",
    "                            *,\n",
    "                            debug: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    O(1) lookup in the cached season DataFrame â€“ zero extra HTTP traffic.\n",
    "    \"\"\"\n",
    "    df = _season_advanced_df(season)\n",
    "    key = _normalise_name(player_name)\n",
    "    row = df.loc[df.player_key == key]\n",
    "    if row.empty:\n",
    "        if debug:\n",
    "            print(f\"[adv] no advanced stats for {player_name} in {season}\")\n",
    "        return {}\n",
    "\n",
    "    row = row.iloc[0]\n",
    "    # Only return columns that actually exist in the DataFrame\n",
    "    available_cols = [col for col in ADV_METRIC_COLS if col in row.index]\n",
    "    result = {col: row[col] for col in available_cols}\n",
    "    if debug:\n",
    "        print(f\"[adv] {player_name} â†’ {result}\")\n",
    "    return result\n",
    "# --- End of new season-level advanced stats ---------------------------------\n",
    "\n",
    "def load_injury_data(\n",
    "    file_path: str | Path | None = None,\n",
    "    *,\n",
    "    base_dir: str | Path | None = None,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the historical injury CSV. By default we look inside the *new*\n",
    "    processed folder; pass ``file_path`` to override a specific file,\n",
    "    or ``base_dir`` to point at a different processed directory.\n",
    "    \"\"\"\n",
    "    root = Path(base_dir) if base_dir else DATA_PROCESSED_DIR\n",
    "    if file_path is None:\n",
    "        file_path = root / \"NBA Player Injury Stats(1951 - 2023).csv\"\n",
    "    file_path = Path(file_path).expanduser().resolve()\n",
    "\n",
    "    try:\n",
    "        injury = (\n",
    "            pd.read_csv(file_path)\n",
    "            .assign(Date=lambda d: pd.to_datetime(d[\"Date\"]))\n",
    "        )\n",
    "        injury[\"Season\"] = injury[\"Date\"].apply(\n",
    "            lambda x: (\n",
    "                f\"{x.year}-{str(x.year + 1)[-2:]}\"\n",
    "                if x.month >= 10\n",
    "                else f\"{x.year - 1}-{str(x.year)[-2:]}\"\n",
    "            )\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"[load_injury_data] loaded {len(injury):,} rows from {file_path}\")\n",
    "        return injury\n",
    "    except FileNotFoundError:\n",
    "        if debug:\n",
    "            print(f\"[load_injury_data] âœ– no injury file at {file_path}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage and testing of all functions\n",
    "    debug = True\n",
    "    start_season = 2022\n",
    "    end_season = 2023\n",
    "    sample_player = \"Ja Morant\"  # Example player\n",
    "\n",
    "    print(\"1. Testing scrape_salary_cap_history:\")\n",
    "    salary_cap_history = scrape_salary_cap_history(debug=debug)\n",
    "\n",
    "    print(\"\\n2. Testing scrape_player_salary_data:\")\n",
    "    player_salary_data = scrape_player_salary_data(start_season, end_season, player_filter=sample_player, debug=debug)\n",
    "\n",
    "    print(\"\\n3. Testing scrape_team_salary_data:\")\n",
    "    team_salary_data = scrape_team_salary_data(f\"{start_season}-{str(start_season+1)[-2:]}\", debug=debug)\n",
    "\n",
    "    print(\"\\n4. Testing scrape_advanced_metrics:\")\n",
    "    advanced_metrics = scrape_advanced_metrics(sample_player, f\"{start_season}-{str(start_season+1)[-2:]}\", debug=debug)\n",
    "    print(f\"Advanced Metrics for {sample_player}:\")\n",
    "    print(advanced_metrics)\n",
    "\n",
    "    print(\"\\n5. Testing load_injury_data and merge_injury_data:\")\n",
    "    injury_data = load_injury_data()\n",
    "    if injury_data is not None:\n",
    "        print(injury_data.head())\n",
    "    else:\n",
    "        print(\"No injury data loaded.\")\n",
    "    if not player_salary_data.empty and injury_data is not None:\n",
    "        from salary_nba_data_pull.process_utils import merge_injury_data\n",
    "        merged_data = merge_injury_data(player_salary_data, injury_data)\n",
    "        print(\"Merged data with injury info:\")\n",
    "        columns_to_display = ['Player', 'Season', 'Salary']\n",
    "        if 'Injured' in merged_data.columns:\n",
    "            columns_to_display.append('Injured')\n",
    "        if 'Injury_Periods' in merged_data.columns:\n",
    "            columns_to_display.append('Injury_Periods')\n",
    "        if 'Total_Days_Injured' in merged_data.columns:\n",
    "            columns_to_display.append('Total_Days_Injured')\n",
    "        if 'Injury_Risk' in merged_data.columns:\n",
    "            columns_to_display.append('Injury_Risk')\n",
    "        print(merged_data[columns_to_display].head())\n",
    "\n",
    "    if not player_salary_data.empty:\n",
    "        avg_salary = player_salary_data['Salary'].mean()\n",
    "        print(f\"Average salary for {sample_player} from {start_season} to {end_season}: ${avg_salary:,.2f}\")\n",
    "\n",
    "    if not team_salary_data.empty:\n",
    "        highest_team_salary = team_salary_data.loc[team_salary_data['Team_Salary'].idxmax()]\n",
    "        print(f\"Team with highest salary in {start_season}-{end_season}: {highest_team_salary['Team']} (${highest_team_salary['Team_Salary']:,.2f})\")\n",
    "\n",
    "    if not injury_data.empty:\n",
    "        injury_count = injury_data['Relinquished'].str.contains(sample_player, case=False).sum()\n",
    "        print(f\"Number of injuries/illnesses for {sample_player} from {start_season} to {end_season}: {injury_count}\")\n",
    "\n",
    "    print(\"\\nAll tests completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/process_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/process_utils.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from salary_nba_data_pull.fetch_utils import fetch_all_players, fetch_career_stats, fetch_player_info, fetch_league_standings\n",
    "from salary_nba_data_pull.scrape_utils import scrape_advanced_metrics\n",
    "\n",
    "# --- CPI lazyâ€‘loader --------------------------------------------------\n",
    "_CPI_AVAILABLE = False  # toggled at runtime\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _ensure_cpi_ready(debug: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Import `cpi` lazily and guarantee its internal SQLite DB is usable.\n",
    "    Returns True when inflation data are available, False otherwise.\n",
    "    \"\"\"\n",
    "    global _CPI_AVAILABLE\n",
    "    try:\n",
    "        import importlib\n",
    "        cpi = importlib.import_module(\"cpi\")        # late import\n",
    "        try:\n",
    "            _ = cpi.models.Series.get_by_id(\"0000\")  # 1â€‘row sanity query\n",
    "            _CPI_AVAILABLE = True\n",
    "            return True\n",
    "        except sqlite3.OperationalError:\n",
    "            if debug:\n",
    "                logging.warning(\"[CPI] DB invalid â€“ rebuilding from BLSâ€¦\")\n",
    "            cpi.update(rebuild=True)                # expensive network call\n",
    "            _CPI_AVAILABLE = True\n",
    "            return True\n",
    "    except ModuleNotFoundError:\n",
    "        if debug:\n",
    "            logging.warning(\"[CPI] package not installed\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            logging.error(\"[CPI] unexpected CPI failure: %s\", e)\n",
    "    return False\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def inflate_value(value: float, year_str: str,\n",
    "                  *, debug: bool = False, skip_inflation: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Inflate `value` from the dollars of `year_str` (YYYY or YYYYâ€‘YY) to 2022 USD.\n",
    "    If CPI data are unavailable or the user opts out, return the original value.\n",
    "    \"\"\"\n",
    "    if skip_inflation or not _ensure_cpi_ready(debug):\n",
    "        return value\n",
    "    try:\n",
    "        import cpi                                       # safe: DB ready\n",
    "        year = int(year_str[:4])\n",
    "        if year >= datetime.now().year:\n",
    "            return value\n",
    "        return float(cpi.inflate(value, year, to=2022))\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            logging.error(\"[CPI] inflate failed for %s: %s\", year_str, e)\n",
    "        return value\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def calculate_percentages(df, debug=False):\n",
    "    \"\"\"\n",
    "    Calculate shooting percentages and other derived statistics.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Calculate shooting percentages\n",
    "    if 'FGA' in df.columns and 'FG' in df.columns:\n",
    "        df['FG%'] = (df['FG'] / df['FGA'] * 100).round(2)\n",
    "        df['FG%'] = df['FG%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if '3PA' in df.columns and '3P' in df.columns:\n",
    "        df['3P%'] = (df['3P'] / df['3PA'] * 100).round(2)\n",
    "        df['3P%'] = df['3P%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'FTA' in df.columns and 'FT' in df.columns:\n",
    "        df['FT%'] = (df['FT'] / df['FTA'] * 100).round(2)\n",
    "        df['FT%'] = df['FT%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    if 'PTS' in df.columns and 'FGA' in df.columns and 'FTA' in df.columns:\n",
    "        df['TS%'] = (df['PTS'] / (2 * (df['FGA'] + 0.44 * df['FTA'])) * 100).round(2)\n",
    "        df['TS%'] = df['TS%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'PTS' in df.columns and 'MP' in df.columns:\n",
    "        df['PTS_per_36'] = (df['PTS'] / df['MP'] * 36).round(2)\n",
    "        df['PTS_per_36'] = df['PTS_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'AST' in df.columns and 'MP' in df.columns:\n",
    "        df['AST_per_36'] = (df['AST'] / df['MP'] * 36).round(2)\n",
    "        df['AST_per_36'] = df['AST_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'TRB' in df.columns and 'MP' in df.columns:\n",
    "        df['TRB_per_36'] = (df['TRB'] / df['MP'] * 36).round(2)\n",
    "        df['TRB_per_36'] = df['TRB_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Percentage calculations completed\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_player_data(player_name: str, season: str,\n",
    "                        all_players: dict[str, dict], *,\n",
    "                        debug: bool = False) -> dict | None:\n",
    "    \"\"\"\n",
    "    Build a singleâ€‘player dict **including Games Started (GS)** and keep the\n",
    "    schema aligned with dataset 1.\n",
    "    \"\"\"\n",
    "    meta = all_players.get(player_name.lower().strip())\n",
    "    if not meta:\n",
    "        return None\n",
    "\n",
    "    pid = meta[\"player_id\"]\n",
    "    info_df   = fetch_player_info(pid, debug=debug)\n",
    "    career_df = fetch_career_stats(pid, debug=debug)\n",
    "    if career_df is None or career_df.empty:\n",
    "        return None\n",
    "\n",
    "    season_row = career_df.loc[career_df.SEASON_ID.eq(season)]\n",
    "    if season_row.empty:\n",
    "        return None\n",
    "    season_row = season_row.iloc[0]\n",
    "\n",
    "    data = {\n",
    "        # ---------- BASIC ------------\n",
    "        \"Player\": player_name,\n",
    "        \"Season\": season,\n",
    "        \"Team\":   season_row[\"TEAM_ABBREVIATION\"],\n",
    "        \"Age\":    season_row[\"PLAYER_AGE\"],\n",
    "        \"GP\":     season_row[\"GP\"],\n",
    "        \"GS\":     season_row.get(\"GS\", 0),        # <-- NEW\n",
    "        \"MP\":     season_row[\"MIN\"],\n",
    "        # ---------- SCORING ----------\n",
    "        \"PTS\": season_row[\"PTS\"],\n",
    "        \"FG\":  season_row[\"FGM\"],  \"FGA\": season_row[\"FGA\"],\n",
    "        \"3P\":  season_row[\"FG3M\"], \"3PA\": season_row[\"FG3A\"],\n",
    "        \"FT\":  season_row[\"FTM\"],  \"FTA\": season_row[\"FTA\"],\n",
    "        # ---------- OTHER ------------\n",
    "        \"TRB\": season_row[\"REB\"], \"AST\": season_row[\"AST\"],\n",
    "        \"STL\": season_row[\"STL\"], \"BLK\": season_row[\"BLK\"],\n",
    "        \"TOV\": season_row[\"TOV\"], \"PF\":  season_row[\"PF\"],\n",
    "        # NEW  â†© rename OREB/DREB so downstream sees ORB/DRB\n",
    "        \"ORB\": season_row.get(\"OREB\", np.nan),\n",
    "        \"DRB\": season_row.get(\"DREB\", np.nan),\n",
    "    }\n",
    "\n",
    "    # roster meta\n",
    "    if info_df is not None and not info_df.empty:\n",
    "        ir = info_df.iloc[0]\n",
    "        data[\"Position\"]          = ir.get(\"POSITION\", \"\")\n",
    "        data[\"TeamID\"]            = ir.get(\"TEAM_ID\", None)\n",
    "        data[\"Years_of_Service\"]  = ir.get(\"SEASON_EXP\", None)\n",
    "    else:\n",
    "        data[\"TeamID\"] = meta.get(\"team_id\")\n",
    "\n",
    "    # ---------- Derived shooting splits ----------\n",
    "    two_att          = data[\"FGA\"] - data[\"3PA\"]\n",
    "    data[\"2P\"]       = data[\"FG\"] - data[\"3P\"]\n",
    "    data[\"2PA\"]      = two_att\n",
    "    data[\"eFG%\"]     = round((data[\"FG\"] + 0.5 * data[\"3P\"]) / data[\"FGA\"] * 100 ,2) if data[\"FGA\"] else None\n",
    "    data[\"2P%\"]      = round(data[\"2P\"] / two_att * 100 ,2)                           if two_att else None\n",
    "\n",
    "    # ---------- Advanced metrics ----------\n",
    "    try:\n",
    "        data.update(scrape_advanced_metrics(player_name, season, debug=debug))\n",
    "    except Exception as exc:\n",
    "        if debug:\n",
    "            logging.warning(\"%s advanced scrape failed: %s\", player_name, exc)\n",
    "\n",
    "    return data\n",
    "\n",
    "def merge_injury_data(player_data: pd.DataFrame,\n",
    "                      injury_data: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach four injuryâ€‘related columns. If a player has no injuries, leave the fields as NA\n",
    "    (pd.NA) instead of empty strings so repeated runs compare equal.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    if player_data.empty:\n",
    "        return player_data\n",
    "\n",
    "    out = player_data.copy()\n",
    "\n",
    "    # Ensure columns exist with NA defaults\n",
    "    defaults = {\n",
    "        \"Injured\": False,\n",
    "        \"Injury_Periods\": pd.NA,\n",
    "        \"Total_Days_Injured\": 0,\n",
    "        \"Injury_Risk\": \"Low Risk\",\n",
    "    }\n",
    "    for c, v in defaults.items():\n",
    "        if c not in out.columns:\n",
    "            out[c] = v\n",
    "\n",
    "    if injury_data is None or injury_data.empty:\n",
    "        # normalize empties just in case\n",
    "        out[\"Injury_Periods\"] = out[\"Injury_Periods\"].replace(\"\", pd.NA)\n",
    "        return out\n",
    "\n",
    "    # Process each player/season\n",
    "    for idx, row in out.iterrows():\n",
    "        pname = row[\"Player\"]\n",
    "        season = row[\"Season\"]\n",
    "\n",
    "        mask = (injury_data[\"Season\"] == season) & \\\n",
    "               (injury_data[\"Relinquished\"].str.contains(pname, case=False, na=False))\n",
    "        player_inj = injury_data.loc[mask]\n",
    "\n",
    "        if player_inj.empty:\n",
    "            continue  # keep defaults\n",
    "\n",
    "        periods = []\n",
    "        total_days = 0\n",
    "        for _, inj in player_inj.iterrows():\n",
    "            start = inj[\"Date\"]\n",
    "            # find the first acquired record after start\n",
    "            got_back = injury_data[\n",
    "                (injury_data[\"Date\"] > start) &\n",
    "                (injury_data[\"Acquired\"].str.contains(pname, case=False, na=False))\n",
    "            ]\n",
    "            if not got_back.empty:\n",
    "                end = got_back.iloc[0][\"Date\"]\n",
    "            else:\n",
    "                end_year = int(season.split(\"-\")[1])\n",
    "                end = pd.Timestamp(f\"{end_year}-06-30\")\n",
    "\n",
    "            total_days += (end - start).days\n",
    "            periods.append(f\"{start:%Y-%m-%d} - {end:%Y-%m-%d}\")\n",
    "\n",
    "        out.at[idx, \"Injured\"] = True\n",
    "        out.at[idx, \"Injury_Periods\"] = \"; \".join(periods) if periods else pd.NA\n",
    "        out.at[idx, \"Total_Days_Injured\"] = total_days\n",
    "\n",
    "        if total_days < 10:\n",
    "            risk = \"Low Risk\"\n",
    "        elif total_days <= 20:\n",
    "            risk = \"Moderate Risk\"\n",
    "        else:\n",
    "            risk = \"High Risk\"\n",
    "        out.at[idx, \"Injury_Risk\"] = risk\n",
    "\n",
    "    # final normalization\n",
    "    out[\"Injury_Periods\"] = out[\"Injury_Periods\"].replace(\"\", pd.NA)\n",
    "\n",
    "    return out\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# USAGE / LOAD METRICS\n",
    "# Inspired by Basketball-Reference (USG%), Nylon Calculus (True Usage parts),\n",
    "# and Thinking Basketball (Offensive Load). See docs in code.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "USAGE_COMPONENT_COLS = [\n",
    "    \"USG%\",               # already scraped but we may recompute if missing\n",
    "    \"Scoring_Usage%\",     # (FGA + 0.44*FTA) share of team poss\n",
    "    \"Playmaking_Usage%\",  # (AST-created FG poss) share\n",
    "    \"Turnover_Usage%\",    # TOV share\n",
    "    \"True_Usage%\",        # Scoring + Playmaking + TO\n",
    "    \"Offensive_Load%\",    # Thinking Basketball style\n",
    "    \"Player_Poss\",        # est. possessions used by player\n",
    "    \"Team_Poss\",          # est. team possessions (for join/QA)\n",
    "]\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return np.where(b == 0, np.nan, a / b)\n",
    "\n",
    "def add_usage_components(df: pd.DataFrame, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute Scoringâ€‘/Playmakingâ€‘/Turnover usage plus Offensiveâ€‘Load.\n",
    "\n",
    "    The helper now:\n",
    "      â€¢ Renames OREB/DREB â†’ ORB/DRB if needed.\n",
    "      â€¢ Warns â€“ but does not crash â€“ when expected stats are missing.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # â”€â”€ 0. Normalise column spelling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    col_map = {\"OREB\": \"ORB\", \"DREB\": \"DRB\"}\n",
    "    out.rename(columns={k: v for k, v in col_map.items() if k in out.columns}, inplace=True)\n",
    "\n",
    "    # â”€â”€ 1. Summarise team totals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    want = [\"FGA\", \"FTA\", \"TOV\", \"FG\", \"ORB\", \"DRB\", \"TRB\", \"MP\", \"AST\"]\n",
    "    have = [c for c in want if c in out.columns]\n",
    "    if len(have) < len(want) and debug:\n",
    "        print(f\"[usage] missing cols â†’ {sorted(set(want) - set(have))}\")\n",
    "\n",
    "    grp = out.groupby([\"Season\", \"Team\"], dropna=False)\n",
    "    team_totals = grp[have].sum(min_count=1).rename(columns=lambda c: f\"Tm_{c}\")\n",
    "    out = out.merge(team_totals, left_on=[\"Season\", \"Team\"], right_index=True, how=\"left\")\n",
    "\n",
    "    # â”€â”€ 2. Possession estimates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    out[\"Team_Poss\"]   = out[\"Tm_FGA\"] + 0.44 * out[\"Tm_FTA\"] + out[\"Tm_TOV\"]\n",
    "    out[\"Player_Poss\"] = out[\"FGA\"]    + 0.44 * out[\"FTA\"]    + out[\"TOV\"]\n",
    "\n",
    "    share = out[\"Player_Poss\"] / out[\"Team_Poss\"]\n",
    "\n",
    "    # fill USG% if missing\n",
    "    if \"USG%\" not in out.columns or out[\"USG%\"].isna().all():\n",
    "        out[\"USG%\"] = (share * 100).round(2)\n",
    "\n",
    "    scor = out[\"FGA\"] + 0.44 * out[\"FTA\"]\n",
    "    tov  = out[\"TOV\"]\n",
    "    ast_cre = 0.37 * out[\"AST\"]\n",
    "\n",
    "    out[\"Scoring_Usage%\"]     = (scor / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"Turnover_Usage%\"]    = (tov  / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"Playmaking_Usage%\"]  = (ast_cre / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"True_Usage%\"]        = (out[\"Scoring_Usage%\"] + out[\"Turnover_Usage%\"] +\n",
    "                                 out[\"Playmaking_Usage%\"]).round(2)\n",
    "\n",
    "    tsa       = scor\n",
    "    creation  = 0.8 * out[\"AST\"]\n",
    "    non_cre   = 0.2 * out[\"AST\"]\n",
    "    out[\"Offensive_Load%\"] = ((tsa + creation + non_cre + tov) / out[\"Team_Poss\"] * 100).round(2)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/data_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/data_utils.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from salary_nba_data_pull.process_utils import (\n",
    "    inflate_value\n",
    ")\n",
    "from salary_nba_data_pull.quality import (\n",
    "    ExpectedSchema, audit_dataframe, write_audit_reports\n",
    ")\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "PRESERVE_EVEN_IF_ALL_NA = {\n",
    "    \"3P%\", \"Injured\", \"Injury_Periods\", \"Total_Days_Injured\", \"Injury_Risk\"\n",
    "}\n",
    "\n",
    "# --- NEW helper ------------------------------------------------------\n",
    "def load_salary_cap_parquet(path: str | Path, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the preâ€‘inflated salaryâ€‘cap parquet file; fall back to CSV loader\n",
    "    if the parquet is not found.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser().with_suffix(\".parquet\")\n",
    "    if path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary-cap] loading Parquet: {path}\")\n",
    "        return pd.read_parquet(path)\n",
    "    # fallback to old CSV helper for legacy compatibility\n",
    "    csv_path = path.with_suffix(\".csv\")\n",
    "    if csv_path.exists():\n",
    "        return load_salary_cap_csv(csv_path, debug=debug)\n",
    "    raise FileNotFoundError(f\"No salaryâ€‘cap parquet or CSV found at {path}\")\n",
    "\n",
    "def load_salary_cap_csv(path: str | Path, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the preprocessed salary cap CSV (inflated) instead of scraping.\n",
    "    We DO NOT fill or coerce silently â€“ if a required column is missing,\n",
    "    we log it and let the caller decide.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser().resolve()\n",
    "    if debug:\n",
    "        print(f\"[salary-cap] loading local file: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    if debug:\n",
    "        print(f\"[salary-cap] rows={len(df)}, cols={df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Remove unnamed columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Remove duplicate columns\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # Remove columns with all NaN values **except** ones we want to keep\n",
    "    all_na = df.columns[df.isna().all()]\n",
    "    to_drop = [c for c in all_na if c not in PRESERVE_EVEN_IF_ALL_NA]\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "    # Remove rows with all NaN values\n",
    "    df = df.dropna(axis=0, how='all')\n",
    "\n",
    "    # Ensure only one 'Season' column exists\n",
    "    season_columns = [col for col in df.columns if 'Season' in col]\n",
    "    if len(season_columns) > 1:\n",
    "        df = df.rename(columns={season_columns[0]: 'Season'})\n",
    "        for col in season_columns[1:]:\n",
    "            df = df.drop(columns=[col])\n",
    "\n",
    "    # Remove '3PAr' and 'FTr' columns\n",
    "    columns_to_remove = ['3PAr', 'FTr']\n",
    "    df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "    # Round numeric columns to 2 decimal places\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_salary_cap_data(player_data: pd.DataFrame,\n",
    "                          salary_cap_data: pd.DataFrame,\n",
    "                          *,\n",
    "                          debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-merge cap data by season-year. Preserve all cap columns even if all NaN.\n",
    "    \"\"\"\n",
    "    if player_data.empty or salary_cap_data.empty:\n",
    "        if debug:\n",
    "            print(\"[merge_salary_cap_data] one side empty -> returning player_data unchanged\")\n",
    "        return player_data\n",
    "\n",
    "    # Make sure we don't mutate originals\n",
    "    p = player_data.copy()\n",
    "    cap = salary_cap_data.copy()\n",
    "\n",
    "    # Extract year\n",
    "    p[\"Season_Year\"]   = p[\"Season\"].str[:4].astype(int)\n",
    "    cap[\"Season_Year\"] = cap[\"Season\"].str[:4].astype(int)\n",
    "\n",
    "    # Inflate cap if not present\n",
    "    if \"Salary_Cap_Inflated\" not in cap.columns:\n",
    "        if debug:\n",
    "            print(\"[merge_salary_cap_data] computing Salary_Cap_Inflated\")\n",
    "        cap[\"Salary_Cap_Inflated\"] = cap.apply(\n",
    "            lambda r: inflate_value(r.get(\"Salary Cap\", np.nan), r.get(\"Season\", \"\")),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Merge\n",
    "    merged = pd.merge(p, cap, on=\"Season_Year\", how=\"left\", suffixes=(\"\", \"_cap\"))\n",
    "\n",
    "    # Figure out which columns came from cap\n",
    "    cap_cols = [c for c in cap.columns if c not in {\"Season_Year\"}]\n",
    "\n",
    "    # For each cap col, if we created a *_cap twin, consolidate\n",
    "    for col in cap_cols:\n",
    "        src = f\"{col}_cap\"\n",
    "        if src in merged.columns:\n",
    "            merged[col] = merged[col].where(~merged[col].isna(), merged[src])\n",
    "            merged.drop(columns=[src], inplace=True)\n",
    "\n",
    "    # Cleanup\n",
    "    merged.drop(columns=[\"Season_Year\"], inplace=True)\n",
    "\n",
    "    # Protect salary-cap columns from being dropped in clean_dataframe\n",
    "    global PRESERVE_EVEN_IF_ALL_NA\n",
    "    PRESERVE_EVEN_IF_ALL_NA = PRESERVE_EVEN_IF_ALL_NA.union(set(cap_cols))\n",
    "\n",
    "    merged = clean_dataframe(merged)\n",
    "\n",
    "    if debug:\n",
    "        miss = [c for c in cap_cols if c not in merged.columns]\n",
    "        if miss:\n",
    "            print(f\"[merge_salary_cap_data] WARNING missing cap cols after merge: {miss}\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "def load_external_salary_data(season: str,\n",
    "                              root: Path | str = DATA_PROCESSED_DIR / \"salary_external\",\n",
    "                              *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read playerâ€‘salary parquet preâ€‘dropped by an upstream job.\n",
    "    Expected path:  {root}/season={YYYY-YY}/part.parquet\n",
    "    \"\"\"\n",
    "    path = Path(root) / f\"season={season}/part.parquet\"\n",
    "    if not path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salaryâ€‘ext] no salary file at {path}\")\n",
    "        return pd.DataFrame(columns=[\"Player\", \"Salary\", \"Season\"])\n",
    "    if debug:\n",
    "        print(f\"[salaryâ€‘ext] loading {path}\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def validate_data(df: pd.DataFrame,\n",
    "                  *,\n",
    "                  name: str = \"player_dataset\",\n",
    "                  save_reports: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Same validation, but salary columns are now OPTIONAL.\n",
    "    \"\"\"\n",
    "    schema = ExpectedSchema(\n",
    "        expected_cols=df.columns,\n",
    "        required_cols=[\"Season\", \"Player\", \"Team\"],   # â€¼ Salary removed\n",
    "        dtypes={\n",
    "            \"Season\": \"object\",\n",
    "            \"Player\": \"object\",\n",
    "        },\n",
    "        # Salary & Team_Salary dropped from nonâ€‘neg / nonâ€‘constant\n",
    "        non_negative_cols=[\"GP\", \"MP\", \"PTS\", \"TRB\", \"AST\"],\n",
    "        non_constant_cols=[\"PTS\"],\n",
    "        unique_key=[\"Season\", \"Player\"]\n",
    "    )\n",
    "\n",
    "    reports = audit_dataframe(df, schema, name=name)\n",
    "\n",
    "    if save_reports:\n",
    "        out_dir = DATA_PROCESSED_DIR / \"audits\"\n",
    "        write_audit_reports(reports, out_dir, prefix=name)\n",
    "\n",
    "    # Print a one-liner summary (optional)\n",
    "    missing_req = reports[\"cols_overview\"].query(\"missing_required == True\")\n",
    "    if not missing_req.empty:\n",
    "        print(f\"[validate_data] Missing required columns: {missing_req['column'].tolist()}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/quality.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/quality.py\n",
    "# src/salary_nba_data_pull/quality.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Mapping, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class ExpectedSchema:\n",
    "    \"\"\"Describe what we *intended* to have in a dataframe.\"\"\"\n",
    "    # All columns we care about (order doesn't matter)\n",
    "    expected_cols: Iterable[str]\n",
    "\n",
    "    # Subset that must be present\n",
    "    required_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Expected pandas dtypes (string form, e.g. 'float64', 'object')\n",
    "    dtypes: Mapping[str, str] = field(default_factory=dict)\n",
    "\n",
    "    # Columns that must be >= 0\n",
    "    non_negative_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Columns that should not be all zeros / all NaN\n",
    "    non_constant_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Unique key columns (together must be unique)\n",
    "    unique_key: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Allowed value sets (enums)\n",
    "    allowed_values: Mapping[str, Iterable[Any]] = field(default_factory=dict)\n",
    "\n",
    "def _series_is_constant(s: pd.Series) -> bool:\n",
    "    return s.nunique(dropna=True) <= 1\n",
    "\n",
    "def audit_dataframe(df: pd.DataFrame,\n",
    "                    schema: ExpectedSchema,\n",
    "                    *,\n",
    "                    name: str = \"dataset\") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Return a dict of small DataFrames summarising quality checks.\n",
    "    Nothing is printed; caller decides how to persist/log.\n",
    "    \"\"\"\n",
    "    exp = set(schema.expected_cols)\n",
    "    req = set(schema.required_cols)\n",
    "\n",
    "    present = set(df.columns)\n",
    "    missing = sorted(list(exp - present))\n",
    "    extra   = sorted(list(present - exp))\n",
    "\n",
    "    # --- Column overview\n",
    "    cols_overview = pd.DataFrame({\n",
    "        \"column\": sorted(list(exp | present)),\n",
    "        \"expected\": [c in exp for c in sorted(list(exp | present))],\n",
    "        \"present\":  [c in present for c in sorted(list(exp | present))],\n",
    "        \"required\": [c in req for c in sorted(list(exp | present))]\n",
    "    })\n",
    "    cols_overview[\"missing_required\"] = cols_overview.apply(\n",
    "        lambda r: r[\"required\"] and not r[\"present\"], axis=1\n",
    "    )\n",
    "\n",
    "    # --- Null report\n",
    "    null_report = (df.isna().sum().to_frame(\"null_count\")\n",
    "                     .assign(total_rows=len(df))\n",
    "                     .assign(null_pct=lambda d: 100 * d[\"null_count\"] / d[\"total_rows\"])\n",
    "                     .reset_index()\n",
    "                     .rename(columns={\"index\": \"column\"}))\n",
    "\n",
    "    # --- Dtype report\n",
    "    type_rows = []\n",
    "    for col in df.columns:\n",
    "        exp_type = schema.dtypes.get(col)\n",
    "        type_rows.append({\n",
    "            \"column\": col,\n",
    "            \"expected_dtype\": exp_type,\n",
    "            \"actual_dtype\": str(df[col].dtype),\n",
    "            \"matches\": (exp_type is None) or (str(df[col].dtype) == exp_type)\n",
    "        })\n",
    "    type_report = pd.DataFrame(type_rows)\n",
    "\n",
    "    # --- Value checks\n",
    "    value_rows = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        series = df[col]\n",
    "        row = {\n",
    "            \"column\": col,\n",
    "            \"min\": series.min(skipna=True),\n",
    "            \"max\": series.max(skipna=True),\n",
    "            \"negatives\": int((series < 0).sum()),\n",
    "            \"zeros\": int((series == 0).sum()),\n",
    "            \"non_zero_pct\": 100 * (series != 0).sum() / len(series),\n",
    "        }\n",
    "        row[\"should_be_non_negative\"] = col in schema.non_negative_cols\n",
    "        row[\"violates_non_negative\"] = row[\"negatives\"] > 0 and row[\"should_be_non_negative\"]\n",
    "        value_rows.append(row)\n",
    "    value_report = pd.DataFrame(value_rows)\n",
    "\n",
    "    # Constant columns\n",
    "    constant_rows = []\n",
    "    for col in df.columns:\n",
    "        constant_rows.append({\n",
    "            \"column\": col,\n",
    "            \"is_constant\": _series_is_constant(df[col]),\n",
    "            \"should_not_be_constant\": col in schema.non_constant_cols\n",
    "        })\n",
    "    constant_report = pd.DataFrame(constant_rows).assign(\n",
    "        violates=lambda d: d[\"is_constant\"] & d[\"should_not_be_constant\"]\n",
    "    )\n",
    "\n",
    "    # Allowed values\n",
    "    enum_rows = []\n",
    "    for col, allowed in schema.allowed_values.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        bad = ~df[col].isin(allowed) & df[col].notna()\n",
    "        enum_rows.append({\n",
    "            \"column\": col,\n",
    "            \"bad_count\": int(bad.sum()),\n",
    "            \"sample_bad\": df.loc[bad, col].drop_duplicates().head(5).tolist()\n",
    "        })\n",
    "    enum_report = pd.DataFrame(enum_rows)\n",
    "\n",
    "    # Unique key\n",
    "    uniq_report = pd.DataFrame()\n",
    "    if schema.unique_key:\n",
    "        dup_mask = df.duplicated(subset=list(schema.unique_key), keep=False)\n",
    "        uniq_report = pd.DataFrame({\n",
    "            \"duplicate_rows\": [int(dup_mask.sum())],\n",
    "            \"subset\": [list(schema.unique_key)]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"cols_overview\": cols_overview,\n",
    "        \"null_report\": null_report,\n",
    "        \"type_report\": type_report,\n",
    "        \"value_report\": value_report,\n",
    "        \"constant_report\": constant_report,\n",
    "        \"enum_report\": enum_report,\n",
    "        \"unique_report\": uniq_report\n",
    "    }\n",
    "\n",
    "def assert_dataframe_ok(df: pd.DataFrame,\n",
    "                        schema: ExpectedSchema,\n",
    "                        *, name: str = \"dataset\") -> None:\n",
    "    \"\"\"\n",
    "    Raise AssertionError with a concise message if critical checks fail.\n",
    "    Designed for pytest or CI.\n",
    "    \"\"\"\n",
    "    rep = audit_dataframe(df, schema, name=name)\n",
    "    bad_missing = rep[\"cols_overview\"].query(\"missing_required == True\")\n",
    "    bad_types = rep[\"type_report\"].query(\"matches == False\")\n",
    "    bad_nonneg = rep[\"value_report\"].query(\"violates_non_negative == True\")\n",
    "    bad_constant = rep[\"constant_report\"].query(\"violates == True\")\n",
    "    dupes = rep[\"unique_report\"][\"duplicate_rows\"].iloc[0] if not rep[\"unique_report\"].empty else 0\n",
    "\n",
    "    msgs = []\n",
    "    if not bad_missing.empty:\n",
    "        msgs.append(f\"Missing required cols: {bad_missing['column'].tolist()}\")\n",
    "    if not bad_types.empty:\n",
    "        msgs.append(f\"Dtype mismatches: {bad_types[['column','expected_dtype','actual_dtype']].to_dict('records')}\")\n",
    "    if not bad_nonneg.empty:\n",
    "        msgs.append(f\"Negative values in non-negative cols: {bad_nonneg['column'].tolist()}\")\n",
    "    if not bad_constant.empty:\n",
    "        msgs.append(f\"Constant-but-shouldn't cols: {bad_constant['column'].tolist()}\")\n",
    "    if dupes:\n",
    "        msgs.append(f\"Duplicate key rows: {dupes}\")\n",
    "\n",
    "    if msgs:\n",
    "        raise AssertionError(f\"[{name}] data quality failures:\\n\" + \"\\n\".join(msgs))\n",
    "\n",
    "def write_audit_reports(reports: Mapping[str, pd.DataFrame],\n",
    "                        out_dir: Path,\n",
    "                        prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Save each report DataFrame as CSV for later inspection.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for key, df in reports.items():\n",
    "        df.to_csv(out_dir / f\"{prefix}_{key}.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/main.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import requests_cache\n",
    "from salary_nba_data_pull.fetch_utils import fetch_all_players, fetch_season_players, fetch_league_standings\n",
    "from salary_nba_data_pull.process_utils import (\n",
    "    process_player_data,\n",
    "    inflate_value,\n",
    "    calculate_percentages,\n",
    "    _ensure_cpi_ready,\n",
    "    add_usage_components,\n",
    ")\n",
    "from salary_nba_data_pull.scrape_utils import (\n",
    "    scrape_salary_cap_history,\n",
    "    load_injury_data,\n",
    "    _season_advanced_df,\n",
    ")\n",
    "from salary_nba_data_pull.process_utils import merge_injury_data\n",
    "from salary_nba_data_pull.data_utils import (\n",
    "    clean_dataframe,\n",
    "    merge_salary_cap_data,\n",
    "    validate_data,\n",
    "    load_salary_cap_csv,\n",
    "    load_salary_cap_parquet,\n",
    "    load_external_salary_data,\n",
    ")\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "# Enable requests-cache for all HTTP traffic\n",
    "requests_cache.install_cache(\"nba_pull\", backend=\"sqlite\", allowable_codes=(200,))\n",
    "\n",
    "# CPI self-test - logs a warning once per run if CPI is unavailable\n",
    "_ensure_cpi_ready(debug=False)\n",
    "\n",
    "# Default number of worker threads\n",
    "DEFAULT_WORKERS = 8                # tweak â‰¤ CPU cores\n",
    "\n",
    "def _almost_equal_numeric(a: pd.Series, b: pd.Series, atol=1e-6, rtol=1e-9):\n",
    "    # Handle NA values first\n",
    "    mask = a.isna() & b.isna()\n",
    "    \n",
    "    # For non-NA values, compare them\n",
    "    both_numeric = pd.api.types.is_numeric_dtype(a) and pd.api.types.is_numeric_dtype(b)\n",
    "    if not both_numeric:\n",
    "        # For non-numeric columns, use pandas equals but handle NA carefully\n",
    "        non_na_mask = ~(a.isna() | b.isna())\n",
    "        eq_result = pd.Series(False, index=a.index)\n",
    "        if non_na_mask.any():\n",
    "            eq_result[non_na_mask] = a[non_na_mask].eq(b[non_na_mask])\n",
    "        return eq_result | mask\n",
    "    else:\n",
    "        # For numeric columns, use numpy isclose\n",
    "        non_na_mask = ~(a.isna() | b.isna())\n",
    "        diff_ok = pd.Series(False, index=a.index)\n",
    "        if non_na_mask.any():\n",
    "            diff_ok[non_na_mask] = np.isclose(\n",
    "                a[non_na_mask].astype(float), \n",
    "                b[non_na_mask].astype(float), \n",
    "                atol=atol, rtol=rtol\n",
    "            )\n",
    "        return diff_ok | mask\n",
    "\n",
    "# helper 1 â”€ column drift\n",
    "def _columns_diff(old_df: pd.DataFrame, new_df: pd.DataFrame):\n",
    "    added   = sorted(set(new_df.columns) - set(old_df.columns))\n",
    "    removed = sorted(set(old_df.columns) - set(new_df.columns))\n",
    "    return added, removed\n",
    "\n",
    "# helper 2 â”€ mean smokeâ€‘test\n",
    "def _mean_diff(old_df: pd.DataFrame, new_df: pd.DataFrame,\n",
    "               tol_pct: float = 0.001) -> pd.DataFrame:\n",
    "    common = old_df.select_dtypes(\"number\").columns.intersection(\n",
    "             new_df.select_dtypes(\"number\").columns)\n",
    "    rows = []\n",
    "    for c in common:\n",
    "        o, n = old_df[c].mean(skipna=True), new_df[c].mean(skipna=True)\n",
    "        if pd.isna(o) or pd.isna(n):\n",
    "            continue\n",
    "        rel = abs(n - o) / (abs(o) + 1e-12)\n",
    "        if rel > tol_pct:\n",
    "            rows.append({\"column\": c, \"old_mean\": o, \"new_mean\": n, \"rel_diff\": rel})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _diff_report(old_df, new_df, key_cols=(\"Season\",\"Player\"),\n",
    "                 numeric_atol=1e-6, numeric_rtol=1e-9, max_print=10):\n",
    "    cols_add, cols_rem = _columns_diff(old_df, new_df)\n",
    "    mean_diffs = _mean_diff(old_df, new_df)\n",
    "\n",
    "    # valueâ€‘level diff (original logic)\n",
    "    common = [c for c in new_df.columns if c in old_df.columns]\n",
    "    old, new = old_df[common], new_df[common]\n",
    "\n",
    "    # Handle case where dataframes have different lengths\n",
    "    if len(old) != len(new):\n",
    "        # If lengths differ, we can't do row-by-row comparison\n",
    "        diffs = []\n",
    "    else:\n",
    "        if all(k in common for k in key_cols):\n",
    "            old = old.sort_values(list(key_cols)).reset_index(drop=True)\n",
    "            new = new.sort_values(list(key_cols)).reset_index(drop=True)\n",
    "        else:\n",
    "            key_cols = (\"__row__\",)\n",
    "            old[\"__row__\"] = new[\"__row__\"] = range(len(old))\n",
    "\n",
    "        diffs = []\n",
    "        for col in common:\n",
    "            equal = _almost_equal_numeric(old[col], new[col],\n",
    "                                          atol=numeric_atol, rtol=numeric_rtol)\n",
    "            for i in np.where(~equal)[0]:\n",
    "                if i < len(old) and i < len(new):  # Safety check\n",
    "                    row_keys = {k: new.iloc[i][k] for k in key_cols}\n",
    "                    diffs.append({**row_keys, \"column\": col,\n",
    "                                  \"old\": old.iloc[i][col], \"new\": new.iloc[i][col]})\n",
    "\n",
    "    is_equal = (not diffs) and (not cols_add) and (not cols_rem) and mean_diffs.empty\n",
    "    summary = (f\"cells:{len(diffs)}  col+:{len(cols_add)}  col-:{len(cols_rem)}  \"\n",
    "               f\"meanÎ”:{len(mean_diffs)}\")\n",
    "    return is_equal, summary, pd.DataFrame(diffs), cols_add, cols_rem, mean_diffs\n",
    "\n",
    "def _file_md5(path: str, chunk: int = 1 << 20) -> str:\n",
    "    \"\"\"Return md5 hexdigest for *path* streaming in 1 MiB chunks.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for blk in iter(lambda: f.read(chunk), b\"\"):\n",
    "            h.update(blk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _season_partition_identical(season: str,\n",
    "                                base_dir: Path | str,\n",
    "                                new_df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if on-disk parquet for `season` is byte-wise equivalent (after\n",
    "    canonical sort & column alignment) to `new_df`.\n",
    "    \"\"\"\n",
    "    ckpt = Path(base_dir) / f\"season={season}\" / \"part.parquet\"\n",
    "    if not ckpt.exists():\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        old_df = pd.read_parquet(ckpt)\n",
    "    except Exception as exc:\n",
    "        logging.warning(\"[identical] failed to read %s â†’ %s\", ckpt, exc)\n",
    "        return False\n",
    "\n",
    "    # STEP B1: align columns and sort only by stable key\n",
    "    cols = sorted(set(old_df.columns) | set(new_df.columns))\n",
    "    key = [\"Season\",\"Player\"]\n",
    "\n",
    "    old_cmp = (old_df.reindex(columns=cols)\n",
    "                     .sort_values(key)\n",
    "                     .reset_index(drop=True))\n",
    "    new_cmp = (new_df.reindex(columns=cols)\n",
    "                     .sort_values(key)\n",
    "                     .reset_index(drop=True))\n",
    "\n",
    "    return old_cmp.equals(new_cmp)   # NaNs treated equal if aligned\n",
    "\n",
    "def _season_partition_exists(season, base_dir):\n",
    "    \"\"\"Check if a season partition already exists in Parquet format.\"\"\"\n",
    "    return os.path.exists(os.path.join(base_dir, f\"season={season}\"))\n",
    "\n",
    "def _player_task(args):\n",
    "    \"\"\"Wrapper for ThreadPoolExecutor.\"\"\"\n",
    "    (player_name, season, salary, all_players, debug) = args\n",
    "    stats = process_player_data(player_name, season, all_players, debug=debug)\n",
    "    if stats:\n",
    "        stats['Salary'] = salary\n",
    "    return stats\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "def update_data(existing_data,\n",
    "                start_year: int,\n",
    "                end_year: int,\n",
    "                *,\n",
    "                player_filter: str = \"all\",\n",
    "                min_avg_minutes: float | None = None,\n",
    "                debug: bool = False,\n",
    "                small_debug: bool = False,          # --- NEW\n",
    "                max_workers: int = 8,\n",
    "                output_base: str | Path = DATA_PROCESSED_DIR,\n",
    "                overwrite: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull seasons in [start_year, end_year] and write under `output_base`.\n",
    "    When `small_debug` is True, suppress perâ€‘player chatter and show only\n",
    "    concise perâ€‘season summaries.\n",
    "    \"\"\"\n",
    "    output_base = Path(output_base)\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Decide low-level debug for helpers\n",
    "    helper_debug = debug and not small_debug\n",
    "\n",
    "    injury = load_injury_data(debug=helper_debug)\n",
    "\n",
    "    # â‡©â‡©  NEW  â‡©â‡©  pull salary from parquet (or leave empty)\n",
    "    salary_dir = Path(output_base).parent / \"salary_external\"\n",
    "    salary_df = pd.concat(\n",
    "        [load_external_salary_data(f\"{y}-{str(y+1)[-2:]}\", root=salary_dir)\n",
    "         for y in range(start_year, end_year + 1)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # if salary not available we'll still proceed\n",
    "    season_has_salary = set(salary_df[\"Season\"].unique())\n",
    "\n",
    "    out_frames: list[pd.DataFrame] = []\n",
    "    season_summaries: list[str] = []  # --- NEW: collect summaries\n",
    "\n",
    "    for y in tqdm(range(start_year, end_year + 1),\n",
    "                  desc=\"Seasons\", disable=small_debug):\n",
    "        season = f\"{y}-{str(y+1)[-2:]}\"\n",
    "        ckpt_dir = output_base / f\"season={season}\"\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # --- 1. Team payroll (removed - no longer scraped)\n",
    "        team_payroll = pd.DataFrame(columns=[\"Team\", \"Team_Salary\", \"Season\"])\n",
    "\n",
    "        # --- 2. Standings (wins/losses)\n",
    "        standings_df = fetch_league_standings(season, debug=helper_debug)\n",
    "        if standings_df is None:\n",
    "            standings_df = pd.DataFrame()\n",
    "\n",
    "        # --- 3. Roster\n",
    "        players_this_season = fetch_season_players(season, debug=helper_debug)\n",
    "        rows = salary_df.query(\"Season == @season\") if season in season_has_salary \\\n",
    "               else pd.DataFrame(columns=[\"Player\", \"Salary\"])\n",
    "        args = [\n",
    "            (row.Player, season, row.Salary, players_this_season, helper_debug)\n",
    "            for _, row in rows.iterrows()\n",
    "        ] if not rows.empty else [\n",
    "            (name.title(), season, None, players_this_season, helper_debug)\n",
    "            for name in players_this_season.keys()\n",
    "        ]\n",
    "\n",
    "        # --- preâ€‘fetch seasonâ€‘wide advanced table so workers reuse the cache\n",
    "        _ = _season_advanced_df(season)        # warm cache under the lock\n",
    "\n",
    "        # --- 4. Player processing in parallel\n",
    "        with ThreadPoolExecutor(max_workers=min(max_workers or DEFAULT_WORKERS, len(args))) as pool:\n",
    "            results, failures = [], 0\n",
    "            for fut in tqdm(as_completed(pool.submit(_player_task, a) for a in args),\n",
    "                            total=len(args), desc=f\"{season} workers\", disable=small_debug):\n",
    "                try:\n",
    "                    res = fut.result()\n",
    "                    if res:\n",
    "                        results.append(res)\n",
    "                except Exception as exc:\n",
    "                    failures += 1\n",
    "                    logging.exception(\"Worker failed for %s: %s\", season, exc)\n",
    "            if failures and debug:\n",
    "                print(f\"âš ï¸  {failures} worker threads raised exceptions\")\n",
    "\n",
    "        missing = rows.loc[~rows.Player.str.lower().isin(players_this_season.keys()),\n",
    "                           \"Player\"].unique()\n",
    "\n",
    "        (ckpt_dir / \"missing_players.txt\").write_text(\"\\n\".join(missing))\n",
    "\n",
    "        df_season = pd.DataFrame(results)\n",
    "        print(f\"[dbg] {season} processed players:\", len(df_season))\n",
    "        \n",
    "        # ---- PROBE: Check for specific duplicate key ----\n",
    "        key = (\"2023-24\", \"Kj Martin\")\n",
    "        if season == \"2023-24\":\n",
    "            probe_count = df_season.query(\"Season == @key[0] & Player == @key[1]\").shape[0]\n",
    "            print(f\"[probe] Kj Martin count in df_season: {probe_count}\")\n",
    "            if probe_count > 1:\n",
    "                print(\"[probe] Kj Martin rows:\")\n",
    "                print(df_season.query(\"Season == @key[0] & Player == @key[1]\")[[\"Season\", \"Player\", \"Team\", \"MP\"]])\n",
    "        \n",
    "        # ---------- season sanity check ----------\n",
    "        if len(df_season) < 150:\n",
    "            logging.warning(\"%s produced only %d rows; retrying after 90 s\", season, len(df_season))\n",
    "            time.sleep(90)\n",
    "            return update_data(existing_data, y, y,  # singleâ€‘season retry\n",
    "                               player_filter=player_filter,\n",
    "                               min_avg_minutes=min_avg_minutes,\n",
    "                               debug=debug,\n",
    "                               small_debug=small_debug,\n",
    "                               max_workers=max_workers,\n",
    "                               output_base=output_base,\n",
    "                               overwrite=True)\n",
    "        if df_season.empty:\n",
    "            # Build tiny summary anyway\n",
    "            season_summaries.append(f\"{season}: 0 players processed.\")\n",
    "            continue\n",
    "\n",
    "        # --- 5. Merge W/L (validate to prevent row blowâ€‘ups)\n",
    "        if not standings_df.empty:\n",
    "            stand_df = standings_df.copy()\n",
    "            if 'W' in stand_df.columns:\n",
    "                stand_df.rename(columns={'W': 'Wins', 'L': 'Losses'}, inplace=True)\n",
    "            if 'WINS' in stand_df.columns:\n",
    "                stand_df.rename(columns={'WINS': 'Wins', 'LOSSES': 'Losses'}, inplace=True)\n",
    "            if 'TEAM_ID' in stand_df.columns:\n",
    "                stand_df.rename(columns={'TEAM_ID': 'TeamID'}, inplace=True)\n",
    "            \n",
    "            print(f\"[dbg] {season} before standings merge:\", len(df_season))\n",
    "            df_season = pd.merge(\n",
    "                df_season,\n",
    "                stand_df[['TeamID', 'Wins', 'Losses']].drop_duplicates('TeamID'),\n",
    "                on='TeamID', how='left', validate='m:1'\n",
    "            )\n",
    "            print(f\"[dbg] {season} after standings merge:\", len(df_season))\n",
    "\n",
    "        # --- 6. Team payroll merge (removed - no longer merged)\n",
    "        merged_tmp2 = df_season if min_avg_minutes is None else df_season.query(\"MP >= @min_avg_minutes\")\n",
    "        print(f\"[dbg] {season} after MP filter:\", len(merged_tmp2))\n",
    "        \n",
    "        merged_tmp3 = merged_tmp2.pipe(merge_injury_data, injury_data=injury)\n",
    "        print(f\"[dbg] {season} after injury merge:\", len(merged_tmp3))\n",
    "        \n",
    "        merged = (merged_tmp3\n",
    "                    .pipe(calculate_percentages, debug=helper_debug)\n",
    "                    # â”€â”€ deep breath â”€â”€ add component usage & load\n",
    "                    .pipe(add_usage_components, debug=helper_debug)\n",
    "                    .pipe(clean_dataframe))\n",
    "        \n",
    "        # ---- FINAL: enforce key uniqueness ----\n",
    "        dups = merged.duplicated(subset=[\"Season\",\"Player\"], keep=False)\n",
    "        if dups.any():\n",
    "            print(f\"[dbg] {season} DUPLICATE KEYS detected ({dups.sum()} rows). Dumping...\")\n",
    "            print(merged.loc[dups, [\"Season\",\"Player\",\"Team\",\"MP\"]]\n",
    "                        .sort_values([\"Player\",\"Team\"]))\n",
    "            # Hard fail so we never persist dirty data:\n",
    "            raise AssertionError(f\"Duplicate (Season,Player) keys in season {season}\")\n",
    "\n",
    "        # STEP A1: deterministic sort & string normalization\n",
    "        key_cols = [\"Season\",\"Player\"]\n",
    "        merged = merged.sort_values(key_cols).reset_index(drop=True)\n",
    "        obj_cols = merged.select_dtypes(include=[\"object\"]).columns\n",
    "        for c in obj_cols:\n",
    "            merged[c] = merged[c].replace(r\"^\\s*$\", pd.NA, regex=True)\n",
    "\n",
    "        print(f\"[dbg] {season} final merged:\", len(merged))\n",
    "\n",
    "        # Skip identical season unless overwrite (moved here to use merged DataFrame)\n",
    "        if (not overwrite\n",
    "            and (ckpt_dir / \"part.parquet\").exists()\n",
    "            and _season_partition_identical(season, output_base, merged)):\n",
    "            if debug and not small_debug:\n",
    "                print(f\"âœ“  {season} unchanged â€“ skipping\")\n",
    "            out_frames.append(merged)\n",
    "            continue\n",
    "        elif debug and not small_debug and (ckpt_dir / \"part.parquet\").exists():\n",
    "            print(f\"â†»  {season} differs â€“ re-scraping\")\n",
    "\n",
    "        parquet_path = ckpt_dir / \"part.parquet\"\n",
    "        merged.to_parquet(parquet_path, index=False)\n",
    "        (ckpt_dir / \"part.md5\").write_text(_file_md5(parquet_path))\n",
    "\n",
    "        out_frames.append(merged)\n",
    "        logging.info(\"wrote %s\", ckpt_dir)\n",
    "\n",
    "        # --- NEW: concise summary\n",
    "        if small_debug:\n",
    "            n_players = len(merged)\n",
    "            n_missing = len(missing)\n",
    "            n_cols = merged.shape[1]\n",
    "            season_summaries.append(\n",
    "                f\"{season}: {n_players} rows, {n_missing} missing roster matches, {n_cols} cols.\"\n",
    "            )\n",
    "\n",
    "    # Print all summaries once\n",
    "    if small_debug and season_summaries:\n",
    "        print(\"\\n--- Season Summaries ---\")\n",
    "        for line in season_summaries:\n",
    "            print(line)\n",
    "        print(\"------------------------\\n\")\n",
    "\n",
    "    return pd.concat(out_frames, ignore_index=True) if out_frames else pd.DataFrame()\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Return a filesystem-safe timestamp string.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def remove_old_logs(log_dir, days_to_keep=7):\n",
    "    current_time = datetime.now()\n",
    "    for log_file in glob.glob(os.path.join(log_dir, 'stat_pull_log_*.txt')):\n",
    "        file_modified_time = datetime.fromtimestamp(os.path.getmtime(log_file))\n",
    "        if current_time - file_modified_time > timedelta(days=days_to_keep):\n",
    "            os.remove(log_file)\n",
    "\n",
    "def persist_final_dataset(new_data: pd.DataFrame, seasons_loaded: list[str],\n",
    "                          *, output_base: Path, debug: bool = False,\n",
    "                          numeric_atol: float = 1e-6, numeric_rtol: float = 1e-9,\n",
    "                          max_print: int = 15, mean_tol_pct: float = 0.001) -> None:\n",
    "    final_parquet = output_base / \"nba_player_data_final_inflated.parquet\"\n",
    "    join_keys = [\"Season\", \"Player\"]\n",
    "\n",
    "    old_master = (pd.read_parquet(final_parquet)\n",
    "                  if final_parquet.exists() else\n",
    "                  pd.DataFrame(columns=new_data.columns))\n",
    "\n",
    "    for df in (old_master, new_data):\n",
    "        for k in join_keys:\n",
    "            if k in df.columns:\n",
    "                df[k] = df[k].astype(str).str.strip()\n",
    "\n",
    "    old_slice = old_master.merge(\n",
    "        pd.DataFrame({\"Season\": seasons_loaded}).drop_duplicates(),\n",
    "        on=\"Season\", how=\"inner\").reset_index(drop=True)\n",
    "    new_slice = new_data.reset_index(drop=True)\n",
    "\n",
    "    equal, summary, diff_cells, cols_add, cols_rem, mean_diffs = \\\n",
    "        _diff_report(old_slice, new_slice, key_cols=join_keys,\n",
    "                     numeric_atol=numeric_atol, numeric_rtol=numeric_rtol)\n",
    "\n",
    "    # Special case: if old_slice is empty but new_slice has data, we should write\n",
    "    if len(old_slice) == 0 and len(new_slice) > 0:\n",
    "        equal = False\n",
    "        if debug:\n",
    "            print(\"[persist] Creating new master parquet with fresh data\")\n",
    "\n",
    "    if equal:\n",
    "        if debug:\n",
    "            print(\"[persist] No changes detected â€“ master Parquet left untouched\")\n",
    "        return\n",
    "\n",
    "    audits = output_base / \"audits\"; audits.mkdir(parents=True, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    if cols_add or cols_rem:\n",
    "        pd.DataFrame({\"added\": [cols_add], \"removed\": [cols_rem]}\n",
    "                     ).to_csv(audits / f\"column_changes_{ts}.csv\", index=False)\n",
    "    if not mean_diffs.empty:\n",
    "        mean_diffs.to_csv(audits / f\"mean_diffs_{ts}.csv\", index=False)\n",
    "    if not diff_cells.empty:\n",
    "        diff_cells.to_csv(audits / f\"value_diffs_{ts}.csv\", index=False)\n",
    "\n",
    "    # ----- rewrite master -----\n",
    "    union_cols = sorted(set(old_master.columns) | set(new_data.columns))\n",
    "    remover = old_master.merge(\n",
    "        pd.DataFrame({\"Season\": seasons_loaded}), on=\"Season\",\n",
    "        how=\"left\", indicator=True)\n",
    "    remover = remover[remover[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
    "    remover = remover.reindex(columns=union_cols)\n",
    "    new_slice = new_slice.reindex(columns=union_cols)\n",
    "\n",
    "    updated_master = pd.concat([remover, new_slice], ignore_index=True)\\\n",
    "                       .sort_values(join_keys).reset_index(drop=True)\n",
    "    updated_master.to_parquet(final_parquet, index=False)\n",
    "    if debug: print(f\"[persist] Master Parquet updated â€“ {summary}\")\n",
    "\n",
    "def main(start_year: int,\n",
    "         end_year: int,\n",
    "         player_filter: str = \"all\",\n",
    "         min_avg_minutes: float = 15,\n",
    "         debug: bool = False,\n",
    "         small_debug: bool = False,      # --- NEW\n",
    "         workers: int = 8,\n",
    "         overwrite: bool = False,\n",
    "         output_base: str | Path = DATA_PROCESSED_DIR) -> None:\n",
    "    \"\"\"\n",
    "    Entry point. `small_debug=True` prints only highâ€‘signal info.\n",
    "    If both `debug` and `small_debug` are True, `debug` wins (full noise).\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    output_base = Path(output_base)\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_dir = output_base.parent / \"stat_pull_output\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remove_old_logs(log_dir)\n",
    "\n",
    "    log_file = log_dir / f\"stat_pull_log_{get_timestamp()}.txt\"\n",
    "    logging.basicConfig(filename=log_file,\n",
    "                        level=logging.DEBUG if debug else logging.INFO,\n",
    "                        format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    updated = update_data(None, start_year, end_year,\n",
    "                          player_filter=player_filter,\n",
    "                          min_avg_minutes=min_avg_minutes,\n",
    "                          debug=debug,\n",
    "                          small_debug=small_debug,          # --- NEW\n",
    "                          max_workers=workers,\n",
    "                          output_base=str(output_base),\n",
    "                          overwrite=overwrite)\n",
    "\n",
    "    if not small_debug:  # keep your old prints in full/quiet modes\n",
    "        print(f\"âœ” Completed pull: {len(updated):,} rows added\")\n",
    "\n",
    "    if not updated.empty:\n",
    "        # ---------------- Salary Cap -----------------\n",
    "        # Prefer local Parquet; fallback to CSV, then scrape only if file missing and user allows\n",
    "        cap_file = Path(output_base) / \"salary_cap_history_inflated\"\n",
    "        use_scrape = False\n",
    "\n",
    "        try:\n",
    "            salary_cap = load_salary_cap_parquet(cap_file, debug=debug and not small_debug)\n",
    "        except FileNotFoundError:\n",
    "            # LAST resort â€“ scrape (can be disabled permanently by setting use_scrape=False)\n",
    "            if debug and not small_debug:\n",
    "                print(\"[salary-cap] local file missing, attempting scrapeâ€¦\")\n",
    "            salary_cap = scrape_salary_cap_history(debug=debug and not small_debug)\n",
    "            if salary_cap is not None:\n",
    "                # Save as both Parquet and CSV for compatibility\n",
    "                salary_cap.to_parquet(f\"{cap_file}.parquet\", index=False)\n",
    "                salary_cap.to_csv(f\"{cap_file}.csv\", index=False)\n",
    "\n",
    "        if salary_cap is not None:\n",
    "            updated = merge_salary_cap_data(updated, salary_cap, debug=debug and not small_debug)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"[salary-cap] No data merged â€” check local file path.\")\n",
    "\n",
    "        # --------------- Validate --------------------\n",
    "        updated = validate_data(updated, name=\"player_dataset\", save_reports=True)\n",
    "\n",
    "        seasons_this_run = sorted(updated[\"Season\"].unique().tolist())\n",
    "        persist_final_dataset(updated,\n",
    "                              seasons_loaded=seasons_this_run,\n",
    "                              output_base=output_base,\n",
    "                              debug=debug)\n",
    "\n",
    "    if not small_debug:\n",
    "        print(f\"Process finished in {time.time() - t0:.1f} s â€” log: {log_file}\")\n",
    "    else:\n",
    "        # minimal closing line\n",
    "        print(f\"Done in {time.time() - t0:.1f}s. Log: {log_file}\")\n",
    "        \n",
    "# ----------------------------------------------------------------------\n",
    "# argparse snippet\n",
    "if __name__ == \"__main__\":\n",
    "    cur = datetime.now().year\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--start_year\", type=int, default=cur-1)\n",
    "    p.add_argument(\"--end_year\",   type=int, default=cur)\n",
    "    p.add_argument(\"--player_filter\", default=\"all\")\n",
    "    p.add_argument(\"--min_avg_minutes\", type=float, default=15)\n",
    "    p.add_argument(\"--debug\", action=\"store_true\")\n",
    "    p.add_argument(\"--small_debug\", action=\"store_true\")   # --- NEW\n",
    "    p.add_argument(\"--workers\", type=int, default=8)\n",
    "    p.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "    p.add_argument(\"--output_base\",\n",
    "                   default=str(DATA_PROCESSED_DIR),\n",
    "                   help=\"Destination root for parquet + csv outputs\")\n",
    "    args = p.parse_args()\n",
    "    main(**vars(args))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/notebook_helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/notebook_helper.py\n",
    "\"\"\"\n",
    "Notebook/REPL helper utilities for salary_nba_data_pull.\n",
    "\n",
    "Goals\n",
    "-----\n",
    "â€¢ Work no matter where the notebook is opened (absolute paths).\n",
    "â€¢ Avoid NameError on __file__.\n",
    "â€¢ Keep hotâ€‘reload for iterative dev.\n",
    "â€¢ Forward arbitrary args to main() so we can test all scenarios.\n",
    "\n",
    "Use:\n",
    ">>> import salary_nba_data_pull.notebook_helper as nb\n",
    ">>> nb.quick_pull(2024, workers=12, debug=True)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import sys, importlib, inspect, os\n",
    "from pathlib import Path\n",
    "import requests_cache\n",
    "from typing import Iterable\n",
    "\n",
    "def _find_repo_root(start: Path | None = None) -> Path:\n",
    "    \"\"\"Find the repository root by looking for pyproject.toml or .git.\"\"\"\n",
    "    markers = {\"pyproject.toml\", \".git\"}\n",
    "    here = (start or Path.cwd()).resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "# Ensure project root & src are on sys.path (defensive)\n",
    "ROOT = _find_repo_root()\n",
    "SRC  = ROOT / \"src\"\n",
    "for p in (ROOT, SRC):\n",
    "    if p.is_dir() and str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "\n",
    "# Sanity print (can be silenced)\n",
    "if __name__ == \"__main__\" or \"JPY_PARENT_PID\" in os.environ:\n",
    "    print(f\"[notebook_helper] sys.path[0:3]={sys.path[:3]}\")\n",
    "\n",
    "# Import after path fix\n",
    "try:\n",
    "    from salary_nba_data_pull import main as nba_main\n",
    "    from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "    from salary_nba_data_pull.fetch_utils import clear_cache as _cc\n",
    "    print(\"âœ… salary_nba_data_pull imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import salary_nba_data_pull: {e}\")\n",
    "    print(f\"   ROOT={ROOT}\")\n",
    "    print(f\"   SRC={SRC}\")\n",
    "    print(f\"   sys.path[0:3]={sys.path[:3]}\")\n",
    "    raise\n",
    "    \n",
    "    \n",
    "def _reload():\n",
    "    \"\"\"Reload the main module so code edits are picked up.\"\"\"\n",
    "    importlib.reload(nba_main)\n",
    "\n",
    "def quick_pull(season: int, **kwargs):\n",
    "    _reload()\n",
    "    print(f\"[quick_pull] season={season}, kwargs={kwargs}\")\n",
    "    nba_main.main(start_year=season, end_year=season, **kwargs)\n",
    "\n",
    "def historical_pull(start_year: int, end_year: int, **kwargs):\n",
    "    _reload()\n",
    "    print(f\"[historical_pull] {start_year}-{end_year}, kwargs={kwargs}\")\n",
    "    nba_main.main(start_year=start_year, end_year=end_year, **kwargs)\n",
    "\n",
    "def check_existing_data(base: Path | str | None = None) -> list[str]:\n",
    "    base = Path(base) if base else DATA_PROCESSED_DIR\n",
    "    seasons = sorted(d.name.split(\"=\", 1)[-1] for d in base.glob(\"season=*\") if d.is_dir())\n",
    "    print(f\"[check_existing_data] found {len(seasons)} seasons in {base}\")\n",
    "    return seasons\n",
    "\n",
    "def load_parquet_data(season: str | None = None, *, base: Path | str | None = None):\n",
    "    import pandas as pd\n",
    "    base = Path(base) if base else DATA_PROCESSED_DIR\n",
    "    files = list(base.glob(f\"season={season}/part.parquet\")) if season else list(base.glob(\"season=*/part.parquet\"))\n",
    "    if not files:\n",
    "        print(\"[load_parquet_data] No parquet files found.\")\n",
    "        return pd.DataFrame()\n",
    "    print(f\"[load_parquet_data] loading {len(files)} files from {base}\")\n",
    "    return pd.concat((pd.read_parquet(f) for f in files), ignore_index=True)\n",
    "\n",
    "def clear_all_caches():\n",
    "    requests_cache.clear()\n",
    "    _cc()\n",
    "    print(\"âœ… caches cleared\")\n",
    "\n",
    "def print_args():\n",
    "    sig = inspect.signature(nba_main.main)\n",
    "    for name, param in sig.parameters.items():\n",
    "        print(f\"{name:<15} default={param.default!r}  kind={param.kind}\")\n",
    "\n",
    "def query_data(sql: str, db: str | None = None):\n",
    "    \"\"\"\n",
    "    Run arbitrary SQL against the DuckDB lake. Example:\n",
    "        query_data(\"SELECT COUNT(*) FROM parquet_scan('data/new_processed/season=*/part.parquet')\")\n",
    "    \"\"\"\n",
    "    import duckdb, pandas as pd\n",
    "    db = db or (DATA_PROCESSED_DIR.parent / \"nba_stats.duckdb\")\n",
    "    with duckdb.connect(str(db), read_only=True) as con:\n",
    "        return con.execute(sql).fetchdf()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_args()\n",
    "    # quick_pull(2023, workers=4, debug=True)\n",
    "\n",
    "\n",
    "\n",
    "    historical_pull(2012, 2024,        # multiâ€‘season, 2012, 2024,\n",
    "                    workers=6,\n",
    "                    min_avg_minutes=10,\n",
    "                    overwrite=True,\n",
    "                    debug=True)\n",
    "    check_existing_data()              # see which seasons are cached\n",
    "    df = load_parquet_data(\"2023-24\")  # inspect a single season\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Data Pipeline DAG Architecture\n",
    "\n",
    "## ðŸ—ï¸ Architecture Overview\n",
    "\n",
    "This document details the **simplified DAG architecture** that focuses on core data sources while removing salary scraping complexity.\n",
    "\n",
    "## ðŸ“Š DAG Comparison\n",
    "\n",
    "| Aspect | Monolithic DAG | Split DAGs | Benefit |\n",
    "|--------|----------------|------------|---------|\n",
    "| **Failure Isolation** | One failure blocks all | Isolated failures | âœ… Higher reliability |\n",
    "| **Scheduling** | Single cadence for all | Source-specific cadences | âœ… Optimized resource usage |\n",
    "| **Maintenance** | All-or-nothing updates | Independent iteration | âœ… Faster development |\n",
    "| **Monitoring** | Single SLA for everything | Granular SLAs | âœ… Better observability |\n",
    "| **Parsing Speed** | Large file slows DagBag | Smaller files | âœ… Faster Airflow startup |\n",
    "\n",
    "## ðŸ—“ï¸ Current DAG Set\n",
    "\n",
    "| # | DAG file | Purpose | Schedule | SLA | Retries |\n",
    "|---|----------|---------|----------|-----|---------|\n",
    "| 1 | `nba_advanced_ingest.py` | Advanced metrics (Basketballâ€‘Reference) | `@daily` | 1 h | 2 |\n",
    "| 2 | `injury_etl.py`          | Injury CSV processing | `@monthly` | 1 h | 1 |\n",
    "| 3 | `nba_data_loader.py`     | Load all sources into DuckDB | `@daily` | 3 h | 2 |\n",
    "\n",
    "> **Salary cap**: the yearly cap/parquet is committed by the build pipeline\n",
    "> and versionâ€‘controlled; no Airflow DAG is required.\n",
    "\n",
    "### Dependency graph\n",
    "\n",
    "```\n",
    "nba_advanced_ingest â”\n",
    "injury_etl â”œâ”€â”€â–º nba_data_loader\n",
    "```\n",
    "\n",
    "## ðŸ—“ï¸ DAG Scheduling Strategy\n",
    "\n",
    "### 1. `nba_advanced_ingest` - Daily\n",
    "**Rationale**: Advanced stats update daily\n",
    "- **Schedule**: `@daily`\n",
    "- **SLA**: 1 hour\n",
    "- **Retries**: 2 with 5-minute delays\n",
    "- **Sources**: Basketball-Reference\n",
    "\n",
    "### 2. `injury_etl` - Monthly\n",
    "**Rationale**: Injury data updates monthly\n",
    "- **Schedule**: `@monthly`\n",
    "- **SLA**: 1 hour\n",
    "- **Retries**: 1 with 5-minute delays\n",
    "- **Sources**: Local CSV files\n",
    "\n",
    "### 3. `nba_data_loader` - Daily\n",
    "**Rationale**: Loads all data into DuckDB daily\n",
    "- **Schedule**: `@daily`\n",
    "- **SLA**: 3 hours\n",
    "- **Dependencies**: Advanced metrics and injury ETL via ExternalTaskSensor\n",
    "\n",
    "## ðŸ”— Dependency Management\n",
    "\n",
    "### ExternalTaskSensor Configuration\n",
    "\n",
    "```python\n",
    "# Wait for advanced metrics\n",
    "wait_advanced = ExternalTaskSensor(\n",
    "    task_id=\"wait_advanced_ingest\",\n",
    "    external_dag_id=\"nba_advanced_ingest\",\n",
    "    external_task_id=\"scrape_advanced_metrics\",\n",
    "    timeout=3600,                     # 1 hour timeout\n",
    "    mode=\"reschedule\",\n",
    "    poke_interval=300,                # Check every 5 minutes\n",
    ")\n",
    "```\n",
    "\n",
    "### Timeout Strategy\n",
    "\n",
    "| DAG | Timeout | Rationale |\n",
    "|-----|---------|-----------|\n",
    "| Daily DAGs | 1 hour | Normal operation time |\n",
    "| Monthly DAGs | 2 hours | Allow for monthly task completion |\n",
    "\n",
    "## ðŸ“ˆ Performance Metrics\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "| Metric | Target | Measurement |\n",
    "|--------|--------|-------------|\n",
    "| **Ingest Success Rate** | >95% | Successful DAG runs / Total runs |\n",
    "| **Data Quality** | >99% | Valid rows / Total rows |\n",
    "| **SLA Compliance** | >90% | On-time completions / Total runs |\n",
    "\n",
    "### Monitoring Dashboard\n",
    "\n",
    "```sql\n",
    "-- DAG Performance Query\n",
    "SELECT \n",
    "    dag_id,\n",
    "    COUNT(*) as total_runs,\n",
    "    AVG(CASE WHEN state = 'success' THEN 1 ELSE 0 END) as success_rate,\n",
    "    AVG(duration) as avg_duration_minutes\n",
    "FROM airflow.task_instance \n",
    "WHERE start_date >= CURRENT_DATE - 30\n",
    "GROUP BY dag_id;\n",
    "```\n",
    "\n",
    "## ðŸ”„ Removed Components\n",
    "\n",
    "### Salary Scraping (Removed)\n",
    "- âŒ `nba_salary_ingest.py` - Player & team salary scraping\n",
    "- âŒ `salary_cap_snapshot.py` - Yearly salary cap scraping\n",
    "- âŒ ESPN/HoopsHype scrapers in `scrape_utils.py`\n",
    "\n",
    "### Salary Cap Handling (Updated)\n",
    "- âœ… **Build pipeline**: Yearly cap data committed to version control\n",
    "- âœ… **No DAG required**: Parquet files pre-baked by build process\n",
    "- âœ… **Loader compatibility**: Still loads cap data if available\n",
    "\n",
    "## ðŸ› ï¸ Implementation Details\n",
    "\n",
    "### Error Handling Strategy\n",
    "\n",
    "1. **Primary Source Failure**: Graceful degradation when data unavailable\n",
    "2. **Rate Limiting**: Exponential backoff with jitter\n",
    "3. **Data Validation**: Quality gates before loading to DuckDB\n",
    "4. **Alerting**: Email notifications for critical failures\n",
    "\n",
    "### Retry Configuration\n",
    "\n",
    "```python\n",
    "default_args = dict(\n",
    "    retries=2,                           # Standard retries\n",
    "    retry_delay=timedelta(minutes=5),    # Standard delays\n",
    "    sla=timedelta(hours=1),              # Standard SLA\n",
    ")\n",
    "```\n",
    "\n",
    "### Data Quality Gates\n",
    "\n",
    "```python\n",
    "# Quality checks before loading\n",
    "if len(df) == 0:\n",
    "    raise ValueError(f\"No data found for season {season}\")\n",
    "\n",
    "required_cols = [\"Season\", \"Player\", \"Team\"]\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "```\n",
    "\n",
    "## ðŸ“Š Cost-Benefit Analysis\n",
    "\n",
    "### Pros of Simplified Architecture\n",
    "\n",
    "| Benefit | Impact | Metric |\n",
    "|---------|--------|--------|\n",
    "| **Reliability** | High | 95%+ uptime per source |\n",
    "| **Maintainability** | High | Independent development cycles |\n",
    "| **Simplicity** | High | Fewer DAGs to manage |\n",
    "| **Monitoring** | High | Granular observability |\n",
    "\n",
    "### Cons of Simplified Architecture\n",
    "\n",
    "| Drawback | Mitigation | Status |\n",
    "|----------|------------|--------|\n",
    "| **Less data sources** | External salary data | âœ… Addressed |\n",
    "| **Reduced functionality** | Core metrics preserved | âœ… Minimized |\n",
    "\n",
    "## ðŸš€ Deployment Checklist\n",
    "\n",
    "### Pre-Deployment\n",
    "- [x] All DAG files created and tested\n",
    "- [x] Salary scraping removed and stubbed\n",
    "- [x] ExternalTaskSensor dependencies configured\n",
    "- [x] Data quality gates implemented\n",
    "- [x] Monitoring and alerting configured\n",
    "\n",
    "### Deployment\n",
    "- [x] Deploy new DAGs to Airflow\n",
    "- [x] Disable old monolithic DAG\n",
    "- [x] Verify all DAGs are running\n",
    "- [x] Check data flow end-to-end\n",
    "- [x] Monitor for 24 hours\n",
    "\n",
    "### Post-Deployment\n",
    "- [x] Compare performance metrics\n",
    "- [x] Validate data quality\n",
    "- [x] Update documentation\n",
    "- [x] Train team on new architecture\n",
    "\n",
    "## ðŸ“š References\n",
    "\n",
    "- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n",
    "- [ExternalTaskSensor Guide](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html)\n",
    "- [DAG Design Patterns](https://medium.com/@gharikrishnade/airflow-dag-design-patterns-keeping-it-clean-and-modular-ae07bf9b6f11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_api_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_api_ingest.py\n",
    "# dags/nba_api_ingest.py\n",
    "\"\"\"\n",
    "Pulls roster + boxâ€‘score data from nba_api once per hour and writes Parquet\n",
    "partitions under data/new_processed/season=<YYYY-YY>/part.parquet.\n",
    "\n",
    "Why hourly?\n",
    "â€¢ The NBA Stats endpoints update within minutes after a game ends.\n",
    "â€¢ Hourly keeps your lake nearâ€‘realâ€‘time without hammering the API.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys, pathlib\n",
    "\n",
    "# Allow `salary_nba_data_pull` imports\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.main import main as pull_main\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,      # explicit\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_api_ingest\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@hourly\",            # unified scheduling API (Airflow â‰¥ 2.4)\n",
    "    catchup=False,\n",
    "    default_args=default_args,\n",
    "    max_active_runs=1,             # avoid overlapping pulls\n",
    "    tags=[\"nba\", \"api\", \"ingest\"],\n",
    "    params={\"season\": \"2024-25\"},  # visible & overridable in the UI\n",
    ") as dag:\n",
    "\n",
    "    def pull_season(**context):\n",
    "        season = context[\"params\"][\"season\"]\n",
    "        start_year = int(season[:4])\n",
    "        pull_main(\n",
    "            start_year=start_year,\n",
    "            end_year=start_year,\n",
    "            small_debug=True,\n",
    "            workers=8,\n",
    "            overwrite=False,\n",
    "        )\n",
    "\n",
    "    PythonOperator(\n",
    "        task_id=\"scrape_season_data\",\n",
    "        python_callable=pull_season,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_advanced_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_advanced_ingest.py\n",
    "# dags/nba_advanced_ingest.py\n",
    "\"\"\"\n",
    "Daily scrape of Basketballâ€‘Reference seasonâ€‘level advanced metrics.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.scrape_utils import _season_advanced_df\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_advanced_ingest\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    tags=[\"nba\", \"advanced\", \"ingest\"],\n",
    "    params={\"season\": \"2024-25\"},\n",
    ") as dag:\n",
    "\n",
    "    def scrape_adv(**ctx):\n",
    "        season = ctx[\"params\"][\"season\"]\n",
    "        df = _season_advanced_df(season)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"No advanced data for {season}\")\n",
    "        out_dir = Path(\"/workspace/data/new_processed/advanced_metrics\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_parquet(out_dir / f\"advanced_{season}.parquet\", index=False)\n",
    "\n",
    "    PythonOperator(\n",
    "        task_id=\"scrape_advanced_metrics\",\n",
    "        python_callable=scrape_adv,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_data_loader.py\n",
    "# dags/nba_data_loader.py\n",
    "\"\"\"\n",
    "Fanâ€‘in loader: waits for api_ingest + advanced_ingest + injury_etl,\n",
    "then materialises season tables and a joined view in DuckDB.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import sys, os, duckdb, pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.data_utils import validate_data\n",
    "\n",
    "DATA_ROOT = Path(\"/workspace/data\")\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=3),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_data_loader\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    tags=[\"nba\", \"loader\", \"duckdb\"],\n",
    "    params={\"season\": \"2024-25\"},\n",
    ") as dag:\n",
    "\n",
    "    # â”€â”€â”€ sensors (one per upstream DAG) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    sensor_args = dict(\n",
    "        poke_interval=300,\n",
    "        mode=\"reschedule\",   # avoids tying up a worker slot\n",
    "    )\n",
    "    wait_api = ExternalTaskSensor(\n",
    "        task_id=\"wait_api_ingest\",\n",
    "        external_dag_id=\"nba_api_ingest\",\n",
    "        external_task_id=\"scrape_season_data\",\n",
    "        timeout=3600,\n",
    "        **sensor_args,\n",
    "    )\n",
    "    wait_adv = ExternalTaskSensor(\n",
    "        task_id=\"wait_advanced_ingest\",\n",
    "        external_dag_id=\"nba_advanced_ingest\",\n",
    "        external_task_id=\"scrape_advanced_metrics\",\n",
    "        timeout=3600,\n",
    "        **sensor_args,\n",
    "    )\n",
    "    wait_injury = ExternalTaskSensor(\n",
    "        task_id=\"wait_injury_etl\",\n",
    "        external_dag_id=\"injury_etl\",\n",
    "        external_task_id=\"process_injury_data\",\n",
    "        timeout=7200,\n",
    "        poke_interval=600,\n",
    "        mode=\"reschedule\",\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€ loader task â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def load_to_duckdb(**ctx):\n",
    "        season = ctx[\"params\"][\"season\"]\n",
    "        db = DATA_ROOT / \"nba_stats.duckdb\"\n",
    "        con = duckdb.connect(db)\n",
    "        sources = {\n",
    "            f\"player_{season}\": DATA_ROOT / f\"new_processed/season={season}/part.parquet\",\n",
    "            f\"advanced_{season}\": DATA_ROOT / f\"new_processed/advanced_metrics/advanced_{season}.parquet\",\n",
    "            \"injury_master\": DATA_ROOT / \"new_processed/injury_reports/injury_master.parquet\",\n",
    "        }\n",
    "\n",
    "        for alias, path in sources.items():\n",
    "            if path.exists():\n",
    "                if alias.startswith(\"player\"):\n",
    "                    df = pd.read_parquet(path)\n",
    "                    validate_data(df, name=alias, save_reports=True)\n",
    "                con.execute(\n",
    "                    f\"CREATE OR REPLACE TABLE {alias.replace('-', '_')} AS \"\n",
    "                    f\"SELECT * FROM read_parquet('{path}')\"\n",
    "                )\n",
    "\n",
    "        # materialised view â€“ wildcard parquet scan is fine too\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE VIEW v_player_full_{season.replace('-', '_')} AS\n",
    "            SELECT *\n",
    "            FROM player_{season.replace('-', '_')} p\n",
    "            LEFT JOIN advanced_{season.replace('-', '_')} a USING(player, season)\n",
    "            LEFT JOIN injury_master i USING(player, season)\n",
    "        \"\"\")\n",
    "        con.close()\n",
    "\n",
    "    loader = PythonOperator(\n",
    "        task_id=\"validate_and_load\",\n",
    "        python_callable=load_to_duckdb,\n",
    "    )\n",
    "\n",
    "    [wait_api, wait_adv, wait_injury] >> loader "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
