{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/18 13:54:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/18 13:54:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2024-08-18 13:54:12,600 - INFO - Data loaded. Schema: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Player: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- TeamID: double (nullable = true)\n",
      " |-- Years of Service: double (nullable = true)\n",
      " |-- GP: double (nullable = true)\n",
      " |-- GS: double (nullable = true)\n",
      " |-- MP: double (nullable = true)\n",
      " |-- FG: double (nullable = true)\n",
      " |-- FGA: double (nullable = true)\n",
      " |-- FG%: double (nullable = true)\n",
      " |-- 3P: double (nullable = true)\n",
      " |-- 3PA: double (nullable = true)\n",
      " |-- 3P%: double (nullable = true)\n",
      " |-- 2P: double (nullable = true)\n",
      " |-- 2PA: double (nullable = true)\n",
      " |-- 2P%: double (nullable = true)\n",
      " |-- eFG%: double (nullable = true)\n",
      " |-- FT: double (nullable = true)\n",
      " |-- FTA: double (nullable = true)\n",
      " |-- FT%: double (nullable = true)\n",
      " |-- ORB: double (nullable = true)\n",
      " |-- DRB: double (nullable = true)\n",
      " |-- TRB: double (nullable = true)\n",
      " |-- AST: double (nullable = true)\n",
      " |-- STL: double (nullable = true)\n",
      " |-- BLK: double (nullable = true)\n",
      " |-- TOV: double (nullable = true)\n",
      " |-- PF: double (nullable = true)\n",
      " |-- PTS: double (nullable = true)\n",
      " |-- PER: double (nullable = true)\n",
      " |-- TS%: double (nullable = true)\n",
      " |-- ORB%: double (nullable = true)\n",
      " |-- DRB%: double (nullable = true)\n",
      " |-- TRB%: double (nullable = true)\n",
      " |-- AST%: double (nullable = true)\n",
      " |-- STL%: double (nullable = true)\n",
      " |-- BLK%: double (nullable = true)\n",
      " |-- TOV%: double (nullable = true)\n",
      " |-- USG%: double (nullable = true)\n",
      " |-- OWS: double (nullable = true)\n",
      " |-- DWS: double (nullable = true)\n",
      " |-- WS: double (nullable = true)\n",
      " |-- WS/48: double (nullable = true)\n",
      " |-- OBPM: double (nullable = true)\n",
      " |-- DBPM: double (nullable = true)\n",
      " |-- BPM: double (nullable = true)\n",
      " |-- VORP: double (nullable = true)\n",
      " |-- Wins: double (nullable = true)\n",
      " |-- Losses: double (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- Injured: boolean (nullable = true)\n",
      " |-- Injury_Periods: string (nullable = true)\n",
      " |-- Total_Days_Injured: double (nullable = true)\n",
      " |-- Injury_Risk: string (nullable = true)\n",
      " |-- Salary Cap: double (nullable = true)\n",
      " |-- Luxury Tax: double (nullable = true)\n",
      " |-- 1st Apron: double (nullable = true)\n",
      " |-- BAE: double (nullable = true)\n",
      " |-- Standard /Non-Taxpayer: double (nullable = true)\n",
      " |-- Taxpayer: double (nullable = true)\n",
      " |-- Team Room /Under Cap: double (nullable = true)\n",
      " |-- Salary_Cap_Inflated: double (nullable = true)\n",
      " |-- 2nd Apron: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 13:54:13,166 - INFO - Seasons in data: [Row(Season=2018), Row(Season=2023), Row(Season=2022), Row(Season=2019), Row(Season=2020), Row(Season=2021)]\n",
      "2024-08-18 13:54:13,569 - INFO - Data filtered. Prior seasons shape: (2294, 66), Target season shape: (476, 66)\n",
      "24/08/18 13:54:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2024-08-18 13:54:14,421 - INFO - Data cleaned. Remaining shape: (2274, 62)\n",
      "2024-08-18 13:54:14,594 - INFO - New features added.\n",
      "2024-08-18 13:54:14,595 - INFO - Data preprocessing completed with PySpark. Ready for further processing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Season           Player        Position   Age Team        TeamID  \\\n",
      "0    2018     Aaron Gordon         Forward  23.0  ORL  1.610613e+09   \n",
      "1    2018    Aaron Holiday           Guard  22.0  IND  1.610613e+09   \n",
      "2    2018      Abdel Nader         Forward  25.0  OKC  1.610613e+09   \n",
      "3    2018       Al Horford  Center-Forward  33.0  BOS  1.610613e+09   \n",
      "4    2018  Al-Farouq Aminu         Forward  28.0  POR  1.610613e+09   \n",
      "\n",
      "   Years of Service   3P%   2P%  eFG%  ...  Efficiency  ValueOverReplacement  \\\n",
      "0               4.0  0.35  0.50  0.51  ...    1.519836          7.873684e-08   \n",
      "1               0.0  0.34  0.46  0.48  ...    1.365439          5.223348e-08   \n",
      "2               2.0  0.32  0.51  0.50  ...    1.471223         -1.451123e-07   \n",
      "3              11.0  0.36  0.60  0.59  ...    1.966341          1.244438e-07   \n",
      "4               8.0  0.34  0.51  0.51  ...    1.877235          2.156069e-07   \n",
      "\n",
      "   ExperienceSquared  Days_Injured_Percentage      WSPG     DWSPG     OWSPG  \\\n",
      "0               16.0                 0.064103  0.065385  0.042308  0.023077   \n",
      "1                0.0                 0.000000  0.018000  0.016000  0.002000   \n",
      "2                4.0                 0.081967  0.014754  0.014754  0.000000   \n",
      "3              121.0                 0.411765  0.108824  0.041176  0.066176   \n",
      "4               64.0                 0.000000  0.071605  0.034568  0.037037   \n",
      "\n",
      "       PFPG      ORPG      DRPG  \n",
      "0  2.205128  1.653846  5.705128  \n",
      "1  1.420000  0.100000  1.240000  \n",
      "2  1.114754  0.229508  1.672131  \n",
      "3  1.852941  1.764706  4.970588  \n",
      "4  1.765432  1.382716  6.148148  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ../src/salary_predict/pyspark_data_loader_preprocessor.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create a Spark session.\"\"\"\n",
    "    spark = SparkSession.builder.appName(\"SalaryPrediction\").getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def load_data(file_path, spark):\n",
    "    \"\"\"Load data using PySpark.\"\"\"\n",
    "    try:\n",
    "        data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        logger.info(f\"Data loaded. Schema: {data.printSchema()}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def format_season(data):\n",
    "    \"\"\"Format the Season column.\"\"\"\n",
    "    try:\n",
    "        # Assuming the 'Season' column contains strings like '2018-19', extract the first 4 characters\n",
    "        data = data.withColumn('Season', col('Season').substr(1, 4).cast('int'))\n",
    "        logger.info(f\"Seasons in data: {data.select('Season').distinct().collect()}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to format season data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"Clean the dataset.\"\"\"\n",
    "    try:\n",
    "        columns_to_drop = ['Injury_Periods', '2nd Apron', 'Wins', 'Losses']\n",
    "        data_clean = data.drop(*columns_to_drop)\n",
    "        \n",
    "        percentage_cols = ['3P%', '2P%', 'FT%', 'TS%']\n",
    "        for col_name in percentage_cols:\n",
    "            mean_value = data_clean.agg({col_name: 'mean'}).collect()[0][0]\n",
    "            data_clean = data_clean.fillna(mean_value, subset=[col_name])\n",
    "        \n",
    "        data_clean = data_clean.na.drop()\n",
    "        logger.info(f\"Data cleaned. Remaining shape: ({data_clean.count()}, {len(data_clean.columns)})\")\n",
    "        return data_clean\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to clean data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def engineer_features(data):\n",
    "    \"\"\"Engineer new features for the dataset.\"\"\"\n",
    "    try:\n",
    "        data = data.withColumn('PPG', col('PTS') / col('GP'))\n",
    "        data = data.withColumn('APG', col('AST') / col('GP'))\n",
    "        data = data.withColumn('RPG', col('TRB') / col('GP'))\n",
    "        data = data.withColumn('SPG', col('STL') / col('GP'))\n",
    "        data = data.withColumn('TOPG', col('TOV') / col('GP'))\n",
    "        data = data.withColumn('Availability', col('GP') / 82)\n",
    "        data = data.withColumn('SalaryPct', col('Salary') / col('Salary_Cap_Inflated'))\n",
    "        data = data.withColumn('Efficiency', \n",
    "                               (col('PTS') + col('TRB') + col('AST') + col('STL') + col('BLK')) / \n",
    "                               (col('FGA') + col('FTA') + col('TOV') + 1))\n",
    "        data = data.withColumn('ValueOverReplacement', col('VORP') / (col('Salary') + 1))\n",
    "        data = data.withColumn('ExperienceSquared', col('Years of Service') ** 2)\n",
    "        data = data.withColumn('Days_Injured_Percentage', col('Total_Days_Injured') / col('GP'))\n",
    "        data = data.withColumn('WSPG', col('WS') / col('GP'))\n",
    "        data = data.withColumn('DWSPG', col('DWS') / col('GP'))\n",
    "        data = data.withColumn('OWSPG', col('OWS') / col('GP'))\n",
    "        data = data.withColumn('PFPG', col('PF') / col('GP'))\n",
    "        data = data.withColumn('ORPG', col('ORB') / col('GP'))\n",
    "        data = data.withColumn('DRPG', col('DRB') / col('GP'))\n",
    "        \n",
    "        columns_to_drop = ['GP', '2PA', 'OBPM', 'BPM', 'DBPM', '2P', 'GS', 'PTS', 'AST', 'TRB', 'STL', 'BLK',\n",
    "                           'TOV', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB', 'DRB', \n",
    "                           'TRB', 'TS%', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', \n",
    "                           'Luxury Tax', '1st Apron', 'BAE', 'Standard /Non-Taxpayer', 'Taxpayer', \n",
    "                           'Team Room /Under Cap', 'WS', 'DWS', 'WS/48', 'PF', 'OWS', 'Injured']\n",
    "        \n",
    "        data = data.drop(*columns_to_drop)\n",
    "        logger.info(\"New features added.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to engineer features: {e}\")\n",
    "        raise\n",
    "\n",
    "def encode_injury_risk(data):\n",
    "    \"\"\"Encode Injury_Risk into numerical categories.\"\"\"\n",
    "    risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    data = data.replace({'Injury_Risk': risk_mapping})\n",
    "    return data, risk_mapping\n",
    "\n",
    "def filter_seasons(data, predict_season):\n",
    "    \"\"\"Filter the dataset into prior seasons and the target season for prediction.\"\"\"\n",
    "    prior_seasons_data = data.filter(col('Season') < predict_season)\n",
    "    target_season_data = data.filter(col('Season') == predict_season)\n",
    "    logger.info(f\"Data filtered. Prior seasons shape: ({prior_seasons_data.count()}, {len(prior_seasons_data.columns)}), Target season shape: ({target_season_data.count()}, {len(target_season_data.columns)})\")\n",
    "    return target_season_data, prior_seasons_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        spark = create_spark_session()\n",
    "        file_path = '../data/processed/nba_player_data_final_inflated.csv'\n",
    "        predict_season = 2023\n",
    "\n",
    "        # Load, preprocess, and engineer features using PySpark\n",
    "        data = load_data(file_path, spark)\n",
    "        data = format_season(data)\n",
    "        target_season_data, prior_seasons_data = filter_seasons(data, predict_season)\n",
    "        prior_seasons_data = clean_data(prior_seasons_data)\n",
    "        prior_seasons_data = engineer_features(prior_seasons_data)\n",
    "\n",
    "        logger.info(\"Data preprocessing completed with PySpark. Ready for further processing.\")\n",
    "        \n",
    "        # Optionally, you can convert PySpark DataFrame to pandas DataFrame if needed\n",
    "        pandas_df = prior_seasons_data.toPandas()\n",
    "        print(pandas_df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Critical error in PySpark data processing pipeline: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 13:54:15,985 - INFO - Data loaded. Schema: None\n",
      "2024-08-18 13:54:16,106 - INFO - Seasons in data: [Row(Season=2018), Row(Season=2023), Row(Season=2022), Row(Season=2019), Row(Season=2020), Row(Season=2021)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Season: string (nullable = true)\n",
      " |-- Player: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- TeamID: double (nullable = true)\n",
      " |-- Years of Service: double (nullable = true)\n",
      " |-- GP: double (nullable = true)\n",
      " |-- GS: double (nullable = true)\n",
      " |-- MP: double (nullable = true)\n",
      " |-- FG: double (nullable = true)\n",
      " |-- FGA: double (nullable = true)\n",
      " |-- FG%: double (nullable = true)\n",
      " |-- 3P: double (nullable = true)\n",
      " |-- 3PA: double (nullable = true)\n",
      " |-- 3P%: double (nullable = true)\n",
      " |-- 2P: double (nullable = true)\n",
      " |-- 2PA: double (nullable = true)\n",
      " |-- 2P%: double (nullable = true)\n",
      " |-- eFG%: double (nullable = true)\n",
      " |-- FT: double (nullable = true)\n",
      " |-- FTA: double (nullable = true)\n",
      " |-- FT%: double (nullable = true)\n",
      " |-- ORB: double (nullable = true)\n",
      " |-- DRB: double (nullable = true)\n",
      " |-- TRB: double (nullable = true)\n",
      " |-- AST: double (nullable = true)\n",
      " |-- STL: double (nullable = true)\n",
      " |-- BLK: double (nullable = true)\n",
      " |-- TOV: double (nullable = true)\n",
      " |-- PF: double (nullable = true)\n",
      " |-- PTS: double (nullable = true)\n",
      " |-- PER: double (nullable = true)\n",
      " |-- TS%: double (nullable = true)\n",
      " |-- ORB%: double (nullable = true)\n",
      " |-- DRB%: double (nullable = true)\n",
      " |-- TRB%: double (nullable = true)\n",
      " |-- AST%: double (nullable = true)\n",
      " |-- STL%: double (nullable = true)\n",
      " |-- BLK%: double (nullable = true)\n",
      " |-- TOV%: double (nullable = true)\n",
      " |-- USG%: double (nullable = true)\n",
      " |-- OWS: double (nullable = true)\n",
      " |-- DWS: double (nullable = true)\n",
      " |-- WS: double (nullable = true)\n",
      " |-- WS/48: double (nullable = true)\n",
      " |-- OBPM: double (nullable = true)\n",
      " |-- DBPM: double (nullable = true)\n",
      " |-- BPM: double (nullable = true)\n",
      " |-- VORP: double (nullable = true)\n",
      " |-- Wins: double (nullable = true)\n",
      " |-- Losses: double (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- Injured: boolean (nullable = true)\n",
      " |-- Injury_Periods: string (nullable = true)\n",
      " |-- Total_Days_Injured: double (nullable = true)\n",
      " |-- Injury_Risk: string (nullable = true)\n",
      " |-- Salary Cap: double (nullable = true)\n",
      " |-- Luxury Tax: double (nullable = true)\n",
      " |-- 1st Apron: double (nullable = true)\n",
      " |-- BAE: double (nullable = true)\n",
      " |-- Standard /Non-Taxpayer: double (nullable = true)\n",
      " |-- Taxpayer: double (nullable = true)\n",
      " |-- Team Room /Under Cap: double (nullable = true)\n",
      " |-- Salary_Cap_Inflated: double (nullable = true)\n",
      " |-- 2nd Apron: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 13:54:16,268 - INFO - Data filtered. Prior seasons shape: (2294, 66), Target season shape: (476, 66)\n",
      "2024-08-18 13:54:16,715 - INFO - Data cleaned. Remaining shape: (2274, 62)\n",
      "2024-08-18 13:54:16,844 - INFO - New features added.\n",
      "2024-08-18 13:54:16,845 - INFO - Columns after feature engineering: ['Season', 'Player', 'Position', 'Age', 'Team', 'TeamID', 'Years of Service', '3P%', '2P%', 'eFG%', 'FT%', 'PER', 'VORP', 'Salary', 'Total_Days_Injured', 'Injury_Risk', 'Salary Cap', 'Salary_Cap_Inflated', 'PPG', 'APG', 'RPG', 'SPG', 'TOPG', 'Availability', 'SalaryPct', 'Efficiency', 'ValueOverReplacement', 'ExperienceSquared', 'Days_Injured_Percentage', 'WSPG', 'DWSPG', 'OWSPG', 'PFPG', 'ORPG', 'DRPG']\n",
      "24/08/18 13:54:19 WARN DAGScheduler: Broadcasting large task binary with size 1058.3 KiB\n",
      "24/08/18 13:54:20 WARN DAGScheduler: Broadcasting large task binary with size 1875.7 KiB\n",
      "24/08/18 13:54:20 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/08/18 13:54:21 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "24/08/18 13:54:22 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n",
      "24/08/18 13:54:22 WARN DAGScheduler: Broadcasting large task binary with size 1112.4 KiB\n",
      "24/08/18 13:54:23 WARN DAGScheduler: Broadcasting large task binary with size 1058.3 KiB\n",
      "24/08/18 13:54:23 WARN DAGScheduler: Broadcasting large task binary with size 1875.7 KiB\n",
      "24/08/18 13:54:24 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/08/18 13:54:24 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "24/08/18 13:54:25 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/08/18 13:54:25 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n",
      "24/08/18 13:54:25 WARN DAGScheduler: Broadcasting large task binary with size 1112.4 KiB\n",
      "24/08/18 13:54:26 WARN DAGScheduler: Broadcasting large task binary with size 10.3 MiB\n",
      "24/08/18 13:54:26 WARN DAGScheduler: Broadcasting large task binary with size 1222.5 KiB\n",
      "24/08/18 13:54:27 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n",
      "24/08/18 13:54:27 WARN DAGScheduler: Broadcasting large task binary with size 1220.0 KiB\n",
      "24/08/18 13:54:28 WARN DAGScheduler: Broadcasting large task binary with size 16.4 MiB\n",
      "24/08/18 13:54:29 WARN DAGScheduler: Broadcasting large task binary with size 1128.4 KiB\n",
      "24/08/18 13:54:29 WARN DAGScheduler: Broadcasting large task binary with size 19.2 MiB\n",
      "24/08/18 13:54:31 WARN DAGScheduler: Broadcasting large task binary with size 21.6 MiB\n",
      "24/08/18 13:54:33 WARN DAGScheduler: Broadcasting large task binary with size 1059.1 KiB\n",
      "24/08/18 13:54:34 WARN DAGScheduler: Broadcasting large task binary with size 1059.1 KiB\n",
      "24/08/18 13:54:34 WARN DAGScheduler: Broadcasting large task binary with size 1972.1 KiB\n",
      "24/08/18 13:54:35 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:54:35 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "24/08/18 13:54:36 WARN DAGScheduler: Broadcasting large task binary with size 1395.8 KiB\n",
      "24/08/18 13:54:37 WARN DAGScheduler: Broadcasting large task binary with size 9.9 MiB\n",
      "24/08/18 13:54:37 WARN DAGScheduler: Broadcasting large task binary with size 1859.6 KiB\n",
      "24/08/18 13:54:38 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "24/08/18 13:54:39 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/08/18 13:54:41 WARN DAGScheduler: Broadcasting large task binary with size 1059.1 KiB\n",
      "24/08/18 13:54:41 WARN DAGScheduler: Broadcasting large task binary with size 1972.1 KiB\n",
      "24/08/18 13:54:42 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:54:42 WARN DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n",
      "24/08/18 13:54:43 WARN DAGScheduler: Broadcasting large task binary with size 1395.8 KiB\n",
      "24/08/18 13:54:44 WARN DAGScheduler: Broadcasting large task binary with size 9.9 MiB\n",
      "24/08/18 13:54:44 WARN DAGScheduler: Broadcasting large task binary with size 1859.6 KiB\n",
      "24/08/18 13:54:45 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "24/08/18 13:54:46 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/08/18 13:54:47 WARN DAGScheduler: Broadcasting large task binary with size 20.6 MiB\n",
      "24/08/18 13:54:48 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/08/18 13:54:50 WARN DAGScheduler: Broadcasting large task binary with size 26.7 MiB\n",
      "24/08/18 13:54:50 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/08/18 13:54:52 WARN DAGScheduler: Broadcasting large task binary with size 32.8 MiB\n",
      "24/08/18 13:54:53 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/08/18 13:54:55 WARN DAGScheduler: Broadcasting large task binary with size 38.4 MiB\n",
      "24/08/18 13:54:56 WARN DAGScheduler: Broadcasting large task binary with size 1983.9 KiB\n",
      "24/08/18 13:54:58 WARN DAGScheduler: Broadcasting large task binary with size 43.2 MiB\n",
      "24/08/18 13:54:59 WARN DAGScheduler: Broadcasting large task binary with size 1627.4 KiB\n",
      "24/08/18 13:55:02 WARN DAGScheduler: Broadcasting large task binary with size 1516.4 KiB\n",
      "24/08/18 13:55:03 WARN DAGScheduler: Broadcasting large task binary with size 1516.4 KiB\n",
      "24/08/18 13:55:04 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/08/18 13:55:04 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "24/08/18 13:55:05 WARN DAGScheduler: Broadcasting large task binary with size 1401.7 KiB\n",
      "24/08/18 13:55:06 WARN DAGScheduler: Broadcasting large task binary with size 9.1 MiB\n",
      "24/08/18 13:55:06 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/08/18 13:55:07 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "24/08/18 13:55:08 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "24/08/18 13:55:10 WARN DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n",
      "24/08/18 13:55:11 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/08/18 13:55:13 WARN DAGScheduler: Broadcasting large task binary with size 1516.4 KiB\n",
      "24/08/18 13:55:14 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/08/18 13:55:15 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "24/08/18 13:55:15 WARN DAGScheduler: Broadcasting large task binary with size 1401.7 KiB\n",
      "24/08/18 13:55:16 WARN DAGScheduler: Broadcasting large task binary with size 9.1 MiB\n",
      "24/08/18 13:55:17 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/08/18 13:55:18 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "24/08/18 13:55:19 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "24/08/18 13:55:20 WARN DAGScheduler: Broadcasting large task binary with size 22.1 MiB\n",
      "24/08/18 13:55:21 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/08/18 13:55:23 WARN DAGScheduler: Broadcasting large task binary with size 30.6 MiB\n",
      "24/08/18 13:55:25 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:55:27 WARN DAGScheduler: Broadcasting large task binary with size 39.8 MiB\n",
      "24/08/18 13:55:28 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:55:31 WARN DAGScheduler: Broadcasting large task binary with size 48.9 MiB\n",
      "24/08/18 13:55:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/08/18 13:55:35 WARN DAGScheduler: Broadcasting large task binary with size 57.2 MiB\n",
      "24/08/18 13:55:36 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/08/18 13:55:40 WARN DAGScheduler: Broadcasting large task binary with size 64.4 MiB\n",
      "24/08/18 13:55:41 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/08/18 13:55:45 WARN DAGScheduler: Broadcasting large task binary with size 1052.3 KiB\n",
      "24/08/18 13:55:45 WARN DAGScheduler: Broadcasting large task binary with size 1849.9 KiB\n",
      "24/08/18 13:55:46 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/08/18 13:55:46 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "24/08/18 13:55:47 WARN DAGScheduler: Broadcasting large task binary with size 7.3 MiB\n",
      "24/08/18 13:55:47 WARN DAGScheduler: Broadcasting large task binary with size 1074.5 KiB\n",
      "24/08/18 13:55:48 WARN DAGScheduler: Broadcasting large task binary with size 1052.3 KiB\n",
      "24/08/18 13:55:48 WARN DAGScheduler: Broadcasting large task binary with size 1849.9 KiB\n",
      "24/08/18 13:55:49 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/08/18 13:55:49 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "24/08/18 13:55:50 WARN DAGScheduler: Broadcasting large task binary with size 7.3 MiB\n",
      "24/08/18 13:55:50 WARN DAGScheduler: Broadcasting large task binary with size 1074.5 KiB\n",
      "24/08/18 13:55:51 WARN DAGScheduler: Broadcasting large task binary with size 10.0 MiB\n",
      "24/08/18 13:55:51 WARN DAGScheduler: Broadcasting large task binary with size 1174.3 KiB\n",
      "24/08/18 13:55:52 WARN DAGScheduler: Broadcasting large task binary with size 13.0 MiB\n",
      "24/08/18 13:55:52 WARN DAGScheduler: Broadcasting large task binary with size 1184.4 KiB\n",
      "24/08/18 13:55:53 WARN DAGScheduler: Broadcasting large task binary with size 15.9 MiB\n",
      "24/08/18 13:55:53 WARN DAGScheduler: Broadcasting large task binary with size 1125.5 KiB\n",
      "24/08/18 13:55:54 WARN DAGScheduler: Broadcasting large task binary with size 18.7 MiB\n",
      "24/08/18 13:55:56 WARN DAGScheduler: Broadcasting large task binary with size 21.1 MiB\n",
      "24/08/18 13:55:57 WARN DAGScheduler: Broadcasting large task binary with size 1057.1 KiB\n",
      "24/08/18 13:55:58 WARN DAGScheduler: Broadcasting large task binary with size 1057.1 KiB\n",
      "24/08/18 13:55:58 WARN DAGScheduler: Broadcasting large task binary with size 1958.0 KiB\n",
      "24/08/18 13:55:59 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:56:00 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "24/08/18 13:56:00 WARN DAGScheduler: Broadcasting large task binary with size 1366.8 KiB\n",
      "24/08/18 13:56:01 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB\n",
      "24/08/18 13:56:01 WARN DAGScheduler: Broadcasting large task binary with size 1803.9 KiB\n",
      "24/08/18 13:56:02 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n",
      "24/08/18 13:56:03 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/08/18 13:56:05 WARN DAGScheduler: Broadcasting large task binary with size 1057.1 KiB\n",
      "24/08/18 13:56:05 WARN DAGScheduler: Broadcasting large task binary with size 1958.0 KiB\n",
      "24/08/18 13:56:05 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:56:06 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "24/08/18 13:56:07 WARN DAGScheduler: Broadcasting large task binary with size 1366.8 KiB\n",
      "24/08/18 13:56:07 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB\n",
      "24/08/18 13:56:08 WARN DAGScheduler: Broadcasting large task binary with size 1803.9 KiB\n",
      "24/08/18 13:56:09 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n",
      "24/08/18 13:56:09 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/08/18 13:56:11 WARN DAGScheduler: Broadcasting large task binary with size 20.0 MiB\n",
      "24/08/18 13:56:11 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/08/18 13:56:13 WARN DAGScheduler: Broadcasting large task binary with size 26.0 MiB\n",
      "24/08/18 13:56:13 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/08/18 13:56:15 WARN DAGScheduler: Broadcasting large task binary with size 31.9 MiB\n",
      "24/08/18 13:56:16 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/08/18 13:56:18 WARN DAGScheduler: Broadcasting large task binary with size 37.3 MiB\n",
      "24/08/18 13:56:19 WARN DAGScheduler: Broadcasting large task binary with size 1950.3 KiB\n",
      "24/08/18 13:56:21 WARN DAGScheduler: Broadcasting large task binary with size 42.1 MiB\n",
      "24/08/18 13:56:22 WARN DAGScheduler: Broadcasting large task binary with size 1606.9 KiB\n",
      "24/08/18 13:56:24 WARN DAGScheduler: Broadcasting large task binary with size 1515.5 KiB\n",
      "24/08/18 13:56:26 WARN DAGScheduler: Broadcasting large task binary with size 1515.5 KiB\n",
      "24/08/18 13:56:26 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/08/18 13:56:27 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "24/08/18 13:56:27 WARN DAGScheduler: Broadcasting large task binary with size 1379.3 KiB\n",
      "24/08/18 13:56:28 WARN DAGScheduler: Broadcasting large task binary with size 9.1 MiB\n",
      "24/08/18 13:56:29 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/08/18 13:56:30 WARN DAGScheduler: Broadcasting large task binary with size 14.6 MiB\n",
      "24/08/18 13:56:30 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "24/08/18 13:56:32 WARN DAGScheduler: Broadcasting large task binary with size 21.6 MiB\n",
      "24/08/18 13:56:33 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/08/18 13:56:35 WARN DAGScheduler: Broadcasting large task binary with size 1515.5 KiB\n",
      "24/08/18 13:56:36 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/08/18 13:56:36 WARN DAGScheduler: Broadcasting large task binary with size 5.2 MiB\n",
      "24/08/18 13:56:37 WARN DAGScheduler: Broadcasting large task binary with size 1379.3 KiB\n",
      "24/08/18 13:56:38 WARN DAGScheduler: Broadcasting large task binary with size 9.1 MiB\n",
      "24/08/18 13:56:38 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/08/18 13:56:39 WARN DAGScheduler: Broadcasting large task binary with size 14.6 MiB\n",
      "24/08/18 13:56:40 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "24/08/18 13:56:42 WARN DAGScheduler: Broadcasting large task binary with size 21.6 MiB\n",
      "24/08/18 13:56:43 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/08/18 13:56:45 WARN DAGScheduler: Broadcasting large task binary with size 29.9 MiB\n",
      "24/08/18 13:56:46 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/08/18 13:56:48 WARN DAGScheduler: Broadcasting large task binary with size 38.8 MiB\n",
      "24/08/18 13:56:50 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:56:53 WARN DAGScheduler: Broadcasting large task binary with size 47.7 MiB\n",
      "24/08/18 13:56:54 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/08/18 13:56:57 WARN DAGScheduler: Broadcasting large task binary with size 55.9 MiB\n",
      "24/08/18 13:56:58 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/08/18 13:57:02 WARN DAGScheduler: Broadcasting large task binary with size 62.9 MiB\n",
      "24/08/18 13:57:03 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/08/18 13:57:07 WARN DAGScheduler: Broadcasting large task binary with size 1054.4 KiB\n",
      "24/08/18 13:57:07 WARN DAGScheduler: Broadcasting large task binary with size 1859.1 KiB\n",
      "24/08/18 13:57:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/08/18 13:57:08 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "24/08/18 13:57:09 WARN DAGScheduler: Broadcasting large task binary with size 7.3 MiB\n",
      "24/08/18 13:57:09 WARN DAGScheduler: Broadcasting large task binary with size 1065.1 KiB\n",
      "24/08/18 13:57:10 WARN DAGScheduler: Broadcasting large task binary with size 1054.4 KiB\n",
      "24/08/18 13:57:10 WARN DAGScheduler: Broadcasting large task binary with size 1859.1 KiB\n",
      "24/08/18 13:57:11 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/08/18 13:57:11 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "24/08/18 13:57:12 WARN DAGScheduler: Broadcasting large task binary with size 7.3 MiB\n",
      "24/08/18 13:57:12 WARN DAGScheduler: Broadcasting large task binary with size 1065.1 KiB\n",
      "24/08/18 13:57:13 WARN DAGScheduler: Broadcasting large task binary with size 10.0 MiB\n",
      "24/08/18 13:57:13 WARN DAGScheduler: Broadcasting large task binary with size 1163.1 KiB\n",
      "24/08/18 13:57:14 WARN DAGScheduler: Broadcasting large task binary with size 12.9 MiB\n",
      "24/08/18 13:57:14 WARN DAGScheduler: Broadcasting large task binary with size 1175.6 KiB\n",
      "24/08/18 13:57:15 WARN DAGScheduler: Broadcasting large task binary with size 15.9 MiB\n",
      "24/08/18 13:57:16 WARN DAGScheduler: Broadcasting large task binary with size 1120.9 KiB\n",
      "24/08/18 13:57:16 WARN DAGScheduler: Broadcasting large task binary with size 18.6 MiB\n",
      "24/08/18 13:57:18 WARN DAGScheduler: Broadcasting large task binary with size 21.0 MiB\n",
      "24/08/18 13:57:20 WARN DAGScheduler: Broadcasting large task binary with size 1059.2 KiB\n",
      "24/08/18 13:57:21 WARN DAGScheduler: Broadcasting large task binary with size 1059.2 KiB\n",
      "24/08/18 13:57:21 WARN DAGScheduler: Broadcasting large task binary with size 1967.1 KiB\n",
      "24/08/18 13:57:21 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:57:22 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "24/08/18 13:57:23 WARN DAGScheduler: Broadcasting large task binary with size 1359.8 KiB\n",
      "24/08/18 13:57:23 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/08/18 13:57:24 WARN DAGScheduler: Broadcasting large task binary with size 1778.2 KiB\n",
      "24/08/18 13:57:25 WARN DAGScheduler: Broadcasting large task binary with size 14.4 MiB\n",
      "24/08/18 13:57:26 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/08/18 13:57:27 WARN DAGScheduler: Broadcasting large task binary with size 1059.2 KiB\n",
      "24/08/18 13:57:27 WARN DAGScheduler: Broadcasting large task binary with size 1967.1 KiB\n",
      "24/08/18 13:57:28 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:57:29 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "24/08/18 13:57:29 WARN DAGScheduler: Broadcasting large task binary with size 1359.8 KiB\n",
      "24/08/18 13:57:30 WARN DAGScheduler: Broadcasting large task binary with size 9.7 MiB\n",
      "24/08/18 13:57:30 WARN DAGScheduler: Broadcasting large task binary with size 1778.2 KiB\n",
      "24/08/18 13:57:31 WARN DAGScheduler: Broadcasting large task binary with size 14.4 MiB\n",
      "24/08/18 13:57:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/08/18 13:57:33 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "24/08/18 13:57:34 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/08/18 13:57:36 WARN DAGScheduler: Broadcasting large task binary with size 25.8 MiB\n",
      "24/08/18 13:57:36 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/08/18 13:57:38 WARN DAGScheduler: Broadcasting large task binary with size 31.7 MiB\n",
      "24/08/18 13:57:39 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/08/18 13:57:41 WARN DAGScheduler: Broadcasting large task binary with size 37.2 MiB\n",
      "24/08/18 13:57:42 WARN DAGScheduler: Broadcasting large task binary with size 1944.7 KiB\n",
      "24/08/18 13:57:44 WARN DAGScheduler: Broadcasting large task binary with size 42.0 MiB\n",
      "24/08/18 13:57:45 WARN DAGScheduler: Broadcasting large task binary with size 1624.3 KiB\n",
      "24/08/18 13:57:48 WARN DAGScheduler: Broadcasting large task binary with size 1514.7 KiB\n",
      "24/08/18 13:57:49 WARN DAGScheduler: Broadcasting large task binary with size 1514.7 KiB\n",
      "24/08/18 13:57:50 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/08/18 13:57:50 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "24/08/18 13:57:51 WARN DAGScheduler: Broadcasting large task binary with size 1373.8 KiB\n",
      "24/08/18 13:57:52 WARN DAGScheduler: Broadcasting large task binary with size 9.0 MiB\n",
      "24/08/18 13:57:53 WARN DAGScheduler: Broadcasting large task binary with size 2016.4 KiB\n",
      "24/08/18 13:57:54 WARN DAGScheduler: Broadcasting large task binary with size 14.4 MiB\n",
      "24/08/18 13:57:55 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "24/08/18 13:57:56 WARN DAGScheduler: Broadcasting large task binary with size 21.5 MiB\n",
      "24/08/18 13:57:57 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/08/18 13:58:00 WARN DAGScheduler: Broadcasting large task binary with size 1514.7 KiB\n",
      "24/08/18 13:58:00 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/08/18 13:58:01 WARN DAGScheduler: Broadcasting large task binary with size 5.1 MiB\n",
      "24/08/18 13:58:02 WARN DAGScheduler: Broadcasting large task binary with size 1373.8 KiB\n",
      "24/08/18 13:58:03 WARN DAGScheduler: Broadcasting large task binary with size 9.0 MiB\n",
      "24/08/18 13:58:03 WARN DAGScheduler: Broadcasting large task binary with size 2016.4 KiB\n",
      "24/08/18 13:58:05 WARN DAGScheduler: Broadcasting large task binary with size 14.4 MiB\n",
      "24/08/18 13:58:05 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "24/08/18 13:58:07 WARN DAGScheduler: Broadcasting large task binary with size 21.5 MiB\n",
      "24/08/18 13:58:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "24/08/18 13:58:10 WARN DAGScheduler: Broadcasting large task binary with size 29.7 MiB\n",
      "24/08/18 13:58:12 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/08/18 13:58:14 WARN DAGScheduler: Broadcasting large task binary with size 38.6 MiB\n",
      "24/08/18 13:58:15 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/08/18 13:58:18 WARN DAGScheduler: Broadcasting large task binary with size 47.4 MiB\n",
      "24/08/18 13:58:20 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "24/08/18 13:58:23 WARN DAGScheduler: Broadcasting large task binary with size 55.5 MiB\n",
      "24/08/18 13:58:24 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/08/18 13:58:27 WARN DAGScheduler: Broadcasting large task binary with size 62.6 MiB\n",
      "24/08/18 13:58:29 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%writefile ../src/pyspark_model_trainer.py\n",
    "\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create a Spark session.\"\"\"\n",
    "    spark = SparkSession.builder.appName(\"SalaryPrediction\").getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def load_data(file_path, spark):\n",
    "    \"\"\"Load data using PySpark.\"\"\"\n",
    "    try:\n",
    "        data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        logger.info(f\"Data loaded. Schema: {data.printSchema()}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def vectorize_data(data, features):\n",
    "    \"\"\"Convert the feature columns to a single vector column.\"\"\"\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    data = assembler.transform(data)\n",
    "    return data\n",
    "\n",
    "def scale_features(data):\n",
    "    \"\"\"Scale the features using StandardScaler.\"\"\"\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "    scaler_model = scaler.fit(data)\n",
    "    data = scaler_model.transform(data)\n",
    "    return data, scaler_model\n",
    "\n",
    "def train_random_forest(data, target_col):\n",
    "    \"\"\"Train a Random Forest model using PySpark.\"\"\"\n",
    "    rf = RandomForestRegressor(featuresCol=\"scaledFeatures\", labelCol=target_col)\n",
    "\n",
    "    # Create a parameter grid for hyperparameter tuning\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [100, 200, 300]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "        .build()\n",
    "\n",
    "    # Set up cross-validator\n",
    "    crossval = CrossValidator(estimator=rf, estimatorParamMaps=param_grid, evaluator=RegressionEvaluator(labelCol=target_col), numFolds=5)\n",
    "\n",
    "    # Train the model using cross-validation\n",
    "    cv_model = crossval.fit(data)\n",
    "    return cv_model.bestModel\n",
    "\n",
    "def evaluate_model(model, data, target_col):\n",
    "    \"\"\"Evaluate the model using RMSE, MAE, and R2 metrics.\"\"\"\n",
    "    predictions = model.transform(data)\n",
    "    evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\")\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "    mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "    r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "    logger.info(f\"Evaluation Metrics - RMSE: {rmse}, MAE: {mae}, R2: {r2}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def save_model(model, model_save_path):\n",
    "    \"\"\"Save the trained model.\"\"\"\n",
    "    model.write().overwrite().save(model_save_path)\n",
    "    logger.info(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        spark = create_spark_session()\n",
    "\n",
    "        # Load the preprocessed data using PySpark\n",
    "        file_path = '../data/processed/nba_player_data_final_inflated.csv'\n",
    "        predict_season = 2023\n",
    "\n",
    "        # Load, preprocess, and engineer features using PySpark\n",
    "        data = load_data(file_path, spark)\n",
    "        data = format_season(data)\n",
    "        target_season_data, prior_seasons_data = filter_seasons(data, predict_season)\n",
    "        prior_seasons_data = clean_data(prior_seasons_data)\n",
    "        \n",
    "        # Engineer features and check if they were added correctly\n",
    "        prior_seasons_data = engineer_features(prior_seasons_data)\n",
    "        logger.info(f\"Columns after feature engineering: {prior_seasons_data.columns}\")\n",
    "\n",
    "        # Define the features and target column\n",
    "        features = ['PPG', 'APG', 'RPG', 'SPG', 'TOPG', 'Years of Service', 'PER', 'VORP', 'WSPG', 'OWSPG']\n",
    "        target_col = 'SalaryPct'\n",
    "\n",
    "        # Vectorize and scale the features\n",
    "        prior_seasons_data = vectorize_data(prior_seasons_data, features)\n",
    "        prior_seasons_data, scaler_model = scale_features(prior_seasons_data)\n",
    "\n",
    "        # Train the Random Forest model\n",
    "        model = train_random_forest(prior_seasons_data, target_col)\n",
    "\n",
    "        # Evaluate the model on the target season data\n",
    "        target_season_data = vectorize_data(target_season_data, features)\n",
    "        target_season_data = scaler_model.transform(target_season_data)\n",
    "        evaluate_model(model, target_season_data, target_col)\n",
    "\n",
    "        # Save the trained model\n",
    "        # model_save_path = 'data/models/salary_prediction_rf'\n",
    "        # save_model(model, model_save_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Critical error in PySpark model training pipeline: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
