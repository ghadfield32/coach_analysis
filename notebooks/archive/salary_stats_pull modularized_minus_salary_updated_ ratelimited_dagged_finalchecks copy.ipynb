{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pyproject.toml\n",
    "[build-system]\n",
    "requires = [\"setuptools>=70\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[project]\n",
    "name = \"data_science\"\n",
    "version = \"0.0.1\"\n",
    "description = \"General data science/ML environment\"\n",
    "authors = [{ name = \"Geoffrey Hadfield\" }]\n",
    "requires-python = \">=3.10,<3.12\"   # stay on 3.10/3.11; 3.12 still shaky for some wheels\n",
    "\n",
    "dependencies = [\n",
    "  \"numpy>=1.26\",              # keep modern NumPy\n",
    "  \"pandas>=2.2\",\n",
    "  \"scikit-learn>=1.5\",\n",
    "  \"joblib\",\n",
    "  \"matplotlib\",\n",
    "  \"seaborn\",\n",
    "  \"jupyterlab<5.0\",\n",
    "  \"ipykernel<6.30\",\n",
    "  \"dash\",\n",
    "  \"dash-bootstrap-components\",\n",
    "  \"plotly\",\n",
    "  \"opencv-python-headless\",\n",
    "  \"pillow\",\n",
    "  \"tqdm\",\n",
    "  \"statsmodels\",\n",
    "  \"streamlit\",\n",
    "  \"xgboost\",\n",
    "  \"lightgbm\",\n",
    "  \"requests\",\n",
    "  \"IPython\",\n",
    "  \"tabulate\",\n",
    "  \"pyarrow>=10.0.0\",\n",
    "  \"requests-cache\",\n",
    "  \"diskcache\",\n",
    "  \"unidecode\",\n",
    "  \"cpi>=2.0.0\",\n",
    "  \"lxml\",\n",
    "  \"duckdb>=0.10.0\",\n",
    "  \"apache-airflow>=2.9.0\",\n",
    "  # ---- Explainability stack ----\n",
    "  \"shap>=0.46.0\",             # supports NumPy 2, so fine with 1.26+\n",
    "  \"numba>=0.58.1,<0.61\",      # 0.58.1 adds NumPy 1.26 support; 0.60 adds NumPy2\n",
    "  # llvmlite will be pulled transitively with the correct version\n",
    "  # ---- NBA tooling ----\n",
    "  \"nba_api<=1.4.1\",\n",
    "  \"beautifulsoup4\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "spark = [\n",
    "  \"pyspark\",\n",
    "  \"install-jdk>=1.1.0\",\n",
    "]\n",
    "dev = [\n",
    "  \"pytest\",\n",
    "  \"black\",\n",
    "  \"flake8\",\n",
    "  \"mypy\",\n",
    "]\n",
    "\n",
    "[tool.black]\n",
    "line-length = 88\n",
    "target-version = [\"py310\"]\n",
    "\n",
    "[tool.flake8]\n",
    "max-line-length = 88\n",
    "extend-ignore = [\"E203\"]\n",
    "\n",
    "[tool.mypy]\n",
    "python_version = \"3.10\"\n",
    "ignore_missing_imports = true\n",
    "strict_optional = true\n",
    "\n",
    "[tool.setuptools.packages.find]\n",
    "where = [\"src\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/__init__.py\n",
    "\"\"\"\n",
    "NBA Data Pull Package\n",
    "\n",
    "A comprehensive package for fetching, processing, and analyzing NBA player data\n",
    "including salaries, statistics, and advanced metrics.\n",
    "\"\"\"\n",
    "\n",
    "__version__ = \"0.1.0\"\n",
    "__all__ = [\n",
    "    \"main\",\n",
    "    \"fetch_utils\", \n",
    "    \"process_utils\",\n",
    "    \"scrape_utils\",\n",
    "    \"data_utils\",\n",
    "    \"settings\",\n",
    "    \"notebook_helper\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/settings.py\n",
    "# src/salary_nba_data_pull/settings.py\n",
    "from pathlib import Path\n",
    "import os\n",
    "import typing as _t\n",
    "\n",
    "# 🗂️  Central data directory (override via env if needed)\n",
    "DATA_PROCESSED_DIR = Path(\n",
    "    (Path(__file__).resolve().parent.parent.parent)  # project root\n",
    "    / \"data\"\n",
    "    / \"new_processed\"\n",
    ")\n",
    "\n",
    "# optional: allow `DATA_PROCESSED_DIR=/tmp/demo python main.py …`\n",
    "ENV_OVERRIDE: _t.Optional[str] = os.getenv(\"DATA_PROCESSED_DIR\")\n",
    "if ENV_OVERRIDE:\n",
    "    DATA_PROCESSED_DIR = Path(ENV_OVERRIDE).expanduser().resolve()\n",
    "\n",
    "# Legacy path for backward compatibility\n",
    "LEGACY_DATA_PROCESSED_DIR = Path(\n",
    "    (Path(__file__).resolve().parent.parent.parent)  # project root\n",
    "    / \"data\"\n",
    "    / \"processed\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/fetch_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/fetch_utils.py\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache, wraps\n",
    "from http import HTTPStatus\n",
    "from typing import Callable\n",
    "import requests\n",
    "from nba_api.stats.endpoints import commonallplayers, commonplayerinfo, playercareerstats, leaguestandings\n",
    "from requests.exceptions import RequestException\n",
    "from json.decoder import JSONDecodeError\n",
    "from joblib import Memory\n",
    "from unidecode import unidecode\n",
    "from tenacity import (\n",
    "    retry, retry_if_exception, wait_random_exponential,\n",
    "    stop_after_attempt, before_log\n",
    ")\n",
    "\n",
    "# --- NEW: Team game logs endpoint detection ---\n",
    "try:\n",
    "    # newer nba_api\n",
    "    from nba_api.stats.endpoints import teamgamelogs as _teamgamelogs_mod\n",
    "    _HAVE_TEAMGAMELOGS_PLURAL = True\n",
    "except Exception:\n",
    "    _HAVE_TEAMGAMELOGS_PLURAL = False\n",
    "try:\n",
    "    # older nba_api\n",
    "    from nba_api.stats.endpoints import teamgamelog as _teamgamelog_mod\n",
    "    _HAVE_TEAMGAMELOG_SINGULAR = True\n",
    "except Exception:\n",
    "    _HAVE_TEAMGAMELOG_SINGULAR = False\n",
    "\n",
    "REQUESTS_PER_MIN = 8   # ↓ a bit safer for long pulls (NBA suggests ≤10)\n",
    "_SEM = threading.BoundedSemaphore(REQUESTS_PER_MIN)\n",
    "\n",
    "# Set up joblib memory for caching API responses\n",
    "cache_dir = os.path.join(os.path.dirname(__file__), '../../data/cache/nba_api')\n",
    "memory = Memory(cache_dir, verbose=0)\n",
    "\n",
    "def _throttle():\n",
    "    \"\"\"Global semaphore + sleep to stay under REQUESTS_PER_MIN.\"\"\"\n",
    "    _SEM.acquire()\n",
    "    time.sleep(60 / REQUESTS_PER_MIN)\n",
    "    _SEM.release()\n",
    "\n",
    "def _needs_retry(exc: Exception) -> bool:\n",
    "    \"\"\"Return True if we should retry.\"\"\"\n",
    "    if isinstance(exc, requests.HTTPError) and exc.response is not None:\n",
    "        code = exc.response.status_code\n",
    "        if code in (HTTPStatus.TOO_MANY_REQUESTS, HTTPStatus.SERVICE_UNAVAILABLE):\n",
    "            return True\n",
    "    return isinstance(exc, (requests.ConnectionError, requests.Timeout))\n",
    "\n",
    "def _respect_retry_after(resp: requests.Response):\n",
    "    \"\"\"Sleep for server‑suggested time if header present.\"\"\"\n",
    "    if resp is not None and 'Retry-After' in resp.headers:\n",
    "        try:\n",
    "            sleep = int(resp.headers['Retry-After'])\n",
    "            logging.warning(\"↺ server asked to wait %ss\", sleep)\n",
    "            time.sleep(sleep)\n",
    "        except ValueError:\n",
    "            pass   # header unparsable, ignore\n",
    "\n",
    "def _make_retry(fn: Callable) -> Callable:\n",
    "    \"\"\"Decorator to add tenacity retry with jitter + respect Retry-After.\"\"\"\n",
    "    @retry(\n",
    "        retry=retry_if_exception(_needs_retry),\n",
    "        wait=wait_random_exponential(multiplier=2, max=60),\n",
    "        stop=stop_after_attempt(5),\n",
    "        before_sleep=before_log(logging.getLogger(__name__), logging.WARNING),\n",
    "        reraise=True,\n",
    "    )\n",
    "    @wraps(fn)\n",
    "    def _wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except requests.HTTPError as exc:\n",
    "            _respect_retry_after(exc.response)\n",
    "            raise\n",
    "    return _wrapper\n",
    "\n",
    "@memory.cache\n",
    "@_make_retry\n",
    "def fetch_with_retry(endpoint, *, timeout=90, debug=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Thread‑safe, rate‑limited, cached NBA‑Stats call with adaptive back‑off.\n",
    "    \"\"\"\n",
    "    _throttle()\n",
    "    start = time.perf_counter()\n",
    "    resp = endpoint(timeout=timeout, **kwargs)\n",
    "    df = resp.get_data_frames()[0]\n",
    "    if debug:\n",
    "        logging.debug(\"✓ %s in %.1fs %s\", endpoint.__name__,\n",
    "                      time.perf_counter() - start, kwargs)\n",
    "    return df\n",
    "\n",
    "@memory.cache\n",
    "def fetch_all_players(season: str, debug: bool = False) -> dict[str, dict]:\n",
    "    \"\"\"Return {clean_name: {'player_id':…, 'team_id':…}} for *active* roster.\"\"\"\n",
    "    roster_df = fetch_with_retry(\n",
    "        commonallplayers.CommonAllPlayers,\n",
    "        season=season,\n",
    "        is_only_current_season=1,        # <‑‑ key fix\n",
    "        league_id=\"00\",\n",
    "        debug=debug,\n",
    "    )\n",
    "    players: dict[str, dict] = {}\n",
    "    if roster_df is not None:\n",
    "        for _, row in roster_df.iterrows():\n",
    "            clean = unidecode(row[\"DISPLAY_FIRST_LAST\"]).strip().lower()\n",
    "            players[clean] = {\n",
    "                \"player_id\": int(row[\"PERSON_ID\"]),\n",
    "                \"team_id\": int(row[\"TEAM_ID\"]),\n",
    "            }\n",
    "    if debug:\n",
    "        print(f\"[fetch_all_players] {len(players)} active players for {season}\")\n",
    "    return players\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def fetch_season_players(season: str, debug: bool = False) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Return {clean_name: {'player_id':…, 'team_id':…}} for *everyone who was\n",
    "    on a roster at any time during the given season*.\n",
    "    \"\"\"\n",
    "    # call once for the whole database (not \"current‑season only\")\n",
    "    df = fetch_with_retry(\n",
    "        commonallplayers.CommonAllPlayers,\n",
    "        season=season,\n",
    "        is_only_current_season=0,         # <-- key change\n",
    "        league_id=\"00\",\n",
    "        debug=debug,\n",
    "    )\n",
    "    players: dict[str, dict] = {}\n",
    "    if df is not None:\n",
    "        yr = int(season[:4])\n",
    "        # keep rows whose career window encloses this season\n",
    "        df = df[(df.FROM_YEAR.astype(int) <= yr) & (df.TO_YEAR.astype(int) >= yr)]\n",
    "        for _, row in df.iterrows():\n",
    "            clean = unidecode(row[\"DISPLAY_FIRST_LAST\"]).strip().lower()\n",
    "            players[clean] = {\n",
    "                \"player_id\": int(row[\"PERSON_ID\"]),\n",
    "                \"team_id\": int(row[\"TEAM_ID\"]),\n",
    "            }\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[fetch_season_players] {len(players)} players for {season}\")\n",
    "    return players\n",
    "\n",
    "@memory.cache\n",
    "def fetch_player_info(player_id, debug=False):\n",
    "    return fetch_with_retry(commonplayerinfo.CommonPlayerInfo, player_id=player_id, debug=debug)\n",
    "\n",
    "@memory.cache\n",
    "def fetch_career_stats(player_id, debug=False):\n",
    "    return fetch_with_retry(playercareerstats.PlayerCareerStats, player_id=player_id, debug=debug)\n",
    "\n",
    "@memory.cache\n",
    "def fetch_league_standings(season, debug=False):\n",
    "    return fetch_with_retry(leaguestandings.LeagueStandings, season=season, debug=debug)\n",
    "\n",
    "def clear_cache():\n",
    "    \"\"\"Clear the joblib memory cache.\"\"\"\n",
    "    memory.clear()\n",
    "\n",
    "@memory.cache\n",
    "def fetch_team_wl_by_season(season: str,\n",
    "                            season_type: str = \"Regular Season\",\n",
    "                            debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return per‑team W/L for a season from team game logs.\n",
    "    Robust to nba_api versions:\n",
    "      - TeamGameLogs(...).get_data_frames()[0]  (new)\n",
    "      - TeamGameLog(...).get_data_frames()[0]   (old)\n",
    "    We do not fill; if logs are empty, we return an empty DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    if _HAVE_TEAMGAMELOGS_PLURAL:\n",
    "        # new endpoint signature (nullable arg names in newer APIs)\n",
    "        df = fetch_with_retry(\n",
    "            _teamgamelogs_mod.TeamGameLogs,\n",
    "            season_nullable=season,\n",
    "            season_type_nullable=season_type,\n",
    "            debug=debug,\n",
    "        )\n",
    "    elif _HAVE_TEAMGAMELOG_SINGULAR:\n",
    "        # older endpoint\n",
    "        df = fetch_with_retry(\n",
    "            _teamgamelog_mod.TeamGameLog,\n",
    "            season=season,\n",
    "            season_type_all_star=season_type,\n",
    "            debug=debug,\n",
    "        )\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"[fetch_team_wl_by_season] no team game log endpoint available\")\n",
    "        return pd.DataFrame(columns=[\"TeamID\", \"Wins\", \"Losses\"])\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        if debug:\n",
    "            print(f\"[fetch_team_wl_by_season] empty logs for {season}\")\n",
    "        return pd.DataFrame(columns=[\"TeamID\", \"Wins\", \"Losses\"])\n",
    "\n",
    "    # Normalize column names used across versions\n",
    "    # Expect at least TEAM_ID and WL fields.\n",
    "    cols = {c.upper(): c for c in df.columns}\n",
    "    team_id_col = cols.get(\"TEAM_ID\", None)\n",
    "    wl_col = cols.get(\"WL\", None)\n",
    "\n",
    "    if team_id_col is None or wl_col is None:\n",
    "        if debug:\n",
    "            print(f\"[fetch_team_wl_by_season] required columns missing in logs \"\n",
    "                  f\"{list(df.columns)}\")\n",
    "        return pd.DataFrame(columns=[\"TeamID\", \"Wins\", \"Losses\"])\n",
    "\n",
    "    # Count W/L by team\n",
    "    grp = (df.assign(_W=(df[wl_col] == \"W\").astype(int),\n",
    "                     _L=(df[wl_col] == \"L\").astype(int))\n",
    "             .groupby(df[team_id_col], dropna=False)[[\"_W\", \"_L\"]]\n",
    "             .sum()\n",
    "             .rename(columns={\"_W\": \"Wins\", \"_L\": \"Losses\"})\n",
    "             .reset_index()\n",
    "             .rename(columns={team_id_col: \"TeamID\"}))\n",
    "\n",
    "    if debug:\n",
    "        tot_w = int(grp[\"Wins\"].sum())\n",
    "        tot_l = int(grp[\"Losses\"].sum())\n",
    "        print(f\"[fetch_team_wl_by_season] {season} totals: W={tot_w}, L={tot_l}\")\n",
    "\n",
    "    return grp\n",
    "\n",
    "@memory.cache\n",
    "def fetch_team_wl_lookup(season: str,\n",
    "                         season_type: str = \"Regular Season\",\n",
    "                         debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Unified W/L by TeamID for a season.\n",
    "    Primary: team game logs aggregation (robust across nba_api versions).\n",
    "    Fallback: LeagueStandings endpoint.\n",
    "    Returns columns: TeamID, Wins, Losses (one row per TeamID).\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Primary\n",
    "    logs = fetch_team_wl_by_season(season, season_type=season_type, debug=debug)\n",
    "    logs = logs.rename(columns={\"Wins\": \"Wins_logs\", \"Losses\": \"Losses_logs\"})\n",
    "\n",
    "    # Fallback (LeagueStandings)\n",
    "    st = fetch_league_standings(season, debug=debug)\n",
    "    # nba_api LeagueStandings uses uppercase WINS/LOSSES\n",
    "    want_cols = {}\n",
    "    for c in st.columns:\n",
    "        uc = str(c).upper()\n",
    "        if uc == \"TEAM_ID\": want_cols[c] = \"TeamID\"\n",
    "        if uc == \"WINS\":    want_cols[c] = \"Wins_stand\"\n",
    "        if uc == \"LOSSES\":  want_cols[c] = \"Losses_stand\"\n",
    "    st = st.rename(columns=want_cols)\n",
    "    st = st[[c for c in [\"TeamID\", \"Wins_stand\", \"Losses_stand\"] if c in st.columns]].drop_duplicates(\"TeamID\")\n",
    "\n",
    "    # Outer join both sources on TeamID, then coalesce\n",
    "    out = pd.merge(logs, st, on=\"TeamID\", how=\"outer\", validate=\"1:1\")\n",
    "    out[\"Wins\"]   = out[\"Wins_logs\"].combine_first(out[\"Wins_stand\"])\n",
    "    out[\"Losses\"] = out[\"Losses_logs\"].combine_first(out[\"Losses_stand\"])\n",
    "    out = out[[\"TeamID\", \"Wins\", \"Losses\"]].drop_duplicates(\"TeamID\").reset_index(drop=True)\n",
    "\n",
    "    if debug:\n",
    "        miss = int(out[\"Wins\"].isna().sum())\n",
    "        if miss:\n",
    "            print(f\"[fetch_team_wl_lookup] WARN: {miss} TeamID rows still missing Wins/Losses\")\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    debug = True\n",
    "    season = \"2022-23\"\n",
    "    sample_player_name = \"LeBron James\"\n",
    "\n",
    "    # Fetch all players\n",
    "    all_players = fetch_all_players(season, debug=debug)\n",
    "    print(f\"Total players fetched: {len(all_players)}\")\n",
    "\n",
    "    # Fetch player info for a sample player\n",
    "    if sample_player_name.lower() in all_players:\n",
    "        sample_player_id = all_players[sample_player_name.lower()]['player_id']\n",
    "        player_info = fetch_player_info(sample_player_id, debug=debug)\n",
    "        print(f\"Sample player info for {sample_player_name}:\")\n",
    "        print(player_info)\n",
    "\n",
    "        # Fetch career stats for the sample player\n",
    "        career_stats = fetch_career_stats(sample_player_id, debug=debug)\n",
    "        print(f\"Sample player career stats for {sample_player_name}:\")\n",
    "        print(career_stats)\n",
    "    else:\n",
    "        print(f\"Player {sample_player_name} not found in the {season} season data.\")\n",
    "\n",
    "    # Fetch league standings\n",
    "    standings = fetch_league_standings(season, debug=debug)\n",
    "    print(\"League standings:\")\n",
    "    print(standings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/scrape_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/scrape_utils.py\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from typing import Optional\n",
    "import os\n",
    "import requests_cache\n",
    "from unidecode import unidecode\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "from functools import lru_cache\n",
    "import threading\n",
    "_ADV_LOCK   = threading.Lock()\n",
    "_ADV_CACHE: dict[str, pd.DataFrame] = {}   # season -> DataFrame\n",
    "\n",
    "# Install cache for all requests\n",
    "requests_cache.install_cache('nba_scraping', expire_after=86400)  # 24 hours\n",
    "\n",
    "# Create cached session with stale-if-error capability\n",
    "session = requests_cache.CachedSession(\n",
    "    'nba_scraping',\n",
    "    expire_after=86400,\n",
    "    stale_if_error=True       # <-- NEW: serve expired cache if remote 429s\n",
    ")\n",
    "\n",
    "def scrape_salary_cap_history(*, debug: bool = False) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Robust pull of historical cap / tax / apron lines.\n",
    "\n",
    "    Strategy:\n",
    "    1. Try RealGM (live HTML).\n",
    "    2. If the selector fails, look for an existing CSV in DATA_PROCESSED_DIR.\n",
    "    3. As a last‑chance fallback, hit NBA.com / Reuters bulletins for the\n",
    "       current season only (so we still merge *something*).\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "    url = \"https://basketball.realgm.com/nba/info/salary_cap\"\n",
    "\n",
    "    try:\n",
    "        html = requests.get(url, timeout=30).text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # -------- 1️⃣  RealGM table (new markup) --------------------\n",
    "        blk = soup.find(\"pre\")                      # new 2025 layout\n",
    "        if blk:                                     # parse fixed‑width block\n",
    "            rows = [r.strip().split() for r in blk.text.strip().splitlines()]\n",
    "            header = rows[0]\n",
    "            data = rows[1:]\n",
    "            df = pd.DataFrame(data, columns=header)\n",
    "        else:\n",
    "            # Legacy table path (kept for safety)\n",
    "            tbl = soup.select_one(\"table\")\n",
    "            if not tbl:\n",
    "                raise ValueError(\"salary_cap table not found\")\n",
    "            df = pd.read_html(str(tbl))[0]\n",
    "\n",
    "        # ---- normalise ----\n",
    "        df[\"Season\"] = df[\"Season\"].str.extract(r\"(\\d{4}-\\d{4})\")\n",
    "        money_cols = [c for c in df.columns if c != \"Season\"]\n",
    "        for c in money_cols:\n",
    "            df[c] = (\n",
    "                df[c]\n",
    "                .astype(str)\n",
    "                .str.replace(r\"[$,]\", \"\", regex=True)\n",
    "                .replace(\"\", pd.NA)\n",
    "                .astype(float)\n",
    "            )\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[salary‑cap] scraped {len(df)} rows from RealGM\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as exc:\n",
    "        if debug:\n",
    "            print(f\"[salary‑cap] primary scrape failed → {exc!s}\")\n",
    "\n",
    "        # -------- 2️⃣  local cached CSV ----------------------------\n",
    "        fallback = DATA_PROCESSED_DIR / \"salary_cap_history_inflated.csv\"\n",
    "        if fallback.exists():\n",
    "            if debug:\n",
    "                print(f\"[salary‑cap] using cached CSV at {fallback}\")\n",
    "            return pd.read_csv(fallback)\n",
    "\n",
    "        # -------- 3️⃣  NBA.com / Reuters one‑liner -----------------\n",
    "        try:\n",
    "            # Latest season only\n",
    "            # For now, create a minimal fallback with current season data\n",
    "            year = datetime.now().year\n",
    "            cap = 140.588  # 2024-25 cap as fallback\n",
    "            df = pd.DataFrame(\n",
    "                {\"Season\": [f\"{year}-{str(year+1)[-2:]}\"],\n",
    "                 \"Salary Cap\": [cap * 1_000_000]}\n",
    "            )\n",
    "            if debug:\n",
    "                print(\"[salary‑cap] built minimal one‑row DataFrame \"\n",
    "                      \"from fallback values\")\n",
    "            return df\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if debug:\n",
    "        print(\"[salary‑cap] giving up – no data available\")\n",
    "    return None\n",
    "\n",
    "# User-Agent header to avoid Cloudflare blocks\n",
    "UA = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/126.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "DELAY_BETWEEN_REQUESTS = 3  # seconds\n",
    "\n",
    "# Define column templates to guarantee DataFrame structure\n",
    "PLAYER_COLS = [\"Player\", \"Salary\", \"Season\"]\n",
    "TEAM_COLS = [\"Team\", \"Team_Salary\", \"Season\"]\n",
    "\n",
    "# Salary parsing pattern\n",
    "_salary_pat = re.compile(r\"\\$?\\d[\\d,]*\")\n",
    "\n",
    "def _clean_salary(text: str) -> int | None:\n",
    "    \"\"\"Return salary as int or None when text has no digits.\"\"\"\n",
    "    m = _salary_pat.search(text)\n",
    "    return int(m.group(0).replace(\",\", \"\").replace(\"$\", \"\")) if m else None\n",
    "\n",
    "# Name normalization pattern with unidecode\n",
    "def _normalise_name(raw: str) -> str:\n",
    "    \"\"\"ASCII‑fold, trim, lower.\"\"\"\n",
    "    return unidecode(raw).split(\",\")[0].split(\"(\")[0].strip().lower()\n",
    "\n",
    "\n",
    "# ------- INTERNAL HELPER --------\n",
    "def _get_hoopshype_soup(url: str, debug: bool = False) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Hit HoopsHype once with a realistic UA.  \n",
    "    Return BeautifulSoup if the page looks OK, else None.\n",
    "    \"\"\"\n",
    "    for attempt in range(2):\n",
    "        try:\n",
    "            if debug:\n",
    "                print(f\"[fetch] {url} (attempt {attempt+1})\")\n",
    "            resp = requests.get(url, headers=UA, timeout=30)\n",
    "            if resp.status_code != 200:\n",
    "                if debug:\n",
    "                    print(f\"  -> HTTP {resp.status_code}, skipping.\")\n",
    "                return None\n",
    "            html = resp.text\n",
    "            # crude Cloudflare challenge check\n",
    "            if (\"Access denied\" in html) or (\"cf-chl\" in html):\n",
    "                if debug:\n",
    "                    print(\"  -> Cloudflare challenge detected; giving up.\")\n",
    "                return None\n",
    "            return BeautifulSoup(html, \"html.parser\")\n",
    "        except requests.RequestException as e:\n",
    "            if debug:\n",
    "                print(f\"  -> network error {e}, retrying…\")\n",
    "            time.sleep(2 ** attempt + random.random())\n",
    "    return None\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _espn_salary_url(year: int, page: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Build the new ESPN salary URL. Examples:\n",
    "      page 1 → https://www.espn.com/nba/salaries/_/year/2024/seasontype/4\n",
    "      page 3 → https://www.espn.com/nba/salaries/_/year/2024/page/3/seasontype/4\n",
    "    \"\"\"\n",
    "    base = f\"https://www.espn.com/nba/salaries/_/year/{year}\"\n",
    "    return f\"{base}/seasontype/4\" if page == 1 else f\"{base}/page/{page}/seasontype/4\"\n",
    "\n",
    "\n",
    "def _scrape_espn_player_salaries(season_start: int, debug: bool = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Salary scraping was removed – consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Salary scraping was removed – consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def scrape_player_salary_data(start_season: int, end_season: int,\n",
    "                              player_filter: str | None = None,\n",
    "                              debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Salary scraping was removed – consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Salary scraping was removed – consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _scrape_espn_team_salaries(season: str, debug: bool = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Team salary scraping was removed – consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Team salary scraping was removed – consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def scrape_team_salary_data(season: str, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DEPRECATED: Team salary scraping was removed – consume pre-loaded salary parquet instead.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\n",
    "        \"Team salary scraping was removed – consume pre-loaded salary parquet instead.\"\n",
    "    )\n",
    "\n",
    "# --- Season‑level advanced stats --------------------------------------------\n",
    "ADV_METRIC_COLS = [\n",
    "    \"PER\", \"TS%\", \"3PAr\", \"FTr\", \"ORB%\", \"DRB%\", \"TRB%\", \"AST%\", \"STL%\", \"BLK%\",\n",
    "    \"TOV%\", \"USG%\", \"OWS\", \"DWS\", \"WS\", \"WS/48\", \"OBPM\", \"DBPM\", \"BPM\", \"VORP\",\n",
    "    \"ORtg\", \"DRtg\",  # extra goodies if you want them\n",
    "]\n",
    "\n",
    "def _season_advanced_df(season: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Thread‑safe, memoised download of the *season‑wide* advanced‑stats table.\n",
    "\n",
    "    The first thread to request a given season does the HTTP work while holding\n",
    "    a lock; all others simply wait for the result instead of firing duplicate\n",
    "    requests. The DataFrame is cached in‑process for the life of the run.\n",
    "    \"\"\"\n",
    "    if season in _ADV_CACHE:            # fast path, no lock\n",
    "        return _ADV_CACHE[season]\n",
    "\n",
    "    with _ADV_LOCK:                     # only one thread may enter the block\n",
    "        if season in _ADV_CACHE:        # double‑checked locking\n",
    "            return _ADV_CACHE[season]\n",
    "\n",
    "        end_year = int(season[:4]) + 1\n",
    "        url = f\"https://www.basketball-reference.com/leagues/NBA_{end_year}_advanced.html\"\n",
    "        print(f\"[adv] fetching {url}\")\n",
    "        resp = session.get(url, headers=UA, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        df = pd.read_html(StringIO(resp.text), header=0)[0]\n",
    "        df = df[df.Player != \"Player\"]          # drop repeated header rows\n",
    "        df[\"player_key\"] = df.Player.map(_normalise_name)\n",
    "\n",
    "        avail = [c for c in ADV_METRIC_COLS if c in df.columns]\n",
    "        if avail:\n",
    "            df[avail] = df[avail].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "        _ADV_CACHE[season] = df                # memoise\n",
    "        time.sleep(random.uniform(1.5, 2.5))   # be polite\n",
    "        return df\n",
    "\n",
    "def scrape_advanced_metrics(player_name: str,\n",
    "                            season: str,\n",
    "                            *,\n",
    "                            debug: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    O(1) lookup in the cached season DataFrame – zero extra HTTP traffic.\n",
    "    Uses a shared normalizer (nba_utils.normalize_name) to reduce mismatches.\n",
    "    Prints closest suggestions when no row is found (no filling).\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "\n",
    "    # Prefer the shared normalizer from nba_utils; fall back to local\n",
    "    try:\n",
    "        from api.src.airflow_project.utils.nba_utils import normalize_name as _norm\n",
    "    except Exception:\n",
    "        _norm = _normalise_name\n",
    "\n",
    "    df = _season_advanced_df(season)\n",
    "    # Ensure the season table uses the same normalizer\n",
    "    if \"player_key\" not in df.columns or df[\"player_key\"].isna().all():\n",
    "        df = df.copy()\n",
    "        df[\"player_key\"] = df[\"Player\"].map(_norm)\n",
    "\n",
    "    key = _norm(player_name)\n",
    "    row = df.loc[df.player_key == key]\n",
    "\n",
    "    if row.empty:\n",
    "        if debug:\n",
    "            # Provide top-3 closest suggestions to help diagnose mismatches\n",
    "            all_keys = df[\"player_key\"].dropna().unique().tolist()\n",
    "            suggestions = difflib.get_close_matches(key, all_keys, n=3, cutoff=0.75)\n",
    "            print(f\"[adv] no advanced stats for '{player_name}' (key='{key}') in {season}. \"\n",
    "                  f\"Closest: {suggestions}\")\n",
    "        return {}\n",
    "\n",
    "    row = row.iloc[0]\n",
    "    # Only return columns that actually exist in the DataFrame\n",
    "    available_cols = [col for col in ADV_METRIC_COLS if col in row.index]\n",
    "    result = {col: row[col] for col in available_cols}\n",
    "    if debug:\n",
    "        print(f\"[adv] {player_name} → {result}\")\n",
    "    return result\n",
    "# --- End of new season-level advanced stats ---------------------------------\n",
    "\n",
    "def load_injury_data(\n",
    "    file_path: str | Path | None = None,\n",
    "    *,\n",
    "    base_dir: str | Path | None = None,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the historical injury CSV. By default we look inside the *new*\n",
    "    processed folder; pass ``file_path`` to override a specific file,\n",
    "    or ``base_dir`` to point at a different processed directory.\n",
    "    \"\"\"\n",
    "    root = Path(base_dir) if base_dir else DATA_PROCESSED_DIR\n",
    "    if file_path is None:\n",
    "        file_path = root / \"NBA Player Injury Stats(1951 - 2023).csv\"\n",
    "    file_path = Path(file_path).expanduser().resolve()\n",
    "\n",
    "    try:\n",
    "        injury = (\n",
    "            pd.read_csv(file_path)\n",
    "            .assign(Date=lambda d: pd.to_datetime(d[\"Date\"]))\n",
    "        )\n",
    "        injury[\"Season\"] = injury[\"Date\"].apply(\n",
    "            lambda x: (\n",
    "                f\"{x.year}-{str(x.year + 1)[-2:]}\"\n",
    "                if x.month >= 10\n",
    "                else f\"{x.year - 1}-{str(x.year)[-2:]}\"\n",
    "            )\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"[load_injury_data] loaded {len(injury):,} rows from {file_path}\")\n",
    "        return injury\n",
    "    except FileNotFoundError:\n",
    "        if debug:\n",
    "            print(f\"[load_injury_data] ✖ no injury file at {file_path}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage and testing of all functions\n",
    "    debug = True\n",
    "    start_season = 2022\n",
    "    end_season = 2023\n",
    "    sample_player = \"Ja Morant\"  # Example player\n",
    "\n",
    "    print(\"1. Testing scrape_salary_cap_history:\")\n",
    "    salary_cap_history = scrape_salary_cap_history(debug=debug)\n",
    "\n",
    "    print(\"\\n2. Testing scrape_player_salary_data:\")\n",
    "    player_salary_data = scrape_player_salary_data(start_season, end_season, player_filter=sample_player, debug=debug)\n",
    "\n",
    "    print(\"\\n3. Testing scrape_team_salary_data:\")\n",
    "    team_salary_data = scrape_team_salary_data(f\"{start_season}-{str(start_season+1)[-2:]}\", debug=debug)\n",
    "\n",
    "    print(\"\\n4. Testing scrape_advanced_metrics:\")\n",
    "    advanced_metrics = scrape_advanced_metrics(sample_player, f\"{start_season}-{str(start_season+1)[-2:]}\", debug=debug)\n",
    "    print(f\"Advanced Metrics for {sample_player}:\")\n",
    "    print(advanced_metrics)\n",
    "\n",
    "    print(\"\\n5. Testing load_injury_data and merge_injury_data:\")\n",
    "    injury_data = load_injury_data()\n",
    "    if injury_data is not None:\n",
    "        print(injury_data.head())\n",
    "    else:\n",
    "        print(\"No injury data loaded.\")\n",
    "    if not player_salary_data.empty and injury_data is not None:\n",
    "        from salary_nba_data_pull.process_utils import merge_injury_data\n",
    "        merged_data = merge_injury_data(player_salary_data, injury_data)\n",
    "        print(\"Merged data with injury info:\")\n",
    "        columns_to_display = ['Player', 'Season', 'Salary']\n",
    "        if 'Injured' in merged_data.columns:\n",
    "            columns_to_display.append('Injured')\n",
    "        if 'Injury_Periods' in merged_data.columns:\n",
    "            columns_to_display.append('Injury_Periods')\n",
    "        if 'Total_Days_Injured' in merged_data.columns:\n",
    "            columns_to_display.append('Total_Days_Injured')\n",
    "        if 'Injury_Risk' in merged_data.columns:\n",
    "            columns_to_display.append('Injury_Risk')\n",
    "        print(merged_data[columns_to_display].head())\n",
    "\n",
    "    if not player_salary_data.empty:\n",
    "        avg_salary = player_salary_data['Salary'].mean()\n",
    "        print(f\"Average salary for {sample_player} from {start_season} to {end_season}: ${avg_salary:,.2f}\")\n",
    "\n",
    "    if not team_salary_data.empty:\n",
    "        highest_team_salary = team_salary_data.loc[team_salary_data['Team_Salary'].idxmax()]\n",
    "        print(f\"Team with highest salary in {start_season}-{end_season}: {highest_team_salary['Team']} (${highest_team_salary['Team_Salary']:,.2f})\")\n",
    "\n",
    "    if not injury_data.empty:\n",
    "        injury_count = injury_data['Relinquished'].str.contains(sample_player, case=False).sum()\n",
    "        print(f\"Number of injuries/illnesses for {sample_player} from {start_season} to {end_season}: {injury_count}\")\n",
    "\n",
    "    print(\"\\nAll tests completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/process_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/process_utils.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from salary_nba_data_pull.fetch_utils import fetch_all_players, fetch_career_stats, fetch_player_info, fetch_league_standings\n",
    "from salary_nba_data_pull.scrape_utils import scrape_advanced_metrics\n",
    "\n",
    "# --- CPI lazy‑loader --------------------------------------------------\n",
    "_CPI_AVAILABLE = False  # toggled at runtime\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _ensure_cpi_ready(debug: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Import `cpi` lazily and guarantee its internal SQLite DB is usable.\n",
    "    Returns True when inflation data are available, False otherwise.\n",
    "    \"\"\"\n",
    "    global _CPI_AVAILABLE\n",
    "    try:\n",
    "        import importlib\n",
    "        cpi = importlib.import_module(\"cpi\")        # late import\n",
    "        try:\n",
    "            _ = cpi.models.Series.get_by_id(\"0000\")  # 1‑row sanity query\n",
    "            _CPI_AVAILABLE = True\n",
    "            return True\n",
    "        except sqlite3.OperationalError:\n",
    "            if debug:\n",
    "                logging.warning(\"[CPI] DB invalid – rebuilding from BLS…\")\n",
    "            cpi.update(rebuild=True)                # expensive network call\n",
    "            _CPI_AVAILABLE = True\n",
    "            return True\n",
    "    except ModuleNotFoundError:\n",
    "        if debug:\n",
    "            logging.warning(\"[CPI] package not installed\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            logging.error(\"[CPI] unexpected CPI failure: %s\", e)\n",
    "    return False\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def inflate_value(value: float, year_str: str,\n",
    "                  *, debug: bool = False, skip_inflation: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Inflate `value` from the dollars of `year_str` (YYYY or YYYY‑YY) to 2022 USD.\n",
    "    If CPI data are unavailable or the user opts out, return the original value.\n",
    "    \"\"\"\n",
    "    if skip_inflation or not _ensure_cpi_ready(debug):\n",
    "        return value\n",
    "    try:\n",
    "        import cpi                                       # safe: DB ready\n",
    "        year = int(year_str[:4])\n",
    "        if year >= datetime.now().year:\n",
    "            return value\n",
    "        return float(cpi.inflate(value, year, to=2022))\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            logging.error(\"[CPI] inflate failed for %s: %s\", year_str, e)\n",
    "        return value\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def calculate_percentages(df, debug=False):\n",
    "    \"\"\"\n",
    "    Calculate shooting percentages and other derived statistics.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Calculate shooting percentages\n",
    "    if 'FGA' in df.columns and 'FG' in df.columns:\n",
    "        df['FG%'] = (df['FG'] / df['FGA'] * 100).round(2)\n",
    "        df['FG%'] = df['FG%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if '3PA' in df.columns and '3P' in df.columns:\n",
    "        df['3P%'] = (df['3P'] / df['3PA'] * 100).round(2)\n",
    "        df['3P%'] = df['3P%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'FTA' in df.columns and 'FT' in df.columns:\n",
    "        df['FT%'] = (df['FT'] / df['FTA'] * 100).round(2)\n",
    "        df['FT%'] = df['FT%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Calculate efficiency metrics\n",
    "    if 'PTS' in df.columns and 'FGA' in df.columns and 'FTA' in df.columns:\n",
    "        df['TS%'] = (df['PTS'] / (2 * (df['FGA'] + 0.44 * df['FTA'])) * 100).round(2)\n",
    "        df['TS%'] = df['TS%'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'PTS' in df.columns and 'MP' in df.columns:\n",
    "        df['PTS_per_36'] = (df['PTS'] / df['MP'] * 36).round(2)\n",
    "        df['PTS_per_36'] = df['PTS_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'AST' in df.columns and 'MP' in df.columns:\n",
    "        df['AST_per_36'] = (df['AST'] / df['MP'] * 36).round(2)\n",
    "        df['AST_per_36'] = df['AST_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if 'TRB' in df.columns and 'MP' in df.columns:\n",
    "        df['TRB_per_36'] = (df['TRB'] / df['MP'] * 36).round(2)\n",
    "        df['TRB_per_36'] = df['TRB_per_36'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Percentage calculations completed\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_player_data(player_name: str, season: str,\n",
    "                        all_players: dict[str, dict], *,\n",
    "                        debug: bool = False) -> dict | None:\n",
    "    \"\"\"\n",
    "    Build a single‑player dict for a given season with a concrete Team/TeamID.\n",
    "    For traded players:\n",
    "      • Prefer a non‑TOT row for that season.\n",
    "      • Pick the row with max GP (tie‑break by MIN).\n",
    "    This avoids ambiguous team context that breaks W/L joins.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    meta = all_players.get(player_name.lower().strip())\n",
    "    if not meta:\n",
    "        return None\n",
    "\n",
    "    pid = meta[\"player_id\"]\n",
    "    info_df   = fetch_player_info(pid, debug=debug)\n",
    "    career_df = fetch_career_stats(pid, debug=debug)\n",
    "    if career_df is None or career_df.empty:\n",
    "        return None\n",
    "\n",
    "    # rows for the requested season (may include multiple teams + a total row)\n",
    "    srows = career_df.loc[career_df.SEASON_ID.eq(season)].copy()\n",
    "    if srows.empty:\n",
    "        return None\n",
    "\n",
    "    # Prefer concrete team rows over season \"TOT/2TM/3TM\" rows\n",
    "    def _is_total_label(x: str) -> bool:\n",
    "        x = str(x).upper()\n",
    "        return x in {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}  # BBR uses 2TM/3TM; NBA may have \"TOT\"\n",
    "    non_tot = srows[~srows[\"TEAM_ABBREVIATION\"].map(_is_total_label)]\n",
    "\n",
    "    pick_from = non_tot if not non_tot.empty else srows\n",
    "    # pick the most representative stint: max GP, then MIN\n",
    "    season_row = (pick_from.sort_values([\"GP\", \"MIN\"], ascending=False)\n",
    "                           .iloc[0])\n",
    "\n",
    "    # Build the record\n",
    "    data = {\n",
    "        \"Player\": player_name,\n",
    "        \"Season\": season,\n",
    "        \"Team\":   season_row[\"TEAM_ABBREVIATION\"],\n",
    "        \"Age\":    season_row[\"PLAYER_AGE\"],\n",
    "        \"GP\":     season_row[\"GP\"],\n",
    "        \"GS\":     season_row.get(\"GS\", 0),\n",
    "        \"MP\":     season_row[\"MIN\"],\n",
    "\n",
    "        \"PTS\": season_row[\"PTS\"],\n",
    "        \"FG\":  season_row[\"FGM\"],  \"FGA\": season_row[\"FGA\"],\n",
    "        \"3P\":  season_row[\"FG3M\"], \"3PA\": season_row[\"FG3A\"],\n",
    "        \"FT\":  season_row[\"FTM\"],  \"FTA\": season_row[\"FTA\"],\n",
    "\n",
    "        \"TRB\": season_row[\"REB\"], \"AST\": season_row[\"AST\"],\n",
    "        \"STL\": season_row[\"STL\"], \"BLK\": season_row[\"BLK\"],\n",
    "        \"TOV\": season_row[\"TOV\"], \"PF\":  season_row[\"PF\"],\n",
    "\n",
    "        \"ORB\": season_row.get(\"OREB\", np.nan),\n",
    "        \"DRB\": season_row.get(\"DREB\", np.nan),\n",
    "    }\n",
    "\n",
    "    # TeamID from the chosen season row whenever possible\n",
    "    data[\"TeamID\"] = season_row.get(\"TEAM_ID\", np.nan)\n",
    "\n",
    "    # roster meta (position, experience)\n",
    "    if info_df is not None and not info_df.empty:\n",
    "        ir = info_df.iloc[0]\n",
    "        data[\"Position\"]          = ir.get(\"POSITION\", \"\")\n",
    "        data[\"Years_of_Service\"]  = ir.get(\"SEASON_EXP\", None)\n",
    "\n",
    "    # Derived splits (leave denominator=0 as NaN, do not fill)\n",
    "    two_att     = data[\"FGA\"] - data[\"3PA\"]\n",
    "    data[\"2P\"]  = data[\"FG\"] - data[\"3P\"]\n",
    "    data[\"2PA\"] = two_att\n",
    "    data[\"eFG%\"] = round((data[\"FG\"] + 0.5 * data[\"3P\"]) / data[\"FGA\"] * 100, 2) if data[\"FGA\"] else np.nan\n",
    "    data[\"2P%\"]  = round(data[\"2P\"] / two_att * 100, 2)                           if two_att else np.nan\n",
    "\n",
    "    return data\n",
    "\n",
    "def merge_injury_data(player_data: pd.DataFrame,\n",
    "                      injury_data: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach four injury‑related columns. If a player has no injuries, leave the fields as NA\n",
    "    (pd.NA) instead of empty strings so repeated runs compare equal.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    if player_data.empty:\n",
    "        return player_data\n",
    "\n",
    "    out = player_data.copy()\n",
    "\n",
    "    # Ensure columns exist with NA defaults\n",
    "    defaults = {\n",
    "        \"Injured\": False,\n",
    "        \"Injury_Periods\": pd.NA,\n",
    "        \"Total_Days_Injured\": 0,\n",
    "        \"Injury_Risk\": \"Low Risk\",\n",
    "    }\n",
    "    for c, v in defaults.items():\n",
    "        if c not in out.columns:\n",
    "            out[c] = v\n",
    "\n",
    "    if injury_data is None or injury_data.empty:\n",
    "        # normalize empties just in case\n",
    "        out[\"Injury_Periods\"] = out[\"Injury_Periods\"].replace(\"\", pd.NA)\n",
    "        return out\n",
    "\n",
    "    # Process each player/season\n",
    "    for idx, row in out.iterrows():\n",
    "        pname = row[\"Player\"]\n",
    "        season = row[\"Season\"]\n",
    "\n",
    "        mask = (injury_data[\"Season\"] == season) & \\\n",
    "               (injury_data[\"Relinquished\"].str.contains(pname, case=False, na=False))\n",
    "        player_inj = injury_data.loc[mask]\n",
    "\n",
    "        if player_inj.empty:\n",
    "            continue  # keep defaults\n",
    "\n",
    "        periods = []\n",
    "        total_days = 0\n",
    "        for _, inj in player_inj.iterrows():\n",
    "            start = inj[\"Date\"]\n",
    "            # find the first acquired record after start\n",
    "            got_back = injury_data[\n",
    "                (injury_data[\"Date\"] > start) &\n",
    "                (injury_data[\"Acquired\"].str.contains(pname, case=False, na=False))\n",
    "            ]\n",
    "            if not got_back.empty:\n",
    "                end = got_back.iloc[0][\"Date\"]\n",
    "            else:\n",
    "                end_year = int(season.split(\"-\")[1])\n",
    "                end = pd.Timestamp(f\"{end_year}-06-30\")\n",
    "\n",
    "            total_days += (end - start).days\n",
    "            periods.append(f\"{start:%Y-%m-%d} - {end:%Y-%m-%d}\")\n",
    "\n",
    "        out.at[idx, \"Injured\"] = True\n",
    "        out.at[idx, \"Injury_Periods\"] = \"; \".join(periods) if periods else pd.NA\n",
    "        out.at[idx, \"Total_Days_Injured\"] = total_days\n",
    "\n",
    "        if total_days < 10:\n",
    "            risk = \"Low Risk\"\n",
    "        elif total_days <= 20:\n",
    "            risk = \"Moderate Risk\"\n",
    "        else:\n",
    "            risk = \"High Risk\"\n",
    "        out.at[idx, \"Injury_Risk\"] = risk\n",
    "\n",
    "    # final normalization\n",
    "    out[\"Injury_Periods\"] = out[\"Injury_Periods\"].replace(\"\", pd.NA)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# USAGE / LOAD METRICS\n",
    "# Inspired by Basketball-Reference (USG%), Nylon Calculus (True Usage parts),\n",
    "# and Thinking Basketball (Offensive Load). See docs in code.\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "USAGE_COMPONENT_COLS = [\n",
    "    \"USG%\",               # already scraped but we may recompute if missing\n",
    "    \"Scoring_Usage%\",     # (FGA + 0.44*FTA) share of team poss\n",
    "    \"Playmaking_Usage%\",  # (AST-created FG poss) share\n",
    "    \"Turnover_Usage%\",    # TOV share\n",
    "    \"True_Usage%\",        # Scoring + Playmaking + TO\n",
    "    \"Offensive_Load%\",    # Thinking Basketball style\n",
    "    \"Player_Poss\",        # est. possessions used by player\n",
    "    \"Team_Poss\",          # est. team possessions (for join/QA)\n",
    "]\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    return np.where(b == 0, np.nan, a / b)\n",
    "\n",
    "def add_usage_components(df: pd.DataFrame, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute Scoring‑/Playmaking‑/Turnover usage plus Offensive‑Load.\n",
    "\n",
    "    The helper now:\n",
    "      • Renames OREB/DREB → ORB/DRB if needed.\n",
    "      • Warns – but does not crash – when expected stats are missing.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # ── 0. Normalise column spelling ───────────────────────────────\n",
    "    col_map = {\"OREB\": \"ORB\", \"DREB\": \"DRB\"}\n",
    "    out.rename(columns={k: v for k, v in col_map.items() if k in out.columns}, inplace=True)\n",
    "\n",
    "    # ── 1. Summarise team totals ───────────────────────────────────\n",
    "    want = [\"FGA\", \"FTA\", \"TOV\", \"FG\", \"ORB\", \"DRB\", \"TRB\", \"MP\", \"AST\"]\n",
    "    have = [c for c in want if c in out.columns]\n",
    "    if len(have) < len(want) and debug:\n",
    "        print(f\"[usage] missing cols → {sorted(set(want) - set(have))}\")\n",
    "\n",
    "    grp = out.groupby([\"Season\", \"Team\"], dropna=False)\n",
    "    team_totals = grp[have].sum(min_count=1).rename(columns=lambda c: f\"Tm_{c}\")\n",
    "    out = out.merge(team_totals, left_on=[\"Season\", \"Team\"], right_index=True, how=\"left\")\n",
    "\n",
    "    # ── 2. Possession estimates ────────────────────────────────────\n",
    "    out[\"Team_Poss\"]   = out[\"Tm_FGA\"] + 0.44 * out[\"Tm_FTA\"] + out[\"Tm_TOV\"]\n",
    "    out[\"Player_Poss\"] = out[\"FGA\"]    + 0.44 * out[\"FTA\"]    + out[\"TOV\"]\n",
    "\n",
    "    share = out[\"Player_Poss\"] / out[\"Team_Poss\"]\n",
    "\n",
    "    # fill USG% if missing\n",
    "    if \"USG%\" not in out.columns or out[\"USG%\"].isna().all():\n",
    "        out[\"USG%\"] = (share * 100).round(2)\n",
    "\n",
    "    scor = out[\"FGA\"] + 0.44 * out[\"FTA\"]\n",
    "    tov  = out[\"TOV\"]\n",
    "    ast_cre = 0.37 * out[\"AST\"]\n",
    "\n",
    "    out[\"Scoring_Usage%\"]     = (scor / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"Turnover_Usage%\"]    = (tov  / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"Playmaking_Usage%\"]  = (ast_cre / out[\"Team_Poss\"] * 100).round(2)\n",
    "    out[\"True_Usage%\"]        = (out[\"Scoring_Usage%\"] + out[\"Turnover_Usage%\"] +\n",
    "                                 out[\"Playmaking_Usage%\"]).round(2)\n",
    "\n",
    "    tsa       = scor\n",
    "    creation  = 0.8 * out[\"AST\"]\n",
    "    non_cre   = 0.2 * out[\"AST\"]\n",
    "    out[\"Offensive_Load%\"] = ((tsa + creation + non_cre + tov) / out[\"Team_Poss\"] * 100).round(2)\n",
    "\n",
    "    return out\n",
    "\n",
    "# --- NEW: Advanced metrics audit helper ---\n",
    "def report_advanced_mismatches(player_names: list[str], season: str, *, topk: int = 3):\n",
    "    \"\"\"\n",
    "    Prints players we couldn't match in the BBR advanced table and closest suggestions.\n",
    "    No filling - just diagnostics.\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "    from salary_nba_data_pull.scrape_utils import _season_advanced_df\n",
    "    \n",
    "    # Prefer the shared normalizer from nba_utils; fall back to local\n",
    "    try:\n",
    "        from api.src.airflow_project.utils.nba_utils import normalize_name as _norm\n",
    "    except Exception:\n",
    "        from salary_nba_data_pull.scrape_utils import _normalise_name as _norm\n",
    "\n",
    "    df = _season_advanced_df(season)\n",
    "    # Ensure the season table uses the same normalizer\n",
    "    if \"player_key\" not in df.columns or df[\"player_key\"].isna().all():\n",
    "        df = df.copy()\n",
    "        df[\"player_key\"] = df[\"Player\"].map(_norm)\n",
    "\n",
    "    keys = set(df[\"player_key\"].dropna())\n",
    "    all_keys = list(keys)\n",
    "    misses = []\n",
    "    \n",
    "    for raw in player_names:\n",
    "        q = _norm(raw)\n",
    "        if q not in keys:\n",
    "            suggestions = difflib.get_close_matches(q, all_keys, n=topk, cutoff=0.75)\n",
    "            print(f\"[adv-miss] {raw}  → key='{q}'  suggestions={suggestions}\")\n",
    "            misses.append(raw)\n",
    "    \n",
    "    print(f\"[adv-miss] total misses: {len(misses)}/{len(player_names)}\")\n",
    "    return misses\n",
    "\n",
    "def attach_wins_losses_using_logs(df_season: pd.DataFrame,\n",
    "                                  season: str,\n",
    "                                  logs_wl: pd.DataFrame,\n",
    "                                  *,\n",
    "                                  debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-merge W/L by TeamID using precomputed team-game-log totals.\n",
    "    No filling. If TeamID is missing or ambiguous (e.g., TOT), W/L stays NaN.\n",
    "    \"\"\"\n",
    "    if df_season.empty or logs_wl.empty:\n",
    "        return df_season\n",
    "\n",
    "    out = df_season.merge(\n",
    "        logs_wl.drop_duplicates(\"TeamID\"),\n",
    "        on=\"TeamID\", how=\"left\", validate=\"m:1\"\n",
    "    )\n",
    "    if debug:\n",
    "        null_rate = out[\"Wins\"].isna().mean() * 100\n",
    "        print(f\"[attach_wins_losses_using_logs] {season} W/L null% = {null_rate:.2f}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def diagnose_wl_nulls(df_after_merge: pd.DataFrame,\n",
    "                      season: str,\n",
    "                      *,\n",
    "                      debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attribute W/L nulls to concrete reasons:\n",
    "      - TeamID missing\n",
    "      - team label equals 'TOT' for multi-team season rows\n",
    "      - TeamID present but no match in W/L lookup\n",
    "      - Player has 0 GP (edge case)\n",
    "    Returns a small DataFrame with reason counts/samples.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    if df_after_merge.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mask_null = df_after_merge[\"Wins\"].isna()\n",
    "    sub = df_after_merge.loc[mask_null].copy()\n",
    "\n",
    "    reasons = []\n",
    "    if \"Team\" in sub.columns:\n",
    "        reasons.append((\"TOT team label\", sub[\"Team\"].str.upper().eq(\"TOT\")))\n",
    "    reasons.append((\"TeamID missing\", sub[\"TeamID\"].isna()))\n",
    "    reasons.append((\"Zero GP\", sub.get(\"GP\", pd.Series(index=sub.index)).fillna(0).eq(0)))\n",
    "    # anything else falls into \"No W/L match for TeamID\"\n",
    "    base_mask = pd.Series(False, index=sub.index)\n",
    "    for _, m in reasons:\n",
    "        base_mask |= m.fillna(False)\n",
    "    reasons.append((\"No W/L match for TeamID\", ~base_mask))\n",
    "\n",
    "    rows = []\n",
    "    for label, m in reasons:\n",
    "        cnt = int(m.sum())\n",
    "        ex = sub.loc[m, [\"Player\",\"Team\",\"TeamID\"]].head(5).to_dict(\"records\") if cnt else []\n",
    "        rows.append({\"season\": season, \"reason\": label, \"count\": cnt, \"examples\": ex})\n",
    "\n",
    "    diag = pd.DataFrame(rows).sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "    if debug:\n",
    "        print(\"[diagnose_wl_nulls]\")\n",
    "        print(diag)\n",
    "    return diag\n",
    "\n",
    "\n",
    "def diagnose_injury_nulls(df: pd.DataFrame,\n",
    "                          injury_df: pd.DataFrame | None,\n",
    "                          *,\n",
    "                          debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Break down Injury_Periods nulls by season:\n",
    "      - season beyond injury source coverage\n",
    "      - player has no injury rows in covered season (legit NA)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    by_season = (df.groupby(\"Season\")\n",
    "                   .agg(total=(\"Player\",\"count\"),\n",
    "                        nulls=(\"Injury_Periods\", lambda s: int(s.isna().sum())))\n",
    "                   .assign(null_pct=lambda d: 100*d[\"nulls\"]/d[\"total\"])\n",
    "                   .reset_index())\n",
    "\n",
    "    if injury_df is not None and not injury_df.empty:\n",
    "        covered = set(injury_df[\"Season\"].dropna().unique())\n",
    "        by_season[\"in_coverage\"] = by_season[\"Season\"].isin(covered)\n",
    "    else:\n",
    "        by_season[\"in_coverage\"] = False\n",
    "\n",
    "    if debug:\n",
    "        print(\"[diagnose_injury_nulls]\")\n",
    "        print(by_season)\n",
    "    return by_season\n",
    "\n",
    "\n",
    "def audit_min_date_alignment(source_map: dict[str, tuple[pd.DataFrame, list[str]]],\n",
    "                             *,\n",
    "                             debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each source, compute the earliest season where *all listed columns*\n",
    "    are non‑NA for at least one row.\n",
    "    `source_map[name] = (df, [\"colA\",\"colB\",...])`\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    rows = []\n",
    "    for name, (df, cols) in source_map.items():\n",
    "        if df is None or df.empty:\n",
    "            rows.append({\"source\": name, \"min_non_na_season\": None, \"cols\": cols})\n",
    "            continue\n",
    "        # seasons with any non-NA across the requested columns\n",
    "        ok = (df[cols].notna().any(axis=1))\n",
    "        seasons = pd.Series(df[\"Season\"][ok].dropna().unique())\n",
    "        min_seas = seasons.sort_values().iloc[0] if not seasons.empty else None\n",
    "        rows.append({\"source\": name, \"min_non_na_season\": min_seas, \"cols\": cols})\n",
    "    rep = pd.DataFrame(rows)\n",
    "    if debug:\n",
    "        print(\"[audit_min_date_alignment]\")\n",
    "        print(rep)\n",
    "    return rep\n",
    "\n",
    "\n",
    "def diagnose_advanced_nulls(df: pd.DataFrame,\n",
    "                            season: str,\n",
    "                            *,\n",
    "                            max_print: int = 5,\n",
    "                            debug: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Summarise advanced-stat availability & name mismatches for a season.\n",
    "    Uses your `report_advanced_mismatches` under the hood.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return {\"season\": season, \"players\": 0, \"adv_nulls\": 0, \"misses\": []}\n",
    "\n",
    "    # Which players lack BPM for the season?\n",
    "    lack = df[\"BPM\"].isna() if \"BPM\" in df.columns else df[\"Player\"].notna()  # best effort\n",
    "    players = df.loc[lack, \"Player\"].dropna().unique().tolist()\n",
    "    misses = report_advanced_mismatches(players, season) if players else []\n",
    "    out = {\"season\": season, \"players\": df[\"Player\"].nunique(),\n",
    "           \"adv_nulls\": int(lack.sum()), \"misses\": misses[:max_print]}\n",
    "    if debug:\n",
    "        print(\"[diagnose_advanced_nulls]\", out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def attach_wins_losses(df_season: pd.DataFrame,\n",
    "                       season: str,\n",
    "                       *,\n",
    "                       debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-merge W/L using unified lookup (game logs + standings).\n",
    "    Emits a reason breakdown for any residual nulls.\n",
    "    \"\"\"\n",
    "    if df_season.empty:\n",
    "        return df_season\n",
    "\n",
    "    from salary_nba_data_pull.fetch_utils import fetch_team_wl_lookup\n",
    "    wl = fetch_team_wl_lookup(season, season_type=\"Regular Season\", debug=debug)\n",
    "\n",
    "    out = df_season.merge(wl.drop_duplicates(\"TeamID\"),\n",
    "                          on=\"TeamID\", how=\"left\", validate=\"m:1\")\n",
    "    if debug:\n",
    "        null_rate = out[\"Wins\"].isna().mean() * 100\n",
    "        print(f\"[attach_wins_losses] {season} W/L null% = {null_rate:.2f}\")\n",
    "        if null_rate > 0:\n",
    "            _ = diagnose_wl_nulls(out, season, debug=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def merge_advanced_metrics(df_season: pd.DataFrame,\n",
    "                           season: str,\n",
    "                           *,\n",
    "                           debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach advanced metrics (PER, TS%, 3PAr, FTr, ORB%, DRB%, TRB%, AST%, STL%, BLK%, TOV%, USG%, \n",
    "    OWS, DWS, WS, WS/48, OBPM, DBPM, BPM, VORP) for the given season.\n",
    "\n",
    "    Strategy:\n",
    "      • Load the season advanced table once (memoized).\n",
    "      • Normalize names using shared normalizer.\n",
    "      • Prefer a player's 'total' (TOT/2TM/3TM) row; else take the row with max MP.\n",
    "      • Join only columns present; do not fill missing values.\n",
    "    \"\"\"\n",
    "    if df_season.empty:\n",
    "        return df_season\n",
    "\n",
    "    from salary_nba_data_pull.scrape_utils import _season_advanced_df, ADV_METRIC_COLS\n",
    "    try:\n",
    "        from api.src.airflow_project.utils.nba_utils import normalize_name as _norm\n",
    "    except Exception:\n",
    "        from salary_nba_data_pull.scrape_utils import _normalise_name as _norm\n",
    "\n",
    "    adv = _season_advanced_df(season)\n",
    "    if adv is None or adv.empty:\n",
    "        if debug:\n",
    "            print(f\"[merge_advanced_metrics] no advanced table for {season}\")\n",
    "        return df_season\n",
    "\n",
    "    # Ensure a normalized key on both sides\n",
    "    adv = adv.copy()\n",
    "    if \"player_key\" not in adv.columns:\n",
    "        adv[\"player_key\"] = adv[\"Player\"].map(_norm)\n",
    "    df = df_season.copy()\n",
    "    df[\"player_key\"] = df[\"Player\"].map(_norm)\n",
    "\n",
    "    # For each player in ADV: prefer total row (TOT/2TM/3TM), else max MP\n",
    "    def _is_total_team(x: str) -> bool:\n",
    "        x = str(x).upper()\n",
    "        return x in {\"TOT\", \"2TM\", \"3TM\", \"4TM\"}\n",
    "\n",
    "    # Check what columns are actually available\n",
    "    available_cols = [c for c in [\"player_key\", \"Player\", \"MP\"] + ADV_METRIC_COLS if c in adv.columns]\n",
    "    # Add team column if it exists (might be \"Tm\" or \"Team\")\n",
    "    team_col = None\n",
    "    for col in [\"Tm\", \"Team\", \"TEAM\"]:\n",
    "        if col in adv.columns:\n",
    "            team_col = col\n",
    "            break\n",
    "    \n",
    "    if team_col:\n",
    "        available_cols.append(team_col)\n",
    "    \n",
    "    adv_small = adv[available_cols].copy()\n",
    "\n",
    "    # Order by total‑first, then MP desc\n",
    "    adv_small[\"_is_tot\"] = adv_small[team_col].map(_is_total_team).astype(int)\n",
    "    adv_small = adv_small.sort_values([\"player_key\", \"_is_tot\", \"MP\"], ascending=[True, False, False])\n",
    "\n",
    "    # Pick first row per player_key\n",
    "    adv_one = adv_small.drop_duplicates(\"player_key\", keep=\"first\").drop(columns=[\"_is_tot\"])\n",
    "\n",
    "    # Join on player_key\n",
    "    merged = df.merge(adv_one.drop(columns=[\"Player\"]), on=\"player_key\", how=\"left\", validate=\"m:1\")\n",
    "    merged = merged.drop(columns=[\"player_key\"])\n",
    "\n",
    "    if debug:\n",
    "        # Report unmatched names\n",
    "        misses = merged.loc[merged[ADV_METRIC_COLS[0] if ADV_METRIC_COLS[0] in merged.columns else \"MP\"].isna(), \"Player\"].unique().tolist()\n",
    "        print(f\"[merge_advanced_metrics] {season}: advanced attached, misses={len(misses)}\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/data_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/data_utils.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from salary_nba_data_pull.process_utils import (\n",
    "    inflate_value\n",
    ")\n",
    "from salary_nba_data_pull.quality import (\n",
    "    ExpectedSchema, audit_dataframe, write_audit_reports\n",
    ")\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "PRESERVE_EVEN_IF_ALL_NA = {\n",
    "    \"3P%\", \"Injured\", \"Injury_Periods\", \"Total_Days_Injured\", \"Injury_Risk\"\n",
    "}\n",
    "\n",
    "# --- NEW: End-of-pipeline column pruning ---\n",
    "DROP_AT_END = {\n",
    "    \"Salary\",\n",
    "    \"2nd Apron\", \"Second Apron\",   # drop only second apron as requested\n",
    "}\n",
    "\n",
    "def prune_end_columns(df: pd.DataFrame, *, debug: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Drop end-of-pipeline columns without masking upstream issues.\"\"\"\n",
    "    existing = [c for c in df.columns if c in DROP_AT_END]\n",
    "    if debug and existing:\n",
    "        print(f\"[prune_end_columns] dropping columns at persist: {existing}\")\n",
    "    return df.drop(columns=existing, errors=\"ignore\")\n",
    "\n",
    "# --- NEW helper ------------------------------------------------------\n",
    "def load_salary_cap_parquet(path: str | Path, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the pre‑inflated salary‑cap parquet file; fall back to CSV loader\n",
    "    if the parquet is not found.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser().with_suffix(\".parquet\")\n",
    "    if path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary-cap] loading Parquet: {path}\")\n",
    "        return pd.read_parquet(path)\n",
    "    # fallback to old CSV helper for legacy compatibility\n",
    "    csv_path = path.with_suffix(\".csv\")\n",
    "    if csv_path.exists():\n",
    "        return load_salary_cap_csv(csv_path, debug=debug)\n",
    "    raise FileNotFoundError(f\"No salary‑cap parquet or CSV found at {path}\")\n",
    "\n",
    "def load_salary_cap_csv(path: str | Path, *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the preprocessed salary cap CSV (inflated) instead of scraping.\n",
    "    We DO NOT fill or coerce silently – if a required column is missing,\n",
    "    we log it and let the caller decide.\n",
    "    \"\"\"\n",
    "    path = Path(path).expanduser().resolve()\n",
    "    if debug:\n",
    "        print(f\"[salary-cap] loading local file: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    if debug:\n",
    "        print(f\"[salary-cap] rows={len(df)}, cols={df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Remove unnamed columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Remove duplicate columns\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # Remove columns with all NaN values **except** ones we want to keep\n",
    "    all_na = df.columns[df.isna().all()]\n",
    "    to_drop = [c for c in all_na if c not in PRESERVE_EVEN_IF_ALL_NA]\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "    # Remove rows with all NaN values\n",
    "    df = df.dropna(axis=0, how='all')\n",
    "\n",
    "    # Ensure only one 'Season' column exists\n",
    "    season_columns = [col for col in df.columns if 'Season' in col]\n",
    "    if len(season_columns) > 1:\n",
    "        df = df.rename(columns={season_columns[0]: 'Season'})\n",
    "        for col in season_columns[1:]:\n",
    "            df = df.drop(columns=[col])\n",
    "\n",
    "    # Remove '3PAr' and 'FTr' columns\n",
    "    columns_to_remove = ['3PAr', 'FTr']\n",
    "    df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "    # Round numeric columns to 2 decimal places\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_salary_cap_data(player_data: pd.DataFrame,\n",
    "                          salary_cap_data: pd.DataFrame,\n",
    "                          *,\n",
    "                          debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-merge cap data by season-year. Preserve all cap columns even if all NaN.\n",
    "    \"\"\"\n",
    "    if player_data.empty or salary_cap_data.empty:\n",
    "        if debug:\n",
    "            print(\"[merge_salary_cap_data] one side empty -> returning player_data unchanged\")\n",
    "        return player_data\n",
    "\n",
    "    # Make sure we don't mutate originals\n",
    "    p = player_data.copy()\n",
    "    cap = salary_cap_data.copy()\n",
    "\n",
    "    # Extract year\n",
    "    p[\"Season_Year\"]   = p[\"Season\"].str[:4].astype(int)\n",
    "    cap[\"Season_Year\"] = cap[\"Season\"].str[:4].astype(int)\n",
    "\n",
    "    # Inflate cap if not present\n",
    "    if \"Salary_Cap_Inflated\" not in cap.columns:\n",
    "        if debug:\n",
    "            print(\"[merge_salary_cap_data] computing Salary_Cap_Inflated\")\n",
    "        cap[\"Salary_Cap_Inflated\"] = cap.apply(\n",
    "            lambda r: inflate_value(r.get(\"Salary Cap\", np.nan), r.get(\"Season\", \"\")),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Merge\n",
    "    merged = pd.merge(p, cap, on=\"Season_Year\", how=\"left\", suffixes=(\"\", \"_cap\"))\n",
    "\n",
    "    # Figure out which columns came from cap\n",
    "    cap_cols = [c for c in cap.columns if c not in {\"Season_Year\"}]\n",
    "\n",
    "    # For each cap col, if we created a *_cap twin, consolidate\n",
    "    for col in cap_cols:\n",
    "        src = f\"{col}_cap\"\n",
    "        if src in merged.columns:\n",
    "            merged[col] = merged[col].where(~merged[col].isna(), merged[src])\n",
    "            merged.drop(columns=[src], inplace=True)\n",
    "\n",
    "    # Cleanup\n",
    "    merged.drop(columns=[\"Season_Year\"], inplace=True)\n",
    "\n",
    "    # Protect salary-cap columns from being dropped in clean_dataframe\n",
    "    global PRESERVE_EVEN_IF_ALL_NA\n",
    "    PRESERVE_EVEN_IF_ALL_NA = PRESERVE_EVEN_IF_ALL_NA.union(set(cap_cols))\n",
    "\n",
    "    merged = clean_dataframe(merged)\n",
    "\n",
    "    if debug:\n",
    "        miss = [c for c in cap_cols if c not in merged.columns]\n",
    "        if miss:\n",
    "            print(f\"[merge_salary_cap_data] WARNING missing cap cols after merge: {miss}\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "def load_external_salary_data(season: str,\n",
    "                              root: Path | str = DATA_PROCESSED_DIR / \"salary_external\",\n",
    "                              *, debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read player‑salary data from various formats.\n",
    "    Expected paths (in order of preference):\n",
    "    1. {root}/season={YYYY-YY}/part.parquet\n",
    "    2. {root}/comprehensive_salary_data.csv (with Season column)\n",
    "    3. {root}/sample_salary_data.csv (with Season column)\n",
    "    \"\"\"\n",
    "    # Try parquet file first\n",
    "    parquet_path = Path(root) / f\"season={season}/part.parquet\"\n",
    "    if parquet_path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary‑ext] loading parquet {parquet_path}\")\n",
    "        return pd.read_parquet(parquet_path)\n",
    "    \n",
    "    # Try comprehensive CSV file\n",
    "    csv_path = Path(root) / \"comprehensive_salary_data.csv\"\n",
    "    if csv_path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary‑ext] loading comprehensive CSV {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if 'Season' in df.columns:\n",
    "            season_data = df[df['Season'] == season]\n",
    "            if not season_data.empty:\n",
    "                return season_data\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"[salary‑ext] no data for season {season} in comprehensive CSV\")\n",
    "    \n",
    "    # Try sample CSV file\n",
    "    sample_csv_path = Path(root) / \"sample_salary_data.csv\"\n",
    "    if sample_csv_path.exists():\n",
    "        if debug:\n",
    "            print(f\"[salary‑ext] loading sample CSV {sample_csv_path}\")\n",
    "        df = pd.read_csv(sample_csv_path)\n",
    "        if 'Season' in df.columns:\n",
    "            season_data = df[df['Season'] == season]\n",
    "            if not season_data.empty:\n",
    "                return season_data\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"[salary‑ext] no data for season {season} in sample CSV\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[salary‑ext] no salary file found for season {season}\")\n",
    "    return pd.DataFrame(columns=[\"Player\", \"Salary\", \"Season\"])\n",
    "\n",
    "def validate_data(df: pd.DataFrame,\n",
    "                  *,\n",
    "                  name: str = \"player_dataset\",\n",
    "                  save_reports: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Same validation, but salary columns are now OPTIONAL.\n",
    "    \"\"\"\n",
    "    schema = ExpectedSchema(\n",
    "        expected_cols=df.columns,\n",
    "        required_cols=[\"Season\", \"Player\", \"Team\"],   # ‼ Salary removed\n",
    "        dtypes={\n",
    "            \"Season\": \"object\",\n",
    "            \"Player\": \"object\",\n",
    "        },\n",
    "        # Salary & Team_Salary dropped from non‑neg / non‑constant\n",
    "        non_negative_cols=[\"GP\", \"MP\", \"PTS\", \"TRB\", \"AST\"],\n",
    "        non_constant_cols=[\"PTS\"],\n",
    "        unique_key=[\"Season\", \"Player\"]\n",
    "    )\n",
    "\n",
    "    reports = audit_dataframe(df, schema, name=name)\n",
    "\n",
    "    if save_reports:\n",
    "        out_dir = DATA_PROCESSED_DIR / \"audits\"\n",
    "        write_audit_reports(reports, out_dir, prefix=name)\n",
    "\n",
    "    # Print a one-liner summary (optional)\n",
    "    missing_req = reports[\"cols_overview\"].query(\"missing_required == True\")\n",
    "    if not missing_req.empty:\n",
    "        print(f\"[validate_data] Missing required columns: {missing_req['column'].tolist()}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/quality.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/quality.py\n",
    "# src/salary_nba_data_pull/quality.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Mapping, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class ExpectedSchema:\n",
    "    \"\"\"Describe what we *intended* to have in a dataframe.\"\"\"\n",
    "    # All columns we care about (order doesn't matter)\n",
    "    expected_cols: Iterable[str]\n",
    "\n",
    "    # Subset that must be present\n",
    "    required_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Expected pandas dtypes (string form, e.g. 'float64', 'object')\n",
    "    dtypes: Mapping[str, str] = field(default_factory=dict)\n",
    "\n",
    "    # Columns that must be >= 0\n",
    "    non_negative_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Columns that should not be all zeros / all NaN\n",
    "    non_constant_cols: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Unique key columns (together must be unique)\n",
    "    unique_key: Iterable[str] = field(default_factory=list)\n",
    "\n",
    "    # Allowed value sets (enums)\n",
    "    allowed_values: Mapping[str, Iterable[Any]] = field(default_factory=dict)\n",
    "\n",
    "def _series_is_constant(s: pd.Series) -> bool:\n",
    "    return s.nunique(dropna=True) <= 1\n",
    "\n",
    "def audit_dataframe(df: pd.DataFrame,\n",
    "                    schema: ExpectedSchema,\n",
    "                    *,\n",
    "                    name: str = \"dataset\") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Return a dict of small DataFrames summarising quality checks.\n",
    "    Nothing is printed; caller decides how to persist/log.\n",
    "    \"\"\"\n",
    "    exp = set(schema.expected_cols)\n",
    "    req = set(schema.required_cols)\n",
    "\n",
    "    present = set(df.columns)\n",
    "    missing = sorted(list(exp - present))\n",
    "    extra   = sorted(list(present - exp))\n",
    "\n",
    "    # --- Column overview\n",
    "    cols_overview = pd.DataFrame({\n",
    "        \"column\": sorted(list(exp | present)),\n",
    "        \"expected\": [c in exp for c in sorted(list(exp | present))],\n",
    "        \"present\":  [c in present for c in sorted(list(exp | present))],\n",
    "        \"required\": [c in req for c in sorted(list(exp | present))]\n",
    "    })\n",
    "    cols_overview[\"missing_required\"] = cols_overview.apply(\n",
    "        lambda r: r[\"required\"] and not r[\"present\"], axis=1\n",
    "    )\n",
    "\n",
    "    # --- Null report\n",
    "    null_report = (df.isna().sum().to_frame(\"null_count\")\n",
    "                     .assign(total_rows=len(df))\n",
    "                     .assign(null_pct=lambda d: 100 * d[\"null_count\"] / d[\"total_rows\"])\n",
    "                     .reset_index()\n",
    "                     .rename(columns={\"index\": \"column\"}))\n",
    "\n",
    "    # --- Dtype report\n",
    "    type_rows = []\n",
    "    for col in df.columns:\n",
    "        exp_type = schema.dtypes.get(col)\n",
    "        type_rows.append({\n",
    "            \"column\": col,\n",
    "            \"expected_dtype\": exp_type,\n",
    "            \"actual_dtype\": str(df[col].dtype),\n",
    "            \"matches\": (exp_type is None) or (str(df[col].dtype) == exp_type)\n",
    "        })\n",
    "    type_report = pd.DataFrame(type_rows)\n",
    "\n",
    "    # --- Value checks\n",
    "    value_rows = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        series = df[col]\n",
    "        row = {\n",
    "            \"column\": col,\n",
    "            \"min\": series.min(skipna=True),\n",
    "            \"max\": series.max(skipna=True),\n",
    "            \"negatives\": int((series < 0).sum()),\n",
    "            \"zeros\": int((series == 0).sum()),\n",
    "            \"non_zero_pct\": 100 * (series != 0).sum() / len(series),\n",
    "        }\n",
    "        row[\"should_be_non_negative\"] = col in schema.non_negative_cols\n",
    "        row[\"violates_non_negative\"] = row[\"negatives\"] > 0 and row[\"should_be_non_negative\"]\n",
    "        value_rows.append(row)\n",
    "    value_report = pd.DataFrame(value_rows)\n",
    "\n",
    "    # Constant columns\n",
    "    constant_rows = []\n",
    "    for col in df.columns:\n",
    "        constant_rows.append({\n",
    "            \"column\": col,\n",
    "            \"is_constant\": _series_is_constant(df[col]),\n",
    "            \"should_not_be_constant\": col in schema.non_constant_cols\n",
    "        })\n",
    "    constant_report = pd.DataFrame(constant_rows).assign(\n",
    "        violates=lambda d: d[\"is_constant\"] & d[\"should_not_be_constant\"]\n",
    "    )\n",
    "\n",
    "    # Allowed values\n",
    "    enum_rows = []\n",
    "    for col, allowed in schema.allowed_values.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        bad = ~df[col].isin(allowed) & df[col].notna()\n",
    "        enum_rows.append({\n",
    "            \"column\": col,\n",
    "            \"bad_count\": int(bad.sum()),\n",
    "            \"sample_bad\": df.loc[bad, col].drop_duplicates().head(5).tolist()\n",
    "        })\n",
    "    enum_report = pd.DataFrame(enum_rows)\n",
    "\n",
    "    # Unique key\n",
    "    uniq_report = pd.DataFrame()\n",
    "    if schema.unique_key:\n",
    "        dup_mask = df.duplicated(subset=list(schema.unique_key), keep=False)\n",
    "        uniq_report = pd.DataFrame({\n",
    "            \"duplicate_rows\": [int(dup_mask.sum())],\n",
    "            \"subset\": [list(schema.unique_key)]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"cols_overview\": cols_overview,\n",
    "        \"null_report\": null_report,\n",
    "        \"type_report\": type_report,\n",
    "        \"value_report\": value_report,\n",
    "        \"constant_report\": constant_report,\n",
    "        \"enum_report\": enum_report,\n",
    "        \"unique_report\": uniq_report\n",
    "    }\n",
    "\n",
    "def assert_dataframe_ok(df: pd.DataFrame,\n",
    "                        schema: ExpectedSchema,\n",
    "                        *, name: str = \"dataset\") -> None:\n",
    "    \"\"\"\n",
    "    Raise AssertionError with a concise message if critical checks fail.\n",
    "    Designed for pytest or CI.\n",
    "    \"\"\"\n",
    "    rep = audit_dataframe(df, schema, name=name)\n",
    "    bad_missing = rep[\"cols_overview\"].query(\"missing_required == True\")\n",
    "    bad_types = rep[\"type_report\"].query(\"matches == False\")\n",
    "    bad_nonneg = rep[\"value_report\"].query(\"violates_non_negative == True\")\n",
    "    bad_constant = rep[\"constant_report\"].query(\"violates == True\")\n",
    "    dupes = rep[\"unique_report\"][\"duplicate_rows\"].iloc[0] if not rep[\"unique_report\"].empty else 0\n",
    "\n",
    "    msgs = []\n",
    "    if not bad_missing.empty:\n",
    "        msgs.append(f\"Missing required cols: {bad_missing['column'].tolist()}\")\n",
    "    if not bad_types.empty:\n",
    "        msgs.append(f\"Dtype mismatches: {bad_types[['column','expected_dtype','actual_dtype']].to_dict('records')}\")\n",
    "    if not bad_nonneg.empty:\n",
    "        msgs.append(f\"Negative values in non-negative cols: {bad_nonneg['column'].tolist()}\")\n",
    "    if not bad_constant.empty:\n",
    "        msgs.append(f\"Constant-but-shouldn't cols: {bad_constant['column'].tolist()}\")\n",
    "    if dupes:\n",
    "        msgs.append(f\"Duplicate key rows: {dupes}\")\n",
    "\n",
    "    if msgs:\n",
    "        raise AssertionError(f\"[{name}] data quality failures:\\n\" + \"\\n\".join(msgs))\n",
    "\n",
    "def write_audit_reports(reports: Mapping[str, pd.DataFrame],\n",
    "                        out_dir: Path,\n",
    "                        prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Save each report DataFrame as CSV for later inspection.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for key, df in reports.items():\n",
    "        df.to_csv(out_dir / f\"{prefix}_{key}.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/main.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import requests_cache\n",
    "from salary_nba_data_pull.fetch_utils import fetch_all_players, fetch_season_players, fetch_league_standings\n",
    "from salary_nba_data_pull.process_utils import (\n",
    "    process_player_data,\n",
    "    inflate_value,\n",
    "    calculate_percentages,\n",
    "    _ensure_cpi_ready,\n",
    "    add_usage_components,\n",
    ")\n",
    "    # Removed advanced metrics scraping imports to eliminate nulls\n",
    "from salary_nba_data_pull.data_utils import (\n",
    "    clean_dataframe,\n",
    "    validate_data,\n",
    ")\n",
    "from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "\n",
    "# Enable requests-cache for all HTTP traffic\n",
    "requests_cache.install_cache(\"nba_pull\", backend=\"sqlite\", allowable_codes=(200,))\n",
    "\n",
    "# CPI self-test - logs a warning once per run if CPI is unavailable\n",
    "_ensure_cpi_ready(debug=False)\n",
    "\n",
    "# Default number of worker threads\n",
    "DEFAULT_WORKERS = 8                # tweak ≤ CPU cores\n",
    "\n",
    "def _almost_equal_numeric(a: pd.Series, b: pd.Series, atol=1e-6, rtol=1e-9):\n",
    "    # Handle NA values first\n",
    "    mask = a.isna() & b.isna()\n",
    "    \n",
    "    # For non-NA values, compare them\n",
    "    both_numeric = pd.api.types.is_numeric_dtype(a) and pd.api.types.is_numeric_dtype(b)\n",
    "    if not both_numeric:\n",
    "        # For non-numeric columns, use pandas equals but handle NA carefully\n",
    "        non_na_mask = ~(a.isna() | b.isna())\n",
    "        eq_result = pd.Series(False, index=a.index)\n",
    "        if non_na_mask.any():\n",
    "            eq_result[non_na_mask] = a[non_na_mask].eq(b[non_na_mask])\n",
    "        return eq_result | mask\n",
    "    else:\n",
    "        # For numeric columns, use numpy isclose\n",
    "        non_na_mask = ~(a.isna() | b.isna())\n",
    "        diff_ok = pd.Series(False, index=a.index)\n",
    "        if non_na_mask.any():\n",
    "            diff_ok[non_na_mask] = np.isclose(\n",
    "                a[non_na_mask].astype(float), \n",
    "                b[non_na_mask].astype(float), \n",
    "                atol=atol, rtol=rtol\n",
    "            )\n",
    "        return diff_ok | mask\n",
    "\n",
    "# helper 1 ─ column drift\n",
    "def _columns_diff(old_df: pd.DataFrame, new_df: pd.DataFrame):\n",
    "    added   = sorted(set(new_df.columns) - set(old_df.columns))\n",
    "    removed = sorted(set(old_df.columns) - set(new_df.columns))\n",
    "    return added, removed\n",
    "\n",
    "# helper 2 ─ mean smoke‑test\n",
    "def _mean_diff(old_df: pd.DataFrame, new_df: pd.DataFrame,\n",
    "               tol_pct: float = 0.001) -> pd.DataFrame:\n",
    "    common = old_df.select_dtypes(\"number\").columns.intersection(\n",
    "             new_df.select_dtypes(\"number\").columns)\n",
    "    rows = []\n",
    "    for c in common:\n",
    "        o, n = old_df[c].mean(skipna=True), new_df[c].mean(skipna=True)\n",
    "        if pd.isna(o) or pd.isna(n):\n",
    "            continue\n",
    "        rel = abs(n - o) / (abs(o) + 1e-12)\n",
    "        if rel > tol_pct:\n",
    "            rows.append({\"column\": c, \"old_mean\": o, \"new_mean\": n, \"rel_diff\": rel})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _diff_report(old_df, new_df, key_cols=(\"Season\",\"Player\"),\n",
    "                 numeric_atol=1e-6, numeric_rtol=1e-9, max_print=10):\n",
    "    cols_add, cols_rem = _columns_diff(old_df, new_df)\n",
    "    mean_diffs = _mean_diff(old_df, new_df)\n",
    "\n",
    "    # value‑level diff (original logic)\n",
    "    common = [c for c in new_df.columns if c in old_df.columns]\n",
    "    old, new = old_df[common], new_df[common]\n",
    "\n",
    "    # Handle case where dataframes have different lengths\n",
    "    if len(old) != len(new):\n",
    "        # If lengths differ, we can't do row-by-row comparison\n",
    "        diffs = []\n",
    "    else:\n",
    "        if all(k in common for k in key_cols):\n",
    "            old = old.sort_values(list(key_cols)).reset_index(drop=True)\n",
    "            new = new.sort_values(list(key_cols)).reset_index(drop=True)\n",
    "        else:\n",
    "            key_cols = (\"__row__\",)\n",
    "            old[\"__row__\"] = new[\"__row__\"] = range(len(old))\n",
    "\n",
    "        diffs = []\n",
    "        for col in common:\n",
    "            equal = _almost_equal_numeric(old[col], new[col],\n",
    "                                          atol=numeric_atol, rtol=numeric_rtol)\n",
    "            for i in np.where(~equal)[0]:\n",
    "                if i < len(old) and i < len(new):  # Safety check\n",
    "                    row_keys = {k: new.iloc[i][k] for k in key_cols}\n",
    "                    diffs.append({**row_keys, \"column\": col,\n",
    "                                  \"old\": old.iloc[i][col], \"new\": new.iloc[i][col]})\n",
    "\n",
    "    is_equal = (not diffs) and (not cols_add) and (not cols_rem) and mean_diffs.empty\n",
    "    summary = (f\"cells:{len(diffs)}  col+:{len(cols_add)}  col-:{len(cols_rem)}  \"\n",
    "               f\"meanΔ:{len(mean_diffs)}\")\n",
    "    return is_equal, summary, pd.DataFrame(diffs), cols_add, cols_rem, mean_diffs\n",
    "\n",
    "def _file_md5(path: str, chunk: int = 1 << 20) -> str:\n",
    "    \"\"\"Return md5 hexdigest for *path* streaming in 1 MiB chunks.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for blk in iter(lambda: f.read(chunk), b\"\"):\n",
    "            h.update(blk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _season_partition_identical(season: str,\n",
    "                                base_dir: Path | str,\n",
    "                                new_df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if on-disk parquet for `season` is byte-wise equivalent (after\n",
    "    canonical sort & column alignment) to `new_df`.\n",
    "    \"\"\"\n",
    "    ckpt = Path(base_dir) / f\"season={season}\" / \"part.parquet\"\n",
    "    if not ckpt.exists():\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        old_df = pd.read_parquet(ckpt)\n",
    "    except Exception as exc:\n",
    "        logging.warning(\"[identical] failed to read %s → %s\", ckpt, exc)\n",
    "        return False\n",
    "\n",
    "    # STEP B1: align columns and sort only by stable key\n",
    "    cols = sorted(set(old_df.columns) | set(new_df.columns))\n",
    "    key = [\"Season\",\"Player\"]\n",
    "\n",
    "    old_cmp = (old_df.reindex(columns=cols)\n",
    "                     .sort_values(key)\n",
    "                     .reset_index(drop=True))\n",
    "    new_cmp = (new_df.reindex(columns=cols)\n",
    "                     .sort_values(key)\n",
    "                     .reset_index(drop=True))\n",
    "\n",
    "    return old_cmp.equals(new_cmp)   # NaNs treated equal if aligned\n",
    "\n",
    "def _season_partition_exists(season, base_dir):\n",
    "    \"\"\"Check if a season partition already exists in Parquet format.\"\"\"\n",
    "    return os.path.exists(os.path.join(base_dir, f\"season={season}\"))\n",
    "\n",
    "def _player_task(args):\n",
    "    \"\"\"Wrapper for ThreadPoolExecutor.\"\"\"\n",
    "    (player_name, season, salary, all_players, debug) = args\n",
    "    stats = process_player_data(player_name, season, all_players, debug=debug)\n",
    "    if stats:\n",
    "        stats['Salary'] = salary\n",
    "    return stats\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "def update_data(existing_data,\n",
    "                start_year: int,\n",
    "                end_year: int,\n",
    "                *,\n",
    "                player_filter: str = \"all\",\n",
    "                min_avg_minutes: float | None = None,    # NEW: filter on avg minutes\n",
    "                min_shot_attempts: int | None = None,    # NEW: filter on shot attempts\n",
    "                debug: bool = False,\n",
    "                small_debug: bool = False,\n",
    "                max_workers: int = 8,\n",
    "                output_base: str | Path = DATA_PROCESSED_DIR,\n",
    "                overwrite: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull seasons in [start_year, end_year], WITHOUT any salary or injury merges.\n",
    "    Ensures we only rely on nba_api rosters + career stats + W/L logs.\n",
    "    \n",
    "    NEW FILTERS:\n",
    "    - min_avg_minutes: Filter out players averaging < this many minutes per game\n",
    "    - min_shot_attempts: Filter out players with fewer than this many total shot attempts (FGA+FTA)\n",
    "    \n",
    "    These filters help eliminate nulls from low-volume players who don't have enough\n",
    "    data for meaningful percentage calculations.\n",
    "    \"\"\"\n",
    "    output_base = Path(output_base)\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "    helper_debug = debug and not small_debug\n",
    "\n",
    "    from salary_nba_data_pull.scrape_utils import _season_advanced_df\n",
    "    from salary_nba_data_pull.fetch_utils import (\n",
    "        fetch_season_players, fetch_league_standings, fetch_team_wl_by_season\n",
    "    )\n",
    "    from salary_nba_data_pull.process_utils import (\n",
    "        process_player_data, calculate_percentages, add_usage_components,\n",
    "        attach_wins_losses, merge_advanced_metrics,\n",
    "    )\n",
    "\n",
    "    out_frames: list[pd.DataFrame] = []\n",
    "    season_summaries: list[str] = []\n",
    "\n",
    "    for y in tqdm(range(start_year, end_year + 1),\n",
    "                  desc=\"Seasons\", disable=small_debug):\n",
    "        season = f\"{y}-{str(y+1)[-2:]}\"\n",
    "        ckpt_dir = output_base / f\"season={season}\"\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] Starting season {season}\")\n",
    "\n",
    "        # 1️⃣ Fetch the complete season roster\n",
    "        roster = fetch_season_players(season, debug=helper_debug)\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] fetched {len(roster)} players for {season}\")\n",
    "\n",
    "        # 2️⃣ Build args for each player (correct signature)\n",
    "        args = [\n",
    "            (name, season, roster, helper_debug)\n",
    "            for name in roster.keys()\n",
    "            if (player_filter == \"all\" or player_filter.lower() in name)\n",
    "        ]\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] processing {len(args)} players after filter\")\n",
    "\n",
    "        # 3️⃣ Process each player in parallel (correct signature)\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        results, failures = [], 0\n",
    "        with ThreadPoolExecutor(max_workers=min(max_workers, len(args) or 1)) as pool:\n",
    "            futures = {pool.submit(\n",
    "                lambda nm, ss, rp, dbg: process_player_data(nm, ss, rp, debug=dbg),\n",
    "                *arg\n",
    "            ): arg[0] for arg in args}\n",
    "\n",
    "            for fut in as_completed(futures):\n",
    "                pname = futures[fut]\n",
    "                try:\n",
    "                    res = fut.result()\n",
    "                    if res is None:\n",
    "                        if helper_debug:\n",
    "                            print(f\"[update_data][WARN] no data for player '{pname}' in {season}\")\n",
    "                    else:\n",
    "                        results.append(res)\n",
    "                except Exception as exc:\n",
    "                    failures += 1\n",
    "                    logging.exception(\"Player task failed for %s (%s): %s\", pname, season, exc)\n",
    "\n",
    "        if failures and debug:\n",
    "            print(f\"[update_data] ⚠️  {failures} player failures in {season}\")\n",
    "\n",
    "        df_season = pd.DataFrame(results)\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] {season} → DataFrame with {len(df_season)} rows\")\n",
    "\n",
    "        # 4️⃣ Attach W/L using unified lookup\n",
    "        df_season = df_season.pipe(\n",
    "            attach_wins_losses, season=season, debug=helper_debug\n",
    "        )\n",
    "\n",
    "        # 5️⃣ Derived metrics & clean\n",
    "        merged = (\n",
    "            df_season\n",
    "            .pipe(calculate_percentages, debug=helper_debug)\n",
    "            .pipe(add_usage_components, debug=helper_debug)\n",
    "            # .pipe(merge_advanced_metrics, season=season, debug=helper_debug)  # Temporarily disabled\n",
    "        )\n",
    "\n",
    "        # ── NEW: apply user‐specified filters ─────────────────────────\n",
    "        if min_avg_minutes is not None:\n",
    "            # MP is minutes per game\n",
    "            before = len(merged)\n",
    "            merged = merged.query(\"MP >= @min_avg_minutes\")\n",
    "            if helper_debug:\n",
    "                print(f\"[filter] {season}: MP ≥ {min_avg_minutes} → {before}→{len(merged)} rows\")\n",
    "        if min_shot_attempts is not None:\n",
    "            # here we assume shot attempts = FGA + FTA; adjust to FGA only if desired\n",
    "            before = len(merged)\n",
    "            merged = merged.assign(_shots=merged[\"FGA\"].fillna(0) + merged[\"FTA\"].fillna(0))\n",
    "            merged = merged.query(\"_shots >= @min_shot_attempts\").drop(columns=[\"_shots\"])\n",
    "            if helper_debug:\n",
    "                print(f\"[filter] {season}: shots ≥ {min_shot_attempts} → {before}→{len(merged)} rows\")\n",
    "        # ── end filters ──────────────────────────────────────────────────\n",
    "\n",
    "        # Key‐column sanity\n",
    "        dups = merged.duplicated(subset=[\"Season\",\"Player\"], keep=False)\n",
    "        if dups.any():\n",
    "            sample = merged.loc[dups, [\"Season\",\"Player\",\"Team\",\"MP\"]]\n",
    "            print(f\"[update_data][ERROR] Duplicate keys in {season}:\\n{sample}\")\n",
    "            raise AssertionError(f\"Duplicate (Season,Player) in {season}\")\n",
    "\n",
    "        # Trim whitespace-only strings → NA\n",
    "        obj_cols = merged.select_dtypes(include=[\"object\"]).columns\n",
    "        for c in obj_cols:\n",
    "            merged[c] = merged[c].replace(r\"^\\s*$\", pd.NA, regex=True)\n",
    "\n",
    "        # Persist per‐season partition\n",
    "        parquet_path = ckpt_dir / \"part.parquet\"\n",
    "        if not overwrite and parquet_path.exists():\n",
    "            from salary_nba_data_pull.main import _season_partition_identical\n",
    "            if _season_partition_identical(season, output_base, merged):\n",
    "                if helper_debug:\n",
    "                    print(f\"[update_data] {season} unchanged, skipping write\")\n",
    "                out_frames.append(merged)\n",
    "                continue\n",
    "        merged.to_parquet(parquet_path, index=False)\n",
    "        if helper_debug:\n",
    "            print(f\"[update_data] wrote {parquet_path}\")\n",
    "\n",
    "        out_frames.append(merged)\n",
    "        season_summaries.append(f\"{season}: {len(merged)} rows\")\n",
    "\n",
    "    if small_debug:\n",
    "        print(\"\\n--- Seasons Summaries ---\")\n",
    "        print(\"\\n\".join(season_summaries))\n",
    "        print(\"-------------------------\\n\")\n",
    "\n",
    "    return pd.concat(out_frames, ignore_index=True) if out_frames else pd.DataFrame()\n",
    "\n",
    "def get_timestamp():\n",
    "    \"\"\"Return a filesystem-safe timestamp string.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "def remove_old_logs(log_dir, days_to_keep=7):\n",
    "    current_time = datetime.now()\n",
    "    for log_file in glob.glob(os.path.join(log_dir, 'stat_pull_log_*.txt')):\n",
    "        file_modified_time = datetime.fromtimestamp(os.path.getmtime(log_file))\n",
    "        if current_time - file_modified_time > timedelta(days=days_to_keep):\n",
    "            os.remove(log_file)\n",
    "\n",
    "def persist_final_dataset(new_data: pd.DataFrame, seasons_loaded: list[str],\n",
    "                          *, output_base: Path, debug: bool = False,\n",
    "                          numeric_atol: float = 1e-6, numeric_rtol: float = 1e-9,\n",
    "                          max_print: int = 15, mean_tol_pct: float = 0.001) -> None:\n",
    "    from salary_nba_data_pull.data_utils import prune_end_columns\n",
    "\n",
    "    final_parquet = output_base / \"nba_player_data_final_inflated.parquet\"\n",
    "    join_keys = [\"Season\", \"Player\"]\n",
    "\n",
    "    # -- NEW: prune end-only columns BEFORE diffing/writing\n",
    "    new_data = prune_end_columns(new_data, debug=debug)\n",
    "\n",
    "    old_master = (pd.read_parquet(final_parquet)\n",
    "                  if final_parquet.exists() else\n",
    "                  pd.DataFrame(columns=new_data.columns))\n",
    "\n",
    "    # -- NEW: also prune any legacy columns in the old master for a fair diff\n",
    "    if not old_master.empty:\n",
    "        old_master = prune_end_columns(old_master, debug=debug)\n",
    "\n",
    "    for df in (old_master, new_data):\n",
    "        for k in join_keys:\n",
    "            if k in df.columns:\n",
    "                df[k] = df[k].astype(str).str.strip()\n",
    "\n",
    "    old_slice = old_master.merge(\n",
    "        pd.DataFrame({\"Season\": seasons_loaded}).drop_duplicates(),\n",
    "        on=\"Season\", how=\"inner\").reset_index(drop=True)\n",
    "    new_slice = new_data.reset_index(drop=True)\n",
    "\n",
    "    equal, summary, diff_cells, cols_add, cols_rem, mean_diffs = \\\n",
    "        _diff_report(old_slice, new_slice, key_cols=join_keys,\n",
    "                     numeric_atol=numeric_atol, numeric_rtol=numeric_rtol)\n",
    "\n",
    "    # Special case: if old_slice is empty but new_slice has data, we should write\n",
    "    if len(old_slice) == 0 and len(new_slice) > 0:\n",
    "        equal = False\n",
    "        if debug:\n",
    "            print(\"[persist] Creating new master parquet with fresh data\")\n",
    "\n",
    "    if equal:\n",
    "        if debug:\n",
    "            print(\"[persist] No changes detected – master Parquet left untouched\")\n",
    "        return\n",
    "\n",
    "    audits = output_base / \"audits\"; audits.mkdir(parents=True, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    if cols_add or cols_rem:\n",
    "        pd.DataFrame({\"added\": [cols_add], \"removed\": [cols_rem]}\n",
    "                     ).to_csv(audits / f\"column_changes_{ts}.csv\", index=False)\n",
    "    if not mean_diffs.empty:\n",
    "        mean_diffs.to_csv(audits / f\"mean_diffs_{ts}.csv\", index=False)\n",
    "    if not diff_cells.empty:\n",
    "        diff_cells.to_csv(audits / f\"value_diffs_{ts}.csv\", index=False)\n",
    "\n",
    "    # ----- rewrite master -----\n",
    "    union_cols = sorted(set(old_master.columns) | set(new_data.columns))\n",
    "    remover = old_master.merge(\n",
    "        pd.DataFrame({\"Season\": seasons_loaded}), on=\"Season\",\n",
    "        how=\"left\", indicator=True)\n",
    "    remover = remover[remover[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
    "    remover = remover.reindex(columns=union_cols)\n",
    "    new_slice = new_slice.reindex(columns=union_cols)\n",
    "\n",
    "    updated_master = pd.concat([remover, new_slice], ignore_index=True)\\\n",
    "                       .sort_values(join_keys).reset_index(drop=True)\n",
    "    updated_master.to_parquet(final_parquet, index=False)\n",
    "    if debug: print(f\"[persist] Master Parquet updated – {summary}\")\n",
    "\n",
    "def main(start_year: int,\n",
    "         end_year: int,\n",
    "         player_filter: str = \"all\",\n",
    "         min_avg_minutes: float = 10,    # NEW default: 10 minutes\n",
    "         min_shot_attempts: int = 50,    # NEW: filter on shot attempts\n",
    "         debug: bool = False,\n",
    "         small_debug: bool = False,      # --- NEW\n",
    "         workers: int = 8,\n",
    "         overwrite: bool = False,\n",
    "         output_base: str | Path = DATA_PROCESSED_DIR) -> None:\n",
    "    \"\"\"\n",
    "    Entry point. `small_debug=True` prints only high‑signal info.\n",
    "    If both `debug` and `small_debug` are True, `debug` wins (full noise).\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    output_base = Path(output_base)\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_dir = output_base.parent / \"stat_pull_output\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remove_old_logs(log_dir)\n",
    "\n",
    "    log_file = log_dir / f\"stat_pull_log_{get_timestamp()}.txt\"\n",
    "    logging.basicConfig(filename=log_file,\n",
    "                        level=logging.DEBUG if debug else logging.INFO,\n",
    "                        format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    updated = update_data(None, start_year, end_year,\n",
    "                          player_filter=player_filter,\n",
    "                          min_avg_minutes=min_avg_minutes,\n",
    "                          min_shot_attempts=min_shot_attempts,  # NEW: pass shot attempts filter\n",
    "                          debug=debug,\n",
    "                          small_debug=small_debug,          # --- NEW\n",
    "                          max_workers=workers,\n",
    "                          output_base=str(output_base),\n",
    "                          overwrite=overwrite)\n",
    "\n",
    "    if not small_debug:  # keep your old prints in full/quiet modes\n",
    "        print(f\"✔ Completed pull: {len(updated):,} rows added\")\n",
    "\n",
    "    if not updated.empty:\n",
    "        # — Skip salary‐cap entirely —\n",
    "        # Validate only core columns (Season,Player,Team)\n",
    "        from salary_nba_data_pull.data_utils import validate_data\n",
    "        updated = validate_data(updated, name=\"player_dataset\", save_reports=True)\n",
    "\n",
    "        # Persist master\n",
    "        seasons_this_run = sorted(updated[\"Season\"].unique().tolist())\n",
    "        persist_final_dataset(\n",
    "            updated,\n",
    "            seasons_loaded=seasons_this_run,\n",
    "            output_base=output_base,\n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "    if not small_debug:\n",
    "        print(f\"Process finished in {time.time() - t0:.1f} s — log: {log_file}\")\n",
    "    else:\n",
    "        # minimal closing line\n",
    "        print(f\"Done in {time.time() - t0:.1f}s. Log: {log_file}\")\n",
    "        \n",
    "# ----------------------------------------------------------------------\n",
    "# argparse snippet\n",
    "if __name__ == \"__main__\":\n",
    "    cur = datetime.now().year\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--start_year\", type=int, default=cur-1)\n",
    "    p.add_argument(\"--end_year\",   type=int, default=cur)\n",
    "    p.add_argument(\"--player_filter\", default=\"all\")\n",
    "    p.add_argument(\"--min_avg_minutes\", type=float, default=10,\n",
    "                   help=\"Filter out players averaging < this many minutes per game\")\n",
    "    p.add_argument(\"--min_shot_attempts\", type=int, default=50,\n",
    "                   help=\"Filter out players with fewer than this many total shot attempts (FGA+FTA)\")\n",
    "    p.add_argument(\"--debug\", action=\"store_true\")\n",
    "    p.add_argument(\"--small_debug\", action=\"store_true\")   # --- NEW\n",
    "    p.add_argument(\"--workers\", type=int, default=8)\n",
    "    p.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "    p.add_argument(\"--output_base\",\n",
    "                   default=str(DATA_PROCESSED_DIR),\n",
    "                   help=\"Destination root for parquet + csv outputs\")\n",
    "    args = p.parse_args()\n",
    "    main(**vars(args))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[notebook_helper] sys.path[0:3]=['C:\\\\docker_projects\\\\coach_analysis', 'C:\\\\Users\\\\ghadf\\\\AppData\\\\Roaming\\\\uv\\\\python\\\\cpython-3.10.17-windows-x86_64-none\\\\python310.zip', 'C:\\\\Users\\\\ghadf\\\\AppData\\\\Roaming\\\\uv\\\\python\\\\cpython-3.10.17-windows-x86_64-none\\\\DLLs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ salary_nba_data_pull imported successfully\n",
      "start_year      default=<class 'inspect._empty'>  kind=POSITIONAL_OR_KEYWORD\n",
      "end_year        default=<class 'inspect._empty'>  kind=POSITIONAL_OR_KEYWORD\n",
      "player_filter   default='all'  kind=POSITIONAL_OR_KEYWORD\n",
      "min_avg_minutes default=10  kind=POSITIONAL_OR_KEYWORD\n",
      "min_shot_attempts default=50  kind=POSITIONAL_OR_KEYWORD\n",
      "debug           default=False  kind=POSITIONAL_OR_KEYWORD\n",
      "small_debug     default=False  kind=POSITIONAL_OR_KEYWORD\n",
      "workers         default=8  kind=POSITIONAL_OR_KEYWORD\n",
      "overwrite       default=False  kind=POSITIONAL_OR_KEYWORD\n",
      "output_base     default=WindowsPath('C:/docker_projects/coach_analysis/data/new_processed')  kind=POSITIONAL_OR_KEYWORD\n",
      "[historical_pull] 2023-2024, kwargs={'workers': 6, 'min_avg_minutes': 10, 'min_shot_attempts': 50, 'overwrite': True, 'debug': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[update_data] Starting season 2023-24\n",
      "[fetch_season_players] 596 players for 2023-24\n",
      "[update_data] fetched 596 players for 2023-24\n",
      "[update_data] processing 596 players after filter\n",
      "[update_data][WARN] no data for player 'steven adams' in 2023-24\n",
      "[update_data][WARN] no data for player 'lonzo ball' in 2023-24\n",
      "[update_data][WARN] no data for player 'tony bradley' in 2023-24\n",
      "[update_data][WARN] no data for player 'josh christopher' in 2023-24\n",
      "[update_data][WARN] no data for player 'jaylen clark' in 2023-24\n",
      "[update_data][WARN] no data for player 'terence davis' in 2023-24\n",
      "[update_data][WARN] no data for player 'pj dozier' in 2023-24\n",
      "[update_data][WARN] no data for player 'marcus garrett' in 2023-24\n",
      "[update_data][WARN] no data for player 'jaylin galloway' in 2023-24\n",
      "[update_data][WARN] no data for player 'nigel hayes-davis' in 2023-24\n",
      "[update_data][WARN] no data for player 'malcolm hill' in 2023-24\n",
      "[update_data][WARN] no data for player 'kai jones' in 2023-24\n",
      "[update_data][WARN] no data for player 'christian koloko' in 2023-24\n",
      "[update_data][WARN] no data for player 'damion lee' in 2023-24\n",
      "[update_data][WARN] no data for player 'skal labissiere' in 2023-24\n",
      "[update_data][WARN] no data for player 'jaylen martin' in 2023-24\n",
      "[update_data][WARN] no data for player 'tyrese martin' in 2023-24\n",
      "[update_data][WARN] no data for player 'mac mcclung' in 2023-24\n",
      "[update_data][WARN] no data for player 'jahlil okafor' in 2023-24\n",
      "[update_data][WARN] no data for player 'elfrid payton' in 2023-24\n",
      "[update_data][WARN] no data for player 'kevin porter jr.' in 2023-24\n",
      "[update_data][WARN] no data for player 'duane washington jr.' in 2023-24\n",
      "[update_data][WARN] no data for player 'guerschon yabusele' in 2023-24\n",
      "[update_data][WARN] no data for player 'vlatko cancar' in 2023-24\n",
      "[update_data] 2023-24 → DataFrame with 572 rows\n",
      "[attach_wins_losses] 2023-24 W/L null% = 0.00\n",
      "[update_data] 2023-24 before derived metrics: 31 columns\n",
      "[update_data] 2023-24 columns: ['Player', 'Season', 'Team', 'Age', 'GP', 'GS', 'MP', 'PTS', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'ORB', 'DRB', 'TeamID', 'Position', 'Years_of_Service', '2P', '2PA', 'eFG%', '2P%', 'Wins', 'Losses']\n",
      "[calculate_percentages] 3PA==0 count: 34\n",
      "[calculate_percentages] FTA==0 count: 36\n",
      "Percentage calculations completed with zero-denominator handling\n",
      "[adv] fetching https://www.basketball-reference.com/leagues/NBA_2024_advanced.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seasons:   0%|          | 0/2 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[merge_advanced_metrics] 2023-24: advanced attached, misses=34\n",
      "[merge_advanced_metrics] 2023-24: added columns: ['PER', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
      "[merge_advanced_metrics] 2023-24: coverage = 94.1%\n",
      "[update_data] 2023-24 after derived metrics: 79 columns\n",
      "[update_data] 2023-24 advanced columns found: ['PER', 'BPM', 'VORP', 'WS', 'DWS', 'OWS', 'WS/48', 'AST%', 'BLK%', 'TOV%', 'TRB%', 'DRB%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UndefinedVariableError",
     "evalue": "name 'MP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\scope.py:231\u001b[0m, in \u001b[0;36mScope.resolve\u001b[1;34m(self, key, is_local)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_resolvers:\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolvers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# if we're here that means that we have no locals and we also have\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# no resolvers\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.17-windows-x86_64-none\\lib\\collections\\__init__.py:986\u001b[0m, in \u001b[0;36mChainMap.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__missing__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.10.17-windows-x86_64-none\\lib\\collections\\__init__.py:978\u001b[0m, in \u001b[0;36mChainMap.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__missing__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'MP'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\scope.py:242\u001b[0m, in \u001b[0;36mScope.resolve\u001b[1;34m(self, key, is_local)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# last ditch effort we look in temporaries\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# these are created when parsing indexing expressions\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# e.g., df[df > 0]\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'MP'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mUndefinedVariableError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 163\u001b[0m\n\u001b[0;32m    158\u001b[0m print_args()\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# quick_pull(2023, workers=4, debug=True)\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m \u001b[43mhistorical_pull\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# multi‑season, 2012, 2024,\u001b[39;49;00m\n\u001b[0;32m    164\u001b[0m \u001b[43m                \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmin_avg_minutes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmin_shot_attempts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m check_existing_data()              \u001b[38;5;66;03m# see which seasons are cached\u001b[39;00m\n\u001b[0;32m    170\u001b[0m df \u001b[38;5;241m=\u001b[39m load_parquet_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-24\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# inspect a single season\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 70\u001b[0m, in \u001b[0;36mhistorical_pull\u001b[1;34m(start_year, end_year, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m _reload()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[historical_pull] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, kwargs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m nba_main\u001b[38;5;241m.\u001b[39mmain(start_year\u001b[38;5;241m=\u001b[39mstart_year, end_year\u001b[38;5;241m=\u001b[39mend_year, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\docker_projects\\coach_analysis\\src\\salary_nba_data_pull\\main.py:445\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(start_year, end_year, player_filter, min_avg_minutes, min_shot_attempts, debug, small_debug, workers, overwrite, output_base)\u001b[0m\n\u001b[0;32m    440\u001b[0m log_file \u001b[38;5;241m=\u001b[39m log_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstat_pull_log_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_timestamp()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(filename\u001b[38;5;241m=\u001b[39mlog_file,\n\u001b[0;32m    442\u001b[0m                     level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG \u001b[38;5;28;01mif\u001b[39;00m debug \u001b[38;5;28;01melse\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m    443\u001b[0m                     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 445\u001b[0m updated \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mplayer_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplayer_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmin_avg_minutes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_avg_minutes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmin_shot_attempts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_shot_attempts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# NEW: pass shot attempts filter\u001b[39;49;00m\n\u001b[0;32m    449\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msmall_debug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmall_debug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# --- NEW\u001b[39;49;00m\n\u001b[0;32m    451\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m                      \u001b[49m\u001b[43moutput_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_base\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m                      \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m small_debug:  \u001b[38;5;66;03m# keep your old prints in full/quiet modes\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✔ Completed pull: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(updated)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows added\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\docker_projects\\coach_analysis\\src\\salary_nba_data_pull\\main.py:290\u001b[0m, in \u001b[0;36mupdate_data\u001b[1;34m(existing_data, start_year, end_year, player_filter, min_avg_minutes, min_shot_attempts, debug, small_debug, max_workers, output_base, overwrite)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_avg_minutes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# MP is minutes per game\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(merged)\n\u001b[1;32m--> 290\u001b[0m     merged \u001b[38;5;241m=\u001b[39m \u001b[43mmerged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMP >= @min_avg_minutes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m helper_debug:\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[filter] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: MP ≥ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_avg_minutes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbefore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m→\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(merged)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4828\u001b[0m, in \u001b[0;36mDataFrame.query\u001b[1;34m(self, expr, inplace, **kwargs)\u001b[0m\n\u001b[0;32m   4826\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4827\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4828\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(expr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4830\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   4831\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[res]\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.eval\u001b[1;34m(self, expr, inplace, **kwargs)\u001b[0m\n\u001b[0;32m   4951\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   4952\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolvers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolvers\u001b[39m\u001b[38;5;124m\"\u001b[39m, ())) \u001b[38;5;241m+\u001b[39m resolvers\n\u001b[1;32m-> 4954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _eval(expr, inplace\u001b[38;5;241m=\u001b[39minplace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\eval.py:339\u001b[0m, in \u001b[0;36meval\u001b[1;34m(expr, parser, engine, local_dict, global_dict, resolvers, level, target, inplace)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# get our (possibly passed-in) scope\u001b[39;00m\n\u001b[0;32m    331\u001b[0m env \u001b[38;5;241m=\u001b[39m ensure_scope(\n\u001b[0;32m    332\u001b[0m     level \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    333\u001b[0m     global_dict\u001b[38;5;241m=\u001b[39mglobal_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[0;32m    337\u001b[0m )\n\u001b[1;32m--> 339\u001b[0m parsed_expr \u001b[38;5;241m=\u001b[39m \u001b[43mExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumexpr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    342\u001b[0m     (\n\u001b[0;32m    343\u001b[0m         is_extension_array_dtype(parsed_expr\u001b[38;5;241m.\u001b[39mterms\u001b[38;5;241m.\u001b[39mreturn_type)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m     )\n\u001b[0;32m    351\u001b[0m ):\n\u001b[0;32m    352\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine has switched to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because numexpr does not support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextension array dtypes. Please set your engine to python manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[0;32m    356\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    357\u001b[0m     )\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:809\u001b[0m, in \u001b[0;36mExpr.__init__\u001b[1;34m(self, expr, engine, parser, env, level)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m parser\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_visitor \u001b[38;5;241m=\u001b[39m PARSERS[parser](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser)\n\u001b[1;32m--> 809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:828\u001b[0m, in \u001b[0;36mExpr.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    825\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;124;03m    Parse an expression.\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_visitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:413\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(node)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method)\n\u001b[1;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:419\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit_Module\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly a single expression is allowed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    418\u001b[0m expr \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mbody[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(expr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:413\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(node)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method)\n\u001b[1;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:422\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit_Expr\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisit_Expr\u001b[39m(\u001b[38;5;28mself\u001b[39m, node, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(node\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:413\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(node)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method)\n\u001b[1;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:719\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit_Compare\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_In(ops[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    718\u001b[0m     binop \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mBinOp(op\u001b[38;5;241m=\u001b[39mop, left\u001b[38;5;241m=\u001b[39mnode\u001b[38;5;241m.\u001b[39mleft, right\u001b[38;5;241m=\u001b[39mcomps[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;66;03m# recursive case: we have a chained comparison, a CMP b CMP c, etc.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m left \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mleft\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:413\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(node)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method)\n\u001b[1;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:535\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit_BinOp\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisit_BinOp\u001b[39m(\u001b[38;5;28mself\u001b[39m, node, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 535\u001b[0m     op, op_class, left, right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_transform_eq_ne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m     left, right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_downcast_constants(left, right)\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_evaluate_binop(op, op_class, left, right)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:453\u001b[0m, in \u001b[0;36mBaseExprVisitor._maybe_transform_eq_ne\u001b[1;34m(self, node, left, right)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_transform_eq_ne\u001b[39m(\u001b[38;5;28mself\u001b[39m, node, left\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, right\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m         left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m         right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisit(node\u001b[38;5;241m.\u001b[39mright, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:413\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(node)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method)\n\u001b[1;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m visitor(node, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expr.py:545\u001b[0m, in \u001b[0;36mBaseExprVisitor.visit_Name\u001b[1;34m(self, node, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisit_Name\u001b[39m(\u001b[38;5;28mself\u001b[39m, node, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Term:\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterm_type(node\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\ops.py:91\u001b[0m, in \u001b[0;36mTerm.__init__\u001b[1;34m(self, name, env, side, encoding)\u001b[0m\n\u001b[0;32m     89\u001b[0m tname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(name)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_local \u001b[38;5;241m=\u001b[39m tname\u001b[38;5;241m.\u001b[39mstartswith(LOCAL_TAG) \u001b[38;5;129;01mor\u001b[39;00m tname \u001b[38;5;129;01min\u001b[39;00m DEFAULT_GLOBALS\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m encoding\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\ops.py:115\u001b[0m, in \u001b[0;36mTerm._resolve_name\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mscope \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mscope[local_name], \u001b[38;5;28mtype\u001b[39m\n\u001b[0;32m    112\u001b[0m ):\n\u001b[0;32m    113\u001b[0m     is_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(res)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m res\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\docker_projects\\coach_analysis\\.venv\\lib\\site-packages\\pandas\\core\\computation\\scope.py:244\u001b[0m, in \u001b[0;36mScope.resolve\u001b[1;34m(self, key, is_local)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemps[key]\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UndefinedVariableError(key, is_local) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mUndefinedVariableError\u001b[0m: name 'MP' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%writefile ../src/salary_nba_data_pull/notebook_helper.py\n",
    "\"\"\"\n",
    "Notebook/REPL helper utilities for salary_nba_data_pull.\n",
    "\n",
    "Goals\n",
    "-----\n",
    "• Work no matter where the notebook is opened (absolute paths).\n",
    "• Avoid NameError on __file__.\n",
    "• Keep hot‑reload for iterative dev.\n",
    "• Forward arbitrary args to main() so we can test all scenarios.\n",
    "\n",
    "Use:\n",
    ">>> import salary_nba_data_pull.notebook_helper as nb\n",
    ">>> nb.quick_pull(2024, workers=12, debug=True)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import sys, importlib, inspect, os\n",
    "from pathlib import Path\n",
    "import requests_cache\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "def _find_repo_root(start: Path | None = None) -> Path:\n",
    "    \"\"\"Find the repository root by looking for pyproject.toml or .git.\"\"\"\n",
    "    markers = {\"pyproject.toml\", \".git\"}\n",
    "    here = (start or Path.cwd()).resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "# Ensure project root & src are on sys.path (defensive)\n",
    "ROOT = _find_repo_root()\n",
    "SRC  = ROOT / \"src\"\n",
    "for p in (ROOT, SRC):\n",
    "    if p.is_dir() and str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "\n",
    "# Sanity print (can be silenced)\n",
    "if __name__ == \"__main__\" or \"JPY_PARENT_PID\" in os.environ:\n",
    "    print(f\"[notebook_helper] sys.path[0:3]={sys.path[:3]}\")\n",
    "\n",
    "# Import after path fix\n",
    "try:\n",
    "    from salary_nba_data_pull import main as nba_main\n",
    "    from salary_nba_data_pull.settings import DATA_PROCESSED_DIR\n",
    "    from salary_nba_data_pull.fetch_utils import clear_cache as _cc\n",
    "    print(\"✅ salary_nba_data_pull imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import salary_nba_data_pull: {e}\")\n",
    "    print(f\"   ROOT={ROOT}\")\n",
    "    print(f\"   SRC={SRC}\")\n",
    "    print(f\"   sys.path[0:3]={sys.path[:3]}\")\n",
    "    raise\n",
    "    \n",
    "    \n",
    "def _reload():\n",
    "    \"\"\"Reload the main module so code edits are picked up.\"\"\"\n",
    "    importlib.reload(nba_main)\n",
    "\n",
    "def quick_pull(season: int, **kwargs):\n",
    "    _reload()\n",
    "    print(f\"[quick_pull] season={season}, kwargs={kwargs}\")\n",
    "    nba_main.main(start_year=season, end_year=season, **kwargs)\n",
    "\n",
    "def historical_pull(start_year: int, end_year: int, **kwargs):\n",
    "    _reload()\n",
    "    print(f\"[historical_pull] {start_year}-{end_year}, kwargs={kwargs}\")\n",
    "    nba_main.main(start_year=start_year, end_year=end_year, **kwargs)\n",
    "\n",
    "def check_existing_data(base: Path | str | None = None) -> list[str]:\n",
    "    base = Path(base) if base else DATA_PROCESSED_DIR\n",
    "    seasons = sorted(d.name.split(\"=\", 1)[-1] for d in base.glob(\"season=*\") if d.is_dir())\n",
    "    print(f\"[check_existing_data] found {len(seasons)} seasons in {base}\")\n",
    "    return seasons\n",
    "\n",
    "def load_parquet_data(season: str | None = None, *, base: Path | str | None = None):\n",
    "    import pandas as pd\n",
    "    base = Path(base) if base else DATA_PROCESSED_DIR\n",
    "    files = list(base.glob(f\"season={season}/part.parquet\")) if season else list(base.glob(\"season=*/part.parquet\"))\n",
    "    if not files:\n",
    "        print(\"[load_parquet_data] No parquet files found.\")\n",
    "        return pd.DataFrame()\n",
    "    print(f\"[load_parquet_data] loading {len(files)} files from {base}\")\n",
    "    return pd.concat((pd.read_parquet(f) for f in files), ignore_index=True)\n",
    "\n",
    "def clear_all_caches():\n",
    "    requests_cache.clear()\n",
    "    _cc()\n",
    "    print(\"✅ caches cleared\")\n",
    "\n",
    "def print_args():\n",
    "    sig = inspect.signature(nba_main.main)\n",
    "    for name, param in sig.parameters.items():\n",
    "        print(f\"{name:<15} default={param.default!r}  kind={param.kind}\")\n",
    "\n",
    "def query_data(sql: str, db: str | None = None):\n",
    "    \"\"\"\n",
    "    Run arbitrary SQL against the DuckDB lake. Example:\n",
    "        query_data(\"SELECT COUNT(*) FROM parquet_scan('data/new_processed/season=*/part.parquet')\")\n",
    "    \"\"\"\n",
    "    import duckdb, pandas as pd\n",
    "    db = db or (DATA_PROCESSED_DIR.parent / \"nba_stats.duckdb\")\n",
    "    with duckdb.connect(str(db), read_only=True) as con:\n",
    "        return con.execute(sql).fetchdf()\n",
    "\n",
    "\n",
    "# ── NEW VALIDATORS ──────────────────────────────────────────────────────────\n",
    "\n",
    "def validate_season_coverage(df: pd.DataFrame,\n",
    "                             expected_seasons: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Check that df['Season'] covers exactly the expected seasons.\n",
    "    Prints missing and extra seasons.\n",
    "    \"\"\"\n",
    "    if \"Season\" not in df.columns:\n",
    "        print(\"[validate_season_coverage] ERROR: no 'Season' column\")\n",
    "        return\n",
    "\n",
    "    actual = sorted(df[\"Season\"].dropna().unique().tolist())\n",
    "    missing = [s for s in expected_seasons if s not in actual]\n",
    "    extra   = [s for s in actual if s not in expected_seasons]\n",
    "\n",
    "    print(f\"[validate_season_coverage] expected: {expected_seasons}\")\n",
    "    print(f\"[validate_season_coverage] actual:   {actual}\")\n",
    "    if missing:\n",
    "        print(f\"[validate_season_coverage] MISSING seasons: {missing}\")\n",
    "    if extra:\n",
    "        print(f\"[validate_season_coverage] EXTRA seasons:   {extra}\")\n",
    "    if not missing and not extra:\n",
    "        print(\"[validate_season_coverage] ✅ coverage OK\")\n",
    "\n",
    "def report_nulls(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise null counts & percentages for each column in df.\n",
    "    Returns a DataFrame with columns: column, null_count, total_rows, null_pct.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    stats = []\n",
    "    for col in df.columns:\n",
    "        nulls = int(df[col].isna().sum())\n",
    "        pct   = 100 * nulls / total if total else 0\n",
    "        stats.append({\n",
    "            \"column\": col,\n",
    "            \"null_count\": nulls,\n",
    "            \"total_rows\": total,\n",
    "            \"null_pct\": round(pct, 2)\n",
    "        })\n",
    "    report = pd.DataFrame(stats).sort_values(\"null_pct\", ascending=False)\n",
    "    print(\"[report_nulls]\")\n",
    "    print(report.to_string(index=False))\n",
    "    return report\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_args()\n",
    "    # quick_pull(2023, workers=4, debug=True)\n",
    "\n",
    "\n",
    "\n",
    "    historical_pull(2023, 2024,        # multi‑season, 2012, 2024,\n",
    "                    workers=6,\n",
    "                    min_avg_minutes=10,\n",
    "                    min_shot_attempts=50,\n",
    "                    overwrite=True,\n",
    "                    debug=True)\n",
    "    check_existing_data()              # see which seasons are cached\n",
    "    df = load_parquet_data(\"2023-24\")  # inspect a single season\n",
    "\n",
    "    # Suppose you want exactly that one season:\n",
    "    validate_season_coverage(df, [\"2023-24\"])\n",
    "    # Check nulls:\n",
    "    null_report = report_nulls(df)\n",
    "    # Examine the top 5 columns by null_pct\n",
    "    null_report.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Data Pipeline DAG Architecture\n",
    "\n",
    "## 🏗️ Architecture Overview\n",
    "\n",
    "This document details the **simplified DAG architecture** that focuses on core data sources while removing salary scraping complexity.\n",
    "\n",
    "## 📊 DAG Comparison\n",
    "\n",
    "| Aspect | Monolithic DAG | Split DAGs | Benefit |\n",
    "|--------|----------------|------------|---------|\n",
    "| **Failure Isolation** | One failure blocks all | Isolated failures | ✅ Higher reliability |\n",
    "| **Scheduling** | Single cadence for all | Source-specific cadences | ✅ Optimized resource usage |\n",
    "| **Maintenance** | All-or-nothing updates | Independent iteration | ✅ Faster development |\n",
    "| **Monitoring** | Single SLA for everything | Granular SLAs | ✅ Better observability |\n",
    "| **Parsing Speed** | Large file slows DagBag | Smaller files | ✅ Faster Airflow startup |\n",
    "\n",
    "## 🗓️ Current DAG Set\n",
    "\n",
    "| # | DAG file | Purpose | Schedule | SLA | Retries |\n",
    "|---|----------|---------|----------|-----|---------|\n",
    "| 1 | `nba_advanced_ingest.py` | Advanced metrics (Basketball‑Reference) | `@daily` | 1 h | 2 |\n",
    "| 2 | `injury_etl.py`          | Injury CSV processing | `@monthly` | 1 h | 1 |\n",
    "| 3 | `nba_data_loader.py`     | Load all sources into DuckDB | `@daily` | 3 h | 2 |\n",
    "\n",
    "> **Salary cap**: the yearly cap/parquet is committed by the build pipeline\n",
    "> and version‑controlled; no Airflow DAG is required.\n",
    "\n",
    "### Dependency graph\n",
    "\n",
    "```\n",
    "nba_advanced_ingest ┐\n",
    "injury_etl ├──► nba_data_loader\n",
    "```\n",
    "\n",
    "## 🗓️ DAG Scheduling Strategy\n",
    "\n",
    "### 1. `nba_advanced_ingest` - Daily\n",
    "**Rationale**: Advanced stats update daily\n",
    "- **Schedule**: `@daily`\n",
    "- **SLA**: 1 hour\n",
    "- **Retries**: 2 with 5-minute delays\n",
    "- **Sources**: Basketball-Reference\n",
    "\n",
    "### 2. `injury_etl` - Monthly\n",
    "**Rationale**: Injury data updates monthly\n",
    "- **Schedule**: `@monthly`\n",
    "- **SLA**: 1 hour\n",
    "- **Retries**: 1 with 5-minute delays\n",
    "- **Sources**: Local CSV files\n",
    "\n",
    "### 3. `nba_data_loader` - Daily\n",
    "**Rationale**: Loads all data into DuckDB daily\n",
    "- **Schedule**: `@daily`\n",
    "- **SLA**: 3 hours\n",
    "- **Dependencies**: Advanced metrics and injury ETL via ExternalTaskSensor\n",
    "\n",
    "## 🔗 Dependency Management\n",
    "\n",
    "### ExternalTaskSensor Configuration\n",
    "\n",
    "```python\n",
    "# Wait for advanced metrics\n",
    "wait_advanced = ExternalTaskSensor(\n",
    "    task_id=\"wait_advanced_ingest\",\n",
    "    external_dag_id=\"nba_advanced_ingest\",\n",
    "    external_task_id=\"scrape_advanced_metrics\",\n",
    "    timeout=3600,                     # 1 hour timeout\n",
    "    mode=\"reschedule\",\n",
    "    poke_interval=300,                # Check every 5 minutes\n",
    ")\n",
    "```\n",
    "\n",
    "### Timeout Strategy\n",
    "\n",
    "| DAG | Timeout | Rationale |\n",
    "|-----|---------|-----------|\n",
    "| Daily DAGs | 1 hour | Normal operation time |\n",
    "| Monthly DAGs | 2 hours | Allow for monthly task completion |\n",
    "\n",
    "## 📈 Performance Metrics\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "| Metric | Target | Measurement |\n",
    "|--------|--------|-------------|\n",
    "| **Ingest Success Rate** | >95% | Successful DAG runs / Total runs |\n",
    "| **Data Quality** | >99% | Valid rows / Total rows |\n",
    "| **SLA Compliance** | >90% | On-time completions / Total runs |\n",
    "\n",
    "### Monitoring Dashboard\n",
    "\n",
    "```sql\n",
    "-- DAG Performance Query\n",
    "SELECT \n",
    "    dag_id,\n",
    "    COUNT(*) as total_runs,\n",
    "    AVG(CASE WHEN state = 'success' THEN 1 ELSE 0 END) as success_rate,\n",
    "    AVG(duration) as avg_duration_minutes\n",
    "FROM airflow.task_instance \n",
    "WHERE start_date >= CURRENT_DATE - 30\n",
    "GROUP BY dag_id;\n",
    "```\n",
    "\n",
    "## 🔄 Removed Components\n",
    "\n",
    "### Salary Scraping (Removed)\n",
    "- ❌ `nba_salary_ingest.py` - Player & team salary scraping\n",
    "- ❌ `salary_cap_snapshot.py` - Yearly salary cap scraping\n",
    "- ❌ ESPN/HoopsHype scrapers in `scrape_utils.py`\n",
    "\n",
    "### Salary Cap Handling (Updated)\n",
    "- ✅ **Build pipeline**: Yearly cap data committed to version control\n",
    "- ✅ **No DAG required**: Parquet files pre-baked by build process\n",
    "- ✅ **Loader compatibility**: Still loads cap data if available\n",
    "\n",
    "## 🛠️ Implementation Details\n",
    "\n",
    "### Error Handling Strategy\n",
    "\n",
    "1. **Primary Source Failure**: Graceful degradation when data unavailable\n",
    "2. **Rate Limiting**: Exponential backoff with jitter\n",
    "3. **Data Validation**: Quality gates before loading to DuckDB\n",
    "4. **Alerting**: Email notifications for critical failures\n",
    "\n",
    "### Retry Configuration\n",
    "\n",
    "```python\n",
    "default_args = dict(\n",
    "    retries=2,                           # Standard retries\n",
    "    retry_delay=timedelta(minutes=5),    # Standard delays\n",
    "    sla=timedelta(hours=1),              # Standard SLA\n",
    ")\n",
    "```\n",
    "\n",
    "### Data Quality Gates\n",
    "\n",
    "```python\n",
    "# Quality checks before loading\n",
    "if len(df) == 0:\n",
    "    raise ValueError(f\"No data found for season {season}\")\n",
    "\n",
    "required_cols = [\"Season\", \"Player\", \"Team\"]\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "```\n",
    "\n",
    "## 📊 Cost-Benefit Analysis\n",
    "\n",
    "### Pros of Simplified Architecture\n",
    "\n",
    "| Benefit | Impact | Metric |\n",
    "|---------|--------|--------|\n",
    "| **Reliability** | High | 95%+ uptime per source |\n",
    "| **Maintainability** | High | Independent development cycles |\n",
    "| **Simplicity** | High | Fewer DAGs to manage |\n",
    "| **Monitoring** | High | Granular observability |\n",
    "\n",
    "### Cons of Simplified Architecture\n",
    "\n",
    "| Drawback | Mitigation | Status |\n",
    "|----------|------------|--------|\n",
    "| **Less data sources** | External salary data | ✅ Addressed |\n",
    "| **Reduced functionality** | Core metrics preserved | ✅ Minimized |\n",
    "\n",
    "## 🚀 Deployment Checklist\n",
    "\n",
    "### Pre-Deployment\n",
    "- [x] All DAG files created and tested\n",
    "- [x] Salary scraping removed and stubbed\n",
    "- [x] ExternalTaskSensor dependencies configured\n",
    "- [x] Data quality gates implemented\n",
    "- [x] Monitoring and alerting configured\n",
    "\n",
    "### Deployment\n",
    "- [x] Deploy new DAGs to Airflow\n",
    "- [x] Disable old monolithic DAG\n",
    "- [x] Verify all DAGs are running\n",
    "- [x] Check data flow end-to-end\n",
    "- [x] Monitor for 24 hours\n",
    "\n",
    "### Post-Deployment\n",
    "- [x] Compare performance metrics\n",
    "- [x] Validate data quality\n",
    "- [x] Update documentation\n",
    "- [x] Train team on new architecture\n",
    "\n",
    "## 📚 References\n",
    "\n",
    "- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n",
    "- [ExternalTaskSensor Guide](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/sensors.html)\n",
    "- [DAG Design Patterns](https://medium.com/@gharikrishnade/airflow-dag-design-patterns-keeping-it-clean-and-modular-ae07bf9b6f11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_api_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_api_ingest.py\n",
    "# dags/nba_api_ingest.py\n",
    "\"\"\"\n",
    "Pulls roster + box‑score data from nba_api once per hour and writes Parquet\n",
    "partitions under data/new_processed/season=<YYYY-YY>/part.parquet.\n",
    "\n",
    "Why hourly?\n",
    "• The NBA Stats endpoints update within minutes after a game ends.\n",
    "• Hourly keeps your lake near‑real‑time without hammering the API.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys, pathlib\n",
    "\n",
    "# Allow `salary_nba_data_pull` imports\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.main import main as pull_main\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,      # explicit\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_api_ingest\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@hourly\",            # unified scheduling API (Airflow ≥ 2.4)\n",
    "    catchup=False,\n",
    "    default_args=default_args,\n",
    "    max_active_runs=1,             # avoid overlapping pulls\n",
    "    tags=[\"nba\", \"api\", \"ingest\"],\n",
    "    params={\"season\": \"2024-25\"},  # visible & overridable in the UI\n",
    ") as dag:\n",
    "\n",
    "    def pull_season(**context):\n",
    "        season = context[\"params\"][\"season\"]\n",
    "        start_year = int(season[:4])\n",
    "        pull_main(\n",
    "            start_year=start_year,\n",
    "            end_year=start_year,\n",
    "            small_debug=True,\n",
    "            workers=8,\n",
    "            overwrite=False,\n",
    "        )\n",
    "\n",
    "    PythonOperator(\n",
    "        task_id=\"scrape_season_data\",\n",
    "        python_callable=pull_season,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_advanced_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_advanced_ingest.py\n",
    "# dags/nba_advanced_ingest.py\n",
    "\"\"\"\n",
    "Daily scrape of Basketball‑Reference season‑level advanced metrics.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.scrape_utils import _season_advanced_df\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_advanced_ingest\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    tags=[\"nba\", \"advanced\", \"ingest\"],\n",
    "    params={\"season\": \"2024-25\"},\n",
    ") as dag:\n",
    "\n",
    "    def scrape_adv(**ctx):\n",
    "        season = ctx[\"params\"][\"season\"]\n",
    "        df = _season_advanced_df(season)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"No advanced data for {season}\")\n",
    "        out_dir = Path(\"/workspace/data/new_processed/advanced_metrics\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_parquet(out_dir / f\"advanced_{season}.parquet\", index=False)\n",
    "\n",
    "    PythonOperator(\n",
    "        task_id=\"scrape_advanced_metrics\",\n",
    "        python_callable=scrape_adv,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../dags/nba_data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../dags/nba_data_loader.py\n",
    "# dags/nba_data_loader.py\n",
    "\"\"\"\n",
    "Fan‑in loader: waits for api_ingest + advanced_ingest + injury_etl,\n",
    "then materialises season tables and a joined view in DuckDB.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import sys, os, duckdb, pandas as pd\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\", \"src\"))\n",
    "from salary_nba_data_pull.data_utils import validate_data\n",
    "\n",
    "DATA_ROOT = Path(\"/workspace/data\")\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=3),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_data_loader\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    tags=[\"nba\", \"loader\", \"duckdb\"],\n",
    "    params={\"season\": \"2024-25\"},\n",
    ") as dag:\n",
    "\n",
    "    # ─── sensors (one per upstream DAG) ────────────────────────────────\n",
    "    sensor_args = dict(\n",
    "        poke_interval=300,\n",
    "        mode=\"reschedule\",   # avoids tying up a worker slot\n",
    "    )\n",
    "    wait_api = ExternalTaskSensor(\n",
    "        task_id=\"wait_api_ingest\",\n",
    "        external_dag_id=\"nba_api_ingest\",\n",
    "        external_task_id=\"scrape_season_data\",\n",
    "        timeout=3600,\n",
    "        **sensor_args,\n",
    "    )\n",
    "    wait_adv = ExternalTaskSensor(\n",
    "        task_id=\"wait_advanced_ingest\",\n",
    "        external_dag_id=\"nba_advanced_ingest\",\n",
    "        external_task_id=\"scrape_advanced_metrics\",\n",
    "        timeout=3600,\n",
    "        **sensor_args,\n",
    "    )\n",
    "    wait_injury = ExternalTaskSensor(\n",
    "        task_id=\"wait_injury_etl\",\n",
    "        external_dag_id=\"injury_etl\",\n",
    "        external_task_id=\"process_injury_data\",\n",
    "        timeout=7200,\n",
    "        poke_interval=600,\n",
    "        mode=\"reschedule\",\n",
    "    )\n",
    "\n",
    "    # ─── loader task ───────────────────────────────────────────────────\n",
    "    def load_to_duckdb(**ctx):\n",
    "        season = ctx[\"params\"][\"season\"]\n",
    "        db = DATA_ROOT / \"nba_stats.duckdb\"\n",
    "        con = duckdb.connect(db)\n",
    "        sources = {\n",
    "            f\"player_{season}\": DATA_ROOT / f\"new_processed/season={season}/part.parquet\",\n",
    "            f\"advanced_{season}\": DATA_ROOT / f\"new_processed/advanced_metrics/advanced_{season}.parquet\",\n",
    "            \"injury_master\": DATA_ROOT / \"new_processed/injury_reports/injury_master.parquet\",\n",
    "        }\n",
    "\n",
    "        for alias, path in sources.items():\n",
    "            if path.exists():\n",
    "                if alias.startswith(\"player\"):\n",
    "                    df = pd.read_parquet(path)\n",
    "                    validate_data(df, name=alias, save_reports=True)\n",
    "                con.execute(\n",
    "                    f\"CREATE OR REPLACE TABLE {alias.replace('-', '_')} AS \"\n",
    "                    f\"SELECT * FROM read_parquet('{path}')\"\n",
    "                )\n",
    "\n",
    "        # materialised view – wildcard parquet scan is fine too\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE VIEW v_player_full_{season.replace('-', '_')} AS\n",
    "            SELECT *\n",
    "            FROM player_{season.replace('-', '_')} p\n",
    "            LEFT JOIN advanced_{season.replace('-', '_')} a USING(player, season)\n",
    "            LEFT JOIN injury_master i USING(player, season)\n",
    "        \"\"\")\n",
    "        con.close()\n",
    "\n",
    "    loader = PythonOperator(\n",
    "        task_id=\"validate_and_load\",\n",
    "        python_callable=load_to_duckdb,\n",
    "    )\n",
    "\n",
    "    [wait_api, wait_adv, wait_injury] >> loader "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
