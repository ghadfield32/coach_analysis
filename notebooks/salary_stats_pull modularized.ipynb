{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/fetch_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/fetch_utils.py\n",
    "import time\n",
    "from nba_api.stats.endpoints import commonallplayers, commonplayerinfo, playercareerstats, leaguestandings\n",
    "from requests.exceptions import RequestException\n",
    "from json.decoder import JSONDecodeError\n",
    "import logging\n",
    "\n",
    "# Define the maximum requests allowed per minute and delay between requests\n",
    "MAX_REQUESTS_PER_MINUTE = 20\n",
    "DELAY_BETWEEN_REQUESTS = 3  # seconds\n",
    "\n",
    "def fetch_with_retry(endpoint, max_retries=5, initial_delay=5, max_delay=120, timeout=120, debug=False, **kwargs):\n",
    "    for attempt in range(max_retries):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if debug:\n",
    "                logging.debug(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Fetching data using {endpoint.__name__} (Attempt {attempt + 1}) with parameters: {kwargs}\")\n",
    "            data = endpoint(**kwargs, timeout=timeout).get_data_frames()\n",
    "\n",
    "            if debug and len(data) == 0:\n",
    "                print(f\"Warning: No data returned from {endpoint.__name__}.\")\n",
    "            if debug:\n",
    "                print(f\"Raw API Response: {endpoint(**kwargs, timeout=timeout).get_json()}\")\n",
    "                \n",
    "            time.sleep(DELAY_BETWEEN_REQUESTS)  # Add delay between requests\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if debug:\n",
    "                logging.debug(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Successfully fetched data using {endpoint.__name__} in {elapsed_time:.2f} seconds\")\n",
    "            return data[0] if isinstance(data, list) else data\n",
    "        except (RequestException, JSONDecodeError, KeyError) as e:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if debug:\n",
    "                logging.debug(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Error occurred during fetching {endpoint.__name__}: {e}\")\n",
    "                logging.debug(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Time taken for attempt {attempt + 1}: {elapsed_time:.2f} seconds\")\n",
    "            if attempt == max_retries - 1:\n",
    "                if debug:\n",
    "                    logging.debug(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Failed to fetch data from {endpoint.__name__} after {max_retries} attempts\")\n",
    "                return None\n",
    "            delay = min(initial_delay * (2 ** attempt), max_delay)\n",
    "            if debug:\n",
    "                logging.debug(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "\n",
    "def fetch_all_players(season, debug=False):\n",
    "    all_players_data = fetch_with_retry(commonallplayers.CommonAllPlayers, season=season, debug=debug)\n",
    "    all_players = {}\n",
    "    if all_players_data is not None:\n",
    "        for _, row in all_players_data.iterrows():\n",
    "            player_name = row['DISPLAY_FIRST_LAST'].strip().lower()\n",
    "            player_id = row['PERSON_ID']\n",
    "            team_id = row['TEAM_ID']\n",
    "            all_players[player_name] = {\n",
    "                'player_id': player_id,\n",
    "                'team_id': team_id\n",
    "            }\n",
    "            if debug:\n",
    "                print(f\"Added player to all_players: {player_name} (ID: {player_id}, Team ID: {team_id})\")\n",
    "    else:\n",
    "        if debug:\n",
    "            print(\"Failed to retrieve any player data from commonallplayers endpoint.\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Fetched {len(all_players)} players for season {season}\")\n",
    "        # List some of the fetched players to verify the contents\n",
    "        for i, (name, details) in enumerate(all_players.items()):\n",
    "            if i < 5:  # Only print the first 5 players for brevity\n",
    "                print(f\"Player: {name}, Details: {details}\")\n",
    "\n",
    "    return all_players\n",
    "\n",
    "\n",
    "\n",
    "def fetch_player_info(player_id, debug=False):\n",
    "    return fetch_with_retry(commonplayerinfo.CommonPlayerInfo, player_id=player_id, debug=debug)\n",
    "\n",
    "def fetch_career_stats(player_id, debug=False):\n",
    "    return fetch_with_retry(playercareerstats.PlayerCareerStats, player_id=player_id, debug=debug)\n",
    "\n",
    "def fetch_league_standings(season, debug=False):\n",
    "    return fetch_with_retry(leaguestandings.LeagueStandings, season=season, debug=debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    debug = True\n",
    "    season = \"2022-23\"\n",
    "    sample_player_name = \"LeBron James\"\n",
    "    \n",
    "    # Fetch all players\n",
    "    all_players = fetch_all_players(season, debug=debug)\n",
    "    print(f\"Total players fetched: {len(all_players)}\")\n",
    "    \n",
    "    # Fetch player info for a sample player\n",
    "    if sample_player_name.lower() in all_players:\n",
    "        sample_player_id = all_players[sample_player_name.lower()]['player_id']\n",
    "        player_info = fetch_player_info(sample_player_id, debug=debug)\n",
    "        print(f\"Sample player info for {sample_player_name}:\")\n",
    "        print(player_info)\n",
    "        \n",
    "        # Fetch career stats for the sample player\n",
    "        career_stats = fetch_career_stats(sample_player_id, debug=debug)\n",
    "        print(f\"Sample player career stats for {sample_player_name}:\")\n",
    "        print(career_stats)\n",
    "    else:\n",
    "        print(f\"Player {sample_player_name} not found in the {season} season data.\")\n",
    "    \n",
    "    # Fetch league standings\n",
    "    standings = fetch_league_standings(season, debug=debug)\n",
    "    print(\"League standings:\")\n",
    "    print(standings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/scrape_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/scrape_utils.py\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import time\n",
    "import os\n",
    "\n",
    "def scrape_salary_cap_history(debug=False):\n",
    "    url = \"https://basketball.realgm.com/nba/info/salary_cap\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', class_='basketball compact')\n",
    "        \n",
    "        if not table:\n",
    "            if debug:\n",
    "                print(\"Could not find the salary cap table on the page.\")\n",
    "            return None\n",
    "\n",
    "        data = []\n",
    "        headers = [th.text.strip() for th in table.find('thead').find_all('th')]\n",
    "        for row in table.find('tbody').find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                row_data = [col.text.strip() for col in cols]\n",
    "                data.append(row_data)\n",
    "\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Clean up the data\n",
    "        df['Season'] = df['Season'].str.extract(r'(\\d{4}-\\d{4})')\n",
    "        df['Salary Cap'] = df['Salary Cap'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "        \n",
    "        # Convert other columns to float, handling non-numeric values\n",
    "        for col in df.columns:\n",
    "            if col not in ['Season', 'Salary Cap']:\n",
    "                df[col] = pd.to_numeric(df[col].str.replace('$', '').str.replace(',', ''), errors='coerce')\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Salary cap data scraped successfully\")\n",
    "            print(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error scraping salary cap history: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "DELAY_BETWEEN_REQUESTS = 3  # seconds\n",
    "\n",
    "def scrape_player_salary_data(start_season, end_season, player_filter=None, debug=False):\n",
    "    all_data = []\n",
    "    \n",
    "    for season in range(start_season, end_season + 1):\n",
    "        season_str = f\"{season}-{str(season+1)[-2:]}\"\n",
    "        url = f\"https://hoopshype.com/salaries/players/{season}-{season+1}/\"\n",
    "        if debug:\n",
    "            print(f\"Scraping data for {season_str} from URL: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', class_='hh-salaries-ranking-table')\n",
    "        \n",
    "        if table:\n",
    "            rows = table.find_all('tr')[1:]\n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                if len(cols) >= 3:\n",
    "                    player = cols[1].get_text(strip=True)\n",
    "                    if player_filter is None or player_filter.lower() == 'all' or player.lower() == player_filter.lower():\n",
    "                        salary_text = cols[2].get_text(strip=True)\n",
    "                        salary = int(salary_text.replace('$', '').replace(',', ''))\n",
    "                        all_data.append({'Player': player, 'Salary': salary, 'Season': season_str})\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"No salary data found for season {season_str}\")\n",
    "        \n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)  # Delay between requests to avoid hitting rate limits\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    if debug:\n",
    "        print(f\"Scraped salary data for {'all players' if player_filter is None or player_filter.lower() == 'all' else player_filter} from seasons {start_season}-{end_season}:\")\n",
    "        print(df.head())\n",
    "    return df\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    if debug:\n",
    "        print(f\"Scraped salary data for {'all players' if player_filter is None or player_filter.lower() == 'all' else player_filter} from seasons {start_season}-{end_season}:\")\n",
    "        print(df.head())\n",
    "    return df\n",
    "\n",
    "def scrape_team_salary_data(season, debug=False):\n",
    "    url = f\"https://hoopshype.com/salaries/{season}/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='hh-salaries-ranking-table')\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        team = cols[1].get_text(strip=True)\n",
    "        salary = int(cols[2].get_text(strip=True).replace('$', '').replace(',', ''))\n",
    "        data.append({'Team': team, 'Team_Salary': salary, 'Season': season})\n",
    "    df = pd.DataFrame(data)\n",
    "    if debug:\n",
    "        print(f\"Scraped team salary data for season {season}:\")\n",
    "        print(df.head())\n",
    "    return df\n",
    "\n",
    "def scrape_advanced_metrics(player_name, season, debug=False, max_retries=3, retry_delay=60):\n",
    "    def make_request(url):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 429:\n",
    "            if debug:\n",
    "                print(f\"Rate limit hit. Waiting for {retry_delay} seconds before retrying.\")\n",
    "            time.sleep(retry_delay)\n",
    "            return None\n",
    "        return response\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            search_url = f\"https://www.basketball-reference.com/search/search.fcgi?search={player_name.replace(' ', '+')}\"\n",
    "            response = make_request(search_url)\n",
    "            if response is None:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            search_results = soup.find('div', {'class': 'search-results'})\n",
    "\n",
    "            if search_results:\n",
    "                for item in search_results.find_all('div', {'class': 'search-item'}):\n",
    "                    link = item.find('a')\n",
    "                    if link and 'players' in link['href']:\n",
    "                        player_url = f\"https://www.basketball-reference.com{link['href']}\"\n",
    "                        break\n",
    "                else:\n",
    "                    if debug:\n",
    "                        print(f\"No player URL found for {player_name}\")\n",
    "                    return {}\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"No search results found for {player_name}\")\n",
    "                return {}\n",
    "\n",
    "            time.sleep(2)  # Wait 2 seconds between requests\n",
    "\n",
    "            response = make_request(player_url)\n",
    "            if response is None:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table', {'id': 'advanced'})\n",
    "            if table:\n",
    "                df = pd.read_html(StringIO(str(table)))[0]\n",
    "                if isinstance(df.columns, pd.MultiIndex):\n",
    "                    df.columns = df.columns.droplevel()\n",
    "                df['Season'] = df['Season'].astype(str)\n",
    "                df = df[df['Season'].str.contains(season.split('-')[0], na=False)]\n",
    "                if not df.empty:\n",
    "                    row = df.iloc[0]\n",
    "                    metrics = ['PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
    "                    result = {col: row[col] for col in metrics if col in row.index}\n",
    "                    if debug:\n",
    "                        print(f\"Scraped advanced metrics for {player_name} in season {season}: {result}\")\n",
    "                    return result\n",
    "                else:\n",
    "                    if debug:\n",
    "                        print(f\"No advanced metrics found for {player_name} in season {season}\")\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"No advanced stats table found for {player_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Error scraping advanced metrics for {player_name}: {e}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            if debug:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Failed to scrape advanced metrics for {player_name} after {max_retries} attempts\")\n",
    "    return {}\n",
    "\n",
    "def load_injury_data(file_path=None):\n",
    "    if file_path is None:\n",
    "        # Construct the path relative to the script's directory\n",
    "        base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        file_path = os.path.join(base_dir, '../../data/processed/NBA Player Injury Stats(1951 - 2023).csv')\n",
    "\n",
    "    try:\n",
    "        injury_data = pd.read_csv(file_path)\n",
    "        injury_data['Date'] = pd.to_datetime(injury_data['Date'])\n",
    "        injury_data['Season'] = injury_data['Date'].apply(lambda x: f\"{x.year}-{str(x.year+1)[-2:]}\" if x.month >= 10 else f\"{x.year-1}-{str(x.year)[-2:]}\")\n",
    "        print(f\"Injury data loaded successfully from {file_path}\")\n",
    "        return injury_data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Injury data file not found at {file_path}. Proceeding without injury data.\")\n",
    "        return None\n",
    "\n",
    "def merge_injury_data(player_data, injury_data):\n",
    "    if injury_data is None:\n",
    "        return player_data\n",
    "\n",
    "    all_players_df = player_data.copy()\n",
    "    all_players_df['Injured'] = False\n",
    "    all_players_df['Injury_Periods'] = ''\n",
    "    all_players_df['Total_Days_Injured'] = 0\n",
    "    all_players_df['Injury_Risk'] = 'Low Risk'\n",
    "\n",
    "    for index, row in all_players_df.iterrows():\n",
    "        player_injuries = injury_data[\n",
    "            (injury_data['Season'] == row['Season']) & \n",
    "            (injury_data['Relinquished'].str.contains(row['Player'], case=False, na=False))\n",
    "        ]\n",
    "        if not player_injuries.empty:\n",
    "            periods = []\n",
    "            total_days = 0\n",
    "            for _, injury in player_injuries.iterrows():\n",
    "                start_date = injury['Date']\n",
    "                acquired_matches = injury_data[\n",
    "                    (injury_data['Date'] > start_date) & \n",
    "                    (injury_data['Acquired'].str.contains(row['Player'], case=False, na=False))\n",
    "                ]\n",
    "                if not acquired_matches.empty:\n",
    "                    end_date = acquired_matches.iloc[0]['Date']\n",
    "                else:\n",
    "                    # Assuming injuries last until the end of the season if no acquired date is found\n",
    "                    end_year = int(row['Season'].split('-')[1])\n",
    "                    end_date = pd.Timestamp(f\"{end_year}-06-30\")\n",
    "                \n",
    "                period_days = (end_date - start_date).days\n",
    "                total_days += period_days\n",
    "                periods.append(f\"{start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "            all_players_df.at[index, 'Injured'] = True\n",
    "            all_players_df.at[index, 'Injury_Periods'] = '; '.join(periods)\n",
    "            all_players_df.at[index, 'Total_Days_Injured'] = total_days\n",
    "            \n",
    "            # Categorize injury risk based on total days\n",
    "            if total_days < 10:\n",
    "                risk = 'Low Risk'\n",
    "            elif 10 <= total_days <= 20:\n",
    "                risk = 'Moderate Risk'\n",
    "            else:\n",
    "                risk = 'High Risk'\n",
    "            all_players_df.at[index, 'Injury_Risk'] = risk\n",
    "\n",
    "    return all_players_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage and testing of all functions\n",
    "    debug = True\n",
    "    start_season = 2022\n",
    "    end_season = 2023\n",
    "    sample_player = \"Ja Morant\"  # Example player\n",
    "    \n",
    "    print(\"1. Testing scrape_salary_cap_history:\")\n",
    "    salary_cap_history = scrape_salary_cap_history(debug=debug)\n",
    "    \n",
    "    print(\"\\n2. Testing scrape_player_salary_data:\")\n",
    "    player_salary_data = scrape_player_salary_data(start_season, end_season, player_filter=sample_player, debug=debug)\n",
    "    \n",
    "    print(\"\\n3. Testing scrape_team_salary_data:\")\n",
    "    team_salary_data = scrape_team_salary_data(f\"{start_season}-{str(start_season+1)[-2:]}\", debug=debug)\n",
    "    \n",
    "    print(\"\\n4. Testing scrape_advanced_metrics:\")\n",
    "    advanced_metrics = scrape_advanced_metrics(sample_player, f\"{start_season}-{str(start_season+1)[-2:]}\", debug=debug)\n",
    "    print(f\"Advanced Metrics for {sample_player}:\")\n",
    "    print(advanced_metrics)\n",
    "    \n",
    "    print(\"\\n5. Testing load_injury_data and merge_injury_data:\")\n",
    "    injury_data = load_injury_data()\n",
    "    if injury_data is not None:\n",
    "        print(data.head())\n",
    "    else:\n",
    "        print(\"No injury data loaded.\")\n",
    "    if not player_salary_data.empty and injury_data is not None:\n",
    "        merged_data = merge_injury_data(player_salary_data, injury_data)\n",
    "        print(\"Merged data with injury info:\")\n",
    "        columns_to_display = ['Player', 'Season', 'Salary']\n",
    "        if 'Injured' in merged_data.columns:\n",
    "            columns_to_display.append('Injured')\n",
    "        if 'Injury_Periods' in merged_data.columns:\n",
    "            columns_to_display.append('Injury_Periods')\n",
    "        if 'Total_Days_Injured' in merged_data.columns:\n",
    "            columns_to_display.append('Total_Days_Injured')\n",
    "        if 'Injury_Risk' in merged_data.columns:\n",
    "            columns_to_display.append('Injury_Risk')\n",
    "        print(merged_data[columns_to_display].head())\n",
    "\n",
    "    if not player_salary_data.empty:\n",
    "        avg_salary = player_salary_data['Salary'].mean()\n",
    "        print(f\"Average salary for {sample_player} from {start_season} to {end_season}: ${avg_salary:,.2f}\")\n",
    "    \n",
    "    if not team_salary_data.empty:\n",
    "        highest_team_salary = team_salary_data.loc[team_salary_data['Team_Salary'].idxmax()]\n",
    "        print(f\"Team with highest salary in {start_season}-{end_season}: {highest_team_salary['Team']} (${highest_team_salary['Team_Salary']:,.2f})\")\n",
    "    \n",
    "    if not injury_data.empty:\n",
    "        injury_count = injury_data['Relinquished'].str.contains(sample_player, case=False).sum()\n",
    "        print(f\"Number of injuries/illnesses for {sample_player} from {start_season} to {end_season}: {injury_count}\")\n",
    "\n",
    "    print(\"\\nAll tests completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/process_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/process_utils.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import cpi\n",
    "from fetch_utils import fetch_player_info, fetch_career_stats, fetch_league_standings\n",
    "from scrape_utils import scrape_advanced_metrics\n",
    "\n",
    "def inflate_value(value, year_str, debug=False):\n",
    "    try:\n",
    "        year = int(year_str[:4])\n",
    "        current_year = datetime.now().year\n",
    "       \n",
    "        if year >= current_year:\n",
    "            return value  # Return the original value for future years\n",
    "        # Adjust to 2022 dollars to match the original data\n",
    "        inflated_value = cpi.inflate(value, year, to=2022)\n",
    "        if debug:\n",
    "            print(f\"Inflated value {value} from {year} to {inflated_value} (2022 dollars)\")\n",
    "        return inflated_value\n",
    "    except ValueError:\n",
    "        if debug:\n",
    "            print(f\"Invalid year format: {year_str}\")\n",
    "        return value\n",
    "    except cpi.errors.CPIObjectDoesNotExist:\n",
    "        # If data for the specific year is not available, use the earliest available year\n",
    "        earliest_year = min(cpi.SURVEYS['CPI-U'].indexes['annual'].keys()).year\n",
    "        inflated_value = cpi.inflate(value, earliest_year, to=2022)\n",
    "        if debug:\n",
    "            print(f\"Used earliest available year {earliest_year} for inflation calculation\")\n",
    "        return inflated_value\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error inflating value for year {year_str}: {e}\")\n",
    "        return value\n",
    "\n",
    "def calculate_percentages(df, debug=False):\n",
    "    df['FG%'] = df['FG'] / df['FGA'].replace(0, np.nan)\n",
    "    df['3P%'] = df['3P'] / df['3PA'].replace(0, np.nan)\n",
    "    df['2P%'] = df['2P'] / df['2PA'].replace(0, np.nan)\n",
    "    df['FT%'] = df['FT'] / df['FTA'].replace(0, np.nan)\n",
    "    df['eFG%'] = (df['FG'] + 0.5 * df['3P']) / df['FGA'].replace(0, np.nan)\n",
    "    if debug:\n",
    "        print(\"Calculated percentages:\")\n",
    "        print(df[['FG%', '3P%', '2P%', 'FT%', 'eFG%']].head())\n",
    "    return df\n",
    "\n",
    "def process_player_data(player, season, all_players, debug=False):\n",
    "    player_lower = player.lower()\n",
    "    if player_lower not in all_players:\n",
    "        if debug:\n",
    "            print(f\"No player ID found for {player} in all_players. Player might be missing or the name format might differ.\")\n",
    "        # Print the first few keys from all_players to check name formatting\n",
    "        if debug:\n",
    "            print(f\"First few player names in all_players: {list(all_players.keys())[:5]}\")\n",
    "        return None\n",
    "\n",
    "    player_id = all_players[player_lower]['player_id']\n",
    "    team_id = all_players[player_lower]['team_id']\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Processing data for player: {player} (ID: {player_id}, Team ID: {team_id})\")\n",
    "\n",
    "    player_info = fetch_player_info(player_id, debug=debug)\n",
    "    career_stats = fetch_career_stats(player_id, debug=debug)\n",
    "    league_standings = fetch_league_standings(season, debug=debug)\n",
    "\n",
    "    # Scrape advanced metrics from Basketball Reference\n",
    "    advanced_metrics = scrape_advanced_metrics(player, season, debug=debug)\n",
    "\n",
    "    if player_info is None or career_stats is None or career_stats.empty:\n",
    "        if debug:\n",
    "            print(f\"Unable to fetch complete data for {player}\")\n",
    "        return None\n",
    "\n",
    "    season_stats = career_stats[career_stats['SEASON_ID'].str.contains(season.split('-')[0], na=False)]\n",
    "    if season_stats.empty:\n",
    "        if debug:\n",
    "            print(f\"No stats found for {player} in season {season}\")\n",
    "        return None\n",
    "\n",
    "    latest_season_stats = season_stats.iloc[0]\n",
    "    \n",
    "    try:\n",
    "        draft_year = int(player_info['DRAFT_YEAR'].iloc[0])\n",
    "    except ValueError:\n",
    "        draft_year = int(player_info['FROM_YEAR'].iloc[0])\n",
    "\n",
    "    current_season_year = int(season.split('-')[0])\n",
    "    years_of_service = max(0, current_season_year - draft_year)\n",
    "\n",
    "    # Handle missing league standings gracefully\n",
    "    if league_standings is not None and not league_standings.empty:\n",
    "        player_stats = calculate_player_stats(latest_season_stats, player_info, years_of_service, team_id, league_standings, advanced_metrics)\n",
    "    else:\n",
    "        player_stats = calculate_player_stats(latest_season_stats, player_info, years_of_service, team_id, pd.DataFrame(), advanced_metrics)\n",
    "\n",
    "    player_stats.update({'Player': player, 'Season': season})\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Processed data for {player} in season {season}: {player_stats}\")\n",
    "    return player_stats\n",
    "\n",
    "\n",
    "def calculate_player_stats(stats, player_info, years_of_service, team_id, league_standings, advanced_metrics):\n",
    "    fg = stats.get('FGM', 0) or 0\n",
    "    fga = stats.get('FGA', 0) or 0\n",
    "    fg3 = stats.get('FG3M', 0) or 0\n",
    "    fg3a = stats.get('FG3A', 0) or 0\n",
    "    efg = (fg + 0.5 * fg3) / fga if fga != 0 else 0\n",
    "    fg2 = fg - fg3\n",
    "    fg2a = fga - fg3a\n",
    "    fg2_pct = (fg2 / fg2a) if fg2a != 0 else 0\n",
    "\n",
    "    player_stats = {\n",
    "        'Position': player_info.iloc[0]['POSITION'],\n",
    "        'Age': stats.get('PLAYER_AGE', None),\n",
    "        'Team': stats.get('TEAM_ABBREVIATION', None),\n",
    "        'TeamID': team_id,\n",
    "        'Years of Service': years_of_service,\n",
    "        'GP': stats.get('GP', None),\n",
    "        'GS': stats.get('GS', None),\n",
    "        'MP': stats.get('MIN', None),\n",
    "        'FG': fg,\n",
    "        'FGA': fga,\n",
    "        'FG%': stats.get('FG_PCT', None),\n",
    "        '3P': fg3,\n",
    "        '3PA': fg3a,\n",
    "        '3P%': stats.get('FG3_PCT', None),\n",
    "        '2P': fg2,\n",
    "        '2PA': fg2a,\n",
    "        '2P%': fg2_pct,\n",
    "        'eFG%': efg,\n",
    "        'FT': stats.get('FTM', None),\n",
    "        'FTA': stats.get('FTA', None),\n",
    "        'FT%': stats.get('FT_PCT', None),\n",
    "        'ORB': stats.get('OREB', None),\n",
    "        'DRB': stats.get('DREB', None),\n",
    "        'TRB': stats.get('REB', None),\n",
    "        'AST': stats.get('AST', None),\n",
    "        'STL': stats.get('STL', None),\n",
    "        'BLK': stats.get('BLK', None),\n",
    "        'TOV': stats.get('TOV', None),\n",
    "        'PF': stats.get('PF', None),\n",
    "        'PTS': stats.get('PTS', None),\n",
    "    }\n",
    "    \n",
    "    # Add advanced metrics\n",
    "    player_stats.update(advanced_metrics)\n",
    "\n",
    "    if league_standings is not None and not league_standings.empty:\n",
    "        team_standings = league_standings[league_standings['TeamID'] == team_id]\n",
    "        if not team_standings.empty:\n",
    "            player_stats.update({\n",
    "                'Wins': team_standings['WINS'].values[0],\n",
    "                'Losses': team_standings['LOSSES'].values[0]\n",
    "            })\n",
    "\n",
    "    return player_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    debug = True\n",
    "    season = \"2022-23\"\n",
    "    sample_value = 1000000\n",
    "    sample_year = \"2022\"\n",
    "    sample_player = \"LeBron James\"\n",
    "    \n",
    "    # Test inflate_value\n",
    "    inflated_value = inflate_value(sample_value, sample_year, debug=debug)\n",
    "    print(f\"Inflated value: {inflated_value}\")\n",
    "    \n",
    "    # Test calculate_percentages\n",
    "    sample_df = pd.DataFrame({\n",
    "        'FG': [100], 'FGA': [200],\n",
    "        '3P': [50], '3PA': [100],\n",
    "        '2P': [50], '2PA': [100],\n",
    "        'FT': [75], 'FTA': [100]\n",
    "    })\n",
    "    calculated_df = calculate_percentages(sample_df, debug=debug)\n",
    "    print(\"Calculated percentages:\")\n",
    "    print(calculated_df)\n",
    "    \n",
    "    # Test process_player_data\n",
    "    # Note: This requires actual data from fetch_utils and scrape_utils\n",
    "    # Here's a mock-up of how it would work:\n",
    "    # from fetch_utils import fetch_all_players\n",
    "    all_players = fetch_all_players(season, debug=debug)\n",
    "    if sample_player.lower() in all_players:\n",
    "        player_data = process_player_data(sample_player, season, all_players, debug=debug)\n",
    "        print(f\"Processed data for {sample_player}:\")\n",
    "        print(player_data)\n",
    "    else:\n",
    "        print(f\"Player {sample_player} not found in the {season} season data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/data_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/data_utils.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from process_utils import inflate_value\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Remove unnamed columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    \n",
    "    # Remove columns with all NaN values\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Remove rows with all NaN values\n",
    "    df = df.dropna(axis=0, how='all')\n",
    "    \n",
    "    # Ensure only one 'Season' column exists\n",
    "    season_columns = [col for col in df.columns if 'Season' in col]\n",
    "    if len(season_columns) > 1:\n",
    "        df = df.rename(columns={season_columns[0]: 'Season'})\n",
    "        for col in season_columns[1:]:\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    # Remove '3PAr' and 'FTr' columns\n",
    "    columns_to_remove = ['3PAr', 'FTr']\n",
    "    df = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "    \n",
    "    # Round numeric columns to 2 decimal places\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_salary_cap_data(player_data, salary_cap_data):\n",
    "    player_data['Season_Year'] = player_data['Season'].str[:4].astype(int)\n",
    "    salary_cap_data['Season_Year'] = salary_cap_data['Season'].str[:4].astype(int)\n",
    "    \n",
    "    # Add inflation-adjusted salary cap\n",
    "    salary_cap_data['Salary_Cap_Inflated'] = salary_cap_data.apply(\n",
    "        lambda row: inflate_value(row['Salary Cap'], row['Season']),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Merge salary cap data\n",
    "    merged_data = pd.merge(player_data, salary_cap_data, on='Season_Year', how='left', suffixes=('', '_cap'))\n",
    "    \n",
    "    # Update salary cap columns\n",
    "    cap_columns = ['Mid-Level Exception', 'Salary Cap', 'Luxury Tax', '1st Apron', '2nd Apron', 'BAE',\n",
    "                   'Standard /Non-Taxpayer', 'Taxpayer', 'Team Room /Under Cap', 'Salary_Cap_Inflated']\n",
    "    for col in cap_columns:\n",
    "        if f'{col}_cap' in merged_data.columns:\n",
    "            merged_data[col] = merged_data[col].fillna(merged_data[f'{col}_cap'])\n",
    "            merged_data.drop(columns=[f'{col}_cap'], inplace=True)\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    merged_data.drop(columns=['Season_Year'], inplace=True)\n",
    "    \n",
    "    # Clean the dataframe\n",
    "    merged_data = clean_dataframe(merged_data)\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def validate_data(df):\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"Warning: Missing values found in the following columns:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicates = df.duplicated()\n",
    "    if duplicates.sum() > 0:\n",
    "        print(f\"Warning: {duplicates.sum()} duplicate rows found\")\n",
    "    \n",
    "    # Check data types\n",
    "    expected_types = {\n",
    "        'Season': 'object',\n",
    "        'Player': 'object',\n",
    "        'Age': 'float64',\n",
    "        'GP': 'float64',\n",
    "        'MP': 'float64',\n",
    "        'Salary': 'float64',\n",
    "        'Team_Salary': 'float64',\n",
    "        'Salary Cap': 'float64',\n",
    "        'Salary_Cap_Inflated': 'float64'\n",
    "    }\n",
    "    for col, expected_type in expected_types.items():\n",
    "        if col in df.columns:\n",
    "            actual_type = df[col].dtype\n",
    "            if str(actual_type) != expected_type:\n",
    "                print(f\"Warning: Column '{col}' has type {actual_type}, expected {expected_type}\")\n",
    "    \n",
    "    # Check for negative values in numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if (df[col] < 0).any():\n",
    "            print(f\"Warning: Negative values found in column '{col}'\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_nba_data_pull/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_nba_data_pull/main.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from fetch_utils import fetch_all_players\n",
    "from process_utils import process_player_data, inflate_value, calculate_percentages\n",
    "from scrape_utils import scrape_salary_cap_history, merge_injury_data, scrape_player_salary_data, scrape_team_salary_data, load_injury_data\n",
    "from data_utils import clean_dataframe, merge_salary_cap_data, validate_data\n",
    "\n",
    "\n",
    "def update_data(existing_data, start_year, end_year, player_filter=None, min_avg_minutes=None, debug=False):\n",
    "    all_data = existing_data.copy() if existing_data is not None else pd.DataFrame()\n",
    "\n",
    "    # Load injury data\n",
    "    injury_data = load_injury_data()\n",
    "\n",
    "    salary_data = scrape_player_salary_data(start_year, end_year, player_filter=player_filter, debug=debug)\n",
    "\n",
    "    new_data = pd.DataFrame()\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Processing season: {season}\")\n",
    "        \n",
    "        team_salary_data = scrape_team_salary_data(season, debug=debug)\n",
    "        all_players = fetch_all_players(season=season, debug=debug)\n",
    "        \n",
    "        season_salary_data = salary_data[salary_data['Season'] == season]\n",
    "        \n",
    "        if player_filter and player_filter.lower() != 'all':\n",
    "            season_salary_data = season_salary_data[season_salary_data['Player'].str.lower() == player_filter.lower()]\n",
    "\n",
    "        additional_stats = []\n",
    "\n",
    "        for _, salary_row in season_salary_data.iterrows():\n",
    "            player_name = salary_row['Player']\n",
    "            player_name_lower = player_name.lower()\n",
    "            \n",
    "            if player_name_lower in all_players:\n",
    "                player_stats = process_player_data(player_name, season, all_players, debug=debug)\n",
    "                if player_stats:\n",
    "                    player_stats['Salary'] = salary_row['Salary']\n",
    "                    additional_stats.append(player_stats)\n",
    "            elif debug:\n",
    "                print(f\"Player not found in all_players: {player_name}\")\n",
    "\n",
    "        additional_stats_df = pd.DataFrame(additional_stats)\n",
    "\n",
    "        if additional_stats_df.empty or 'Team' not in additional_stats_df.columns:\n",
    "            if debug:\n",
    "                print(f\"Warning: No valid player stats data for season {season}\")\n",
    "            continue\n",
    "\n",
    "        # Merge team salary data\n",
    "        merged_data = pd.merge(additional_stats_df, team_salary_data, on=['Team', 'Season'], how='left', suffixes=('', '_team'))\n",
    "\n",
    "        # Apply minimum average minutes filter if specified\n",
    "        if min_avg_minutes is not None:\n",
    "            before_filter = len(merged_data)\n",
    "            merged_data = merged_data[merged_data['MP'] >= min_avg_minutes]\n",
    "            if debug:\n",
    "                print(f\"Filtered {before_filter - len(merged_data)} players based on minimum average minutes\")\n",
    "\n",
    "        # Merge injury data\n",
    "        merged_data = merge_injury_data(merged_data, injury_data)\n",
    "\n",
    "        new_data = pd.concat([new_data, merged_data], ignore_index=True, sort=False)\n",
    "\n",
    "    # Check if 'Season' column exists before sorting\n",
    "    if 'Season' not in new_data.columns:\n",
    "        if debug:\n",
    "            print(\"Error: 'Season' column is missing in new_data before sorting.\")\n",
    "            print(f\"Columns in new_data: {new_data.columns.tolist()}\")\n",
    "        raise KeyError(\"'Season' column is missing in new_data before sorting.\")\n",
    "\n",
    "    # Remove existing data for the players and seasons we just updated\n",
    "    if not all_data.empty and not new_data.empty:\n",
    "        all_data = all_data[~((all_data['Season'].isin(new_data['Season'])) & \n",
    "                              (all_data['Player'].isin(new_data['Player'])))]\n",
    "\n",
    "    # Combine existing data with new data\n",
    "    all_data = pd.concat([all_data, new_data], ignore_index=True, sort=False)\n",
    "\n",
    "    # Sort the final data by season and player\n",
    "    all_data.sort_values(by=['Season', 'Player'], inplace=True)\n",
    "\n",
    "    # Calculate percentages\n",
    "    all_data = calculate_percentages(all_data)\n",
    "\n",
    "    # Clean the dataframe\n",
    "    all_data = clean_dataframe(all_data)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Final data shape: {all_data.shape}\")\n",
    "        print(f\"Columns: {all_data.columns.tolist()}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def remove_old_logs(log_dir, days_to_keep=7):\n",
    "    current_time = datetime.now()\n",
    "    for log_file in glob.glob(os.path.join(log_dir, 'stat_pull_log_*.txt')):\n",
    "        file_modified_time = datetime.fromtimestamp(os.path.getmtime(log_file))\n",
    "        if current_time - file_modified_time > timedelta(days=days_to_keep):\n",
    "            os.remove(log_file)\n",
    "\n",
    "def main(start_year, end_year, player_filter=None, min_avg_minutes=None, debug=False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_dir = '../../data/stat_pull_output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Remove old log files\n",
    "    remove_old_logs(output_dir)\n",
    "    \n",
    "    # Set up logging\n",
    "    log_file = os.path.join(output_dir, f'stat_pull_log_{get_timestamp()}.txt')\n",
    "    logging.basicConfig(filename=log_file, level=logging.DEBUG if debug else logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    try:\n",
    "        logging.info(f\"Starting data update for years {start_year} to {end_year}\")\n",
    "        \n",
    "        \n",
    "        processed_file_path = '../../data/processed/nba_player_data_final_inflated.csv'\n",
    "        salary_cap_file_path = '../../data/processed/salary_cap_history_inflated.csv'\n",
    "\n",
    "        # Load existing data\n",
    "        try:\n",
    "            existing_data = pd.read_csv(processed_file_path)\n",
    "        except FileNotFoundError:\n",
    "            existing_data = pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            if debug:\n",
    "                print(f\"Updating data for years {start_year} to {end_year}\")\n",
    "            updated_data = update_data(existing_data, start_year, end_year, player_filter, min_avg_minutes, debug=debug)\n",
    "\n",
    "            if not updated_data.empty:\n",
    "                if debug:\n",
    "                    print(\"New data retrieved. Processing and saving...\")\n",
    "\n",
    "                salary_cap_data = scrape_salary_cap_history(debug=debug)\n",
    "\n",
    "                if salary_cap_data is not None:\n",
    "                    salary_cap_data.to_csv(salary_cap_file_path, index=False)\n",
    "                    updated_data = merge_salary_cap_data(updated_data, salary_cap_data)\n",
    "\n",
    "                # Final cleaning of the data\n",
    "                updated_data = clean_dataframe(updated_data)\n",
    "\n",
    "                # Save the updated data\n",
    "                updated_data.to_csv(processed_file_path, index=False, float_format='%.2f')\n",
    "                if debug:\n",
    "                    print(f\"Updated data saved to {processed_file_path}\")\n",
    "\n",
    "                # Print summary of the data\n",
    "                summary_columns = ['Season', 'Player', 'Salary', 'GP', 'PTS', 'TRB', 'AST', 'PER', 'WS', 'VORP', 'Injured', 'FG%', '3P%', 'FT%', 'Team_Salary', 'Salary Cap', 'Salary_Cap_Inflated']\n",
    "                available_columns = [col for col in summary_columns if col in updated_data.columns]\n",
    "                print(\"\\nData summary:\")\n",
    "                print(updated_data[available_columns].head().to_string(index=False))\n",
    "            else:\n",
    "                print(\"No new data to save. The dataset is empty.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Traceback:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        logging.info(\"Data update completed successfully\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {str(e)}\")\n",
    "        logging.error(\"Traceback:\", exc_info=True)\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        logging.info(f\"Total execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "        # Print summary to console as well\n",
    "        print(f\"Process completed. Log file saved to: {log_file}\")\n",
    "        print(f\"Total execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    current_year = datetime.now().year\n",
    "    parser = argparse.ArgumentParser(description=\"Update NBA player data\")\n",
    "    parser.add_argument(\"--start_year\", type=int, default=current_year-1, help=\"Start year for data update\")\n",
    "    parser.add_argument(\"--end_year\", type=int, default=current_year, help=\"End year for data update\")\n",
    "    parser.add_argument(\"--player_filter\", type=str, default=\"all\", help=\"Filter for specific player or 'all'\")\n",
    "    parser.add_argument(\"--min_avg_minutes\", type=float, default=25, help=\"Minimum average minutes per game\")\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug mode\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.start_year, args.end_year, args.player_filter, args.min_avg_minutes, args.debug)\n",
    "\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     start_year = 2019\n",
    "#     end_year = 2023\n",
    "#     player_filter = input(\"Enter player name or 'all' for all players: \").strip()\n",
    "#     min_avg_minutes = None\n",
    "#     if player_filter.lower() == 'all':\n",
    "#         min_avg_minutes = float(input(\"Enter the minimum average minutes per game (default 25 mins): \") or 25)\n",
    "\n",
    "#     debug = True  # Set to False to disable debug output\n",
    "\n",
    "#     main(start_year, end_year, player_filter, min_avg_minutes, debug)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/coach_analysis/notebooks\n",
      "/workspaces/coach_analysis/src/salary_nba_data_pull\n",
      "/workspaces/coach_analysis/src/salary_nba_data_pull\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "%cd ../src/salary_nba_data_pull\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python main.py --debug --start_year 2018 --end_year 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
