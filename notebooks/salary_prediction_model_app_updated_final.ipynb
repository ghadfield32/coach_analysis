{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/data_loader_preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/data_loader_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logger.info(f\"Data loaded. Shape: {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def format_season(data):\n",
    "    try:\n",
    "        data['Season'] = data['Season'].apply(lambda x: int(x.split('-')[0]))\n",
    "        logger.info(f\"Seasons in data: {data['Season'].unique()}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to format season data: {e}\")\n",
    "        raise\n",
    "\n",
    "def clean_data(data):\n",
    "    try:\n",
    "        data_clean = data.copy()\n",
    "        columns_to_drop = ['Injury_Periods', '2nd Apron', 'Wins', 'Losses']\n",
    "        data_clean.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "        \n",
    "        percentage_cols = ['3P%', '2P%', 'FT%', 'TS%']\n",
    "        for col in percentage_cols:\n",
    "            if col in data_clean.columns:\n",
    "                data_clean[col] = data_clean[col].fillna(data_clean[col].mean())\n",
    "        \n",
    "        data_clean = data_clean.dropna()\n",
    "        logger.info(f\"Data cleaned. Remaining shape: {data_clean.shape}\")\n",
    "        return data_clean\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to clean data: {e}\")\n",
    "        raise\n",
    "\n",
    "def engineer_features(data):\n",
    "    # Calculate per-game statistics to normalize performance data\n",
    "    per_game_cols = ['PTS', 'AST', 'TRB', 'STL', 'BLK', 'TOV']\n",
    "    for col in per_game_cols:\n",
    "        data[f'{col[0]}PG'] = data[col] / data['GP']\n",
    "    \n",
    "    # Derive additional features to capture important aspects of a player's performance\n",
    "    data['Availability'] = data['GP'] / 82\n",
    "    data['SalaryPct'] = data['Salary'] / data['Salary_Cap_Inflated']\n",
    "    data['Efficiency'] = (data['PTS'] + data['TRB'] + data['AST'] + data['STL'] + data['BLK']) / (data['FGA'] + data['FTA'] + data['TOV'] + 1)\n",
    "    data['ValueOverReplacement'] = data['VORP'] / (data['Salary'] + 1)\n",
    "    data['ExperienceSquared'] = data['Years of Service'] ** 2\n",
    "    data['Days_Injured_Percentage'] = data['Total_Days_Injured'] / data['GP']\n",
    "    data['WSPG'] = data['WS'] / data['GP']\n",
    "    data['DWSPG'] = data['DWS'] / data['GP']\n",
    "    data['OWSPG'] = data['OWS'] / data['GP']\n",
    "    data['PFPG'] = data['PF'] / data['GP']\n",
    "    data['ORPG'] = data['ORB'] / data['GP']\n",
    "    data['DRPG'] = data['DRB'] / data['GP']\n",
    "    \n",
    "    # Drop columns used in feature creation or deemed less relevant\n",
    "    columns_to_drop = ['GP', '2PA', 'OBPM', 'BPM', 'DBPM', '2P', 'GS', 'PTS', 'AST', 'TRB', 'STL', 'BLK',\n",
    "                       'TOV', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB', 'DRB', 'TRB',\n",
    "                       'TS%', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'Luxury Tax', '1st Apron', 'BAE',\n",
    "                       'Standard /Non-Taxpayer', 'Taxpayer', 'Team Room /Under Cap', 'WS', 'DWS', 'WS/48', 'PF', 'OWS', 'Injured']\n",
    "    data.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "    print(\"New features added.\")\n",
    "    return data\n",
    "\n",
    "def encode_injury_risk(data):\n",
    "    # Encode injury risk levels for model training\n",
    "    risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    data['Injury_Risk'] = data['Injury_Risk'].map(risk_mapping).fillna(0)  # Default to Low if unknown\n",
    "    return data, risk_mapping\n",
    "\n",
    "def encode_categorical(data, columns):\n",
    "    # Encode categorical columns using one-hot encoding\n",
    "    encoders = {}\n",
    "    for col in columns:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded = encoder.fit_transform(data[[col]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]), index=data.index)\n",
    "        data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "        encoders[col] = encoder\n",
    "    return data, encoders\n",
    "\n",
    "\n",
    "def encode_data(data, encoders=None, player_encoder=None):\n",
    "    print(\"Columns before encoding:\", data.columns)\n",
    "\n",
    "    # Encode Injury_Risk\n",
    "    risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    data['Injury_Risk'] = data['Injury_Risk'].map(risk_mapping).fillna(0)  # Default to Low if unknown\n",
    "\n",
    "    # Encode Player column if it's present\n",
    "    if 'Player' in data.columns:\n",
    "        if player_encoder is None:\n",
    "            player_encoder = LabelEncoder()\n",
    "            data['Player_Encoded'] = player_encoder.fit_transform(data['Player'])\n",
    "        else:\n",
    "            data['Player_Encoded'] = player_encoder.transform(data['Player'])\n",
    "        data.drop('Player', axis=1, inplace=True)  # Drop original Player column after encoding\n",
    "    \n",
    "    # Identify initial numeric columns\n",
    "    initial_numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Encode categorical variables (excluding Season)\n",
    "    categorical_cols = ['Position', 'Team']\n",
    "    if encoders is None:\n",
    "        encoders = {}\n",
    "        for col in categorical_cols:\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Updated line\n",
    "            encoded = encoder.fit_transform(data[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]), index=data.index)\n",
    "            data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "            encoders[col] = encoder\n",
    "    else:\n",
    "        for col in categorical_cols:\n",
    "            encoded = encoders[col].transform(data[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=encoders[col].get_feature_names_out([col]), index=data.index)\n",
    "            data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "\n",
    "    # Identify final numeric columns (excluding one-hot encoded columns and 'Season')\n",
    "    numeric_cols = [col for col in initial_numeric_cols if col not in ['Season', 'Injury_Risk', 'Player_Encoded']]\n",
    "\n",
    "    # Scale numeric features (excluding 'Player_Encoded')\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "    print(\"Encoded data shape:\", data.shape)\n",
    "    print(\"Columns after encoding:\", data.columns)\n",
    "\n",
    "    return data, risk_mapping, encoders, scaler, numeric_cols, player_encoder\n",
    "\n",
    "\n",
    "\n",
    "def scale_features(data, numeric_cols):\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    return data, scaler\n",
    "\n",
    "def decode_data(encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder):\n",
    "    decoded_data = encoded_data.copy()\n",
    "    \n",
    "    # Decode Injury_Risk\n",
    "    inv_injury_risk_mapping = {v: k for k, v in injury_risk_mapping.items()}\n",
    "    decoded_data['Injury_Risk'] = decoded_data['Injury_Risk'].map(inv_injury_risk_mapping)\n",
    "    \n",
    "    # Decode Player column\n",
    "    if 'Player_Encoded' in decoded_data.columns:\n",
    "        decoded_data['Player'] = player_encoder.inverse_transform(decoded_data['Player_Encoded'])\n",
    "        decoded_data.drop('Player_Encoded', axis=1, inplace=True)\n",
    "    \n",
    "    # Decode categorical variables\n",
    "    for col, encoder in encoders.items():\n",
    "        encoded_cols = [c for c in decoded_data.columns if c.startswith(f\"{col}_\")]\n",
    "        decoded_col = encoder.inverse_transform(decoded_data[encoded_cols])\n",
    "        decoded_data[col] = decoded_col.ravel()  # Flatten the 2D array to 1D\n",
    "        decoded_data.drop(encoded_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Inverse transform scaled features\n",
    "    decoded_data[numeric_cols] = scaler.inverse_transform(decoded_data[numeric_cols])\n",
    "    \n",
    "    return decoded_data\n",
    "\n",
    "def select_top_features(X, y, k=10):\n",
    "    # Select top features based on statistical significance\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "    top_features = X.columns[selector.get_support()].tolist()\n",
    "    print(f\"Top {k} features:\", top_features)\n",
    "    return top_features\n",
    "\n",
    "def calculate_tree_feature_importance(X, y):\n",
    "    # Calculate feature importance using a Random Forest Regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    importances = rf.feature_importances_\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "    plt.title('Top 20 Feature Importances from Random Forest')\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        file_path = 'data/processed/nba_player_data_final_inflated.csv'\n",
    "        data = load_data(file_path)\n",
    "        data = format_season(data)\n",
    "        data = clean_data(data)\n",
    "        data = engineer_features(data)\n",
    "\n",
    "        # Separate features and target\n",
    "        X = data.drop(['SalaryPct', 'Salary'], axis=1)\n",
    "        y = data['SalaryPct']\n",
    "\n",
    "        # Encode data\n",
    "        encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder = encode_data(X)\n",
    "        \n",
    "        logger.info(\"Data preprocessing completed. Ready for model training.\")\n",
    "        \n",
    "\n",
    "        print(\"\\nInjury Risk Mapping:\", injury_risk_mapping)\n",
    "        print(\"Encoded Injury Risk range:\", encoded_data['Injury_Risk'].min(), \"-\", encoded_data['Injury_Risk'].max())\n",
    "        print(\"\\nNumeric columns for scaling:\", numeric_cols)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        feature_importances = calculate_tree_feature_importance(encoded_data, y)\n",
    "        print(\"\\nTree-based feature importances:\")\n",
    "        print(feature_importances.head(20))\n",
    "\n",
    "        # Select top features\n",
    "        top_features = select_top_features(encoded_data, y)\n",
    "        print(\"\\nTop features selected using statistical methods:\", top_features)\n",
    "\n",
    "        # Decoding example\n",
    "        print(\"\\nDecoding Example:\")\n",
    "        decoded_data = decode_data(encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder)\n",
    "        \n",
    "        print(\"\\nFirst few rows of decoded data:\")\n",
    "        print(decoded_data[['Player', 'Injury_Risk', 'Position', 'Team', 'Season'] + top_features].head())\n",
    "\n",
    "        print(\"\\nData types after decoding:\")\n",
    "        print(decoded_data.dtypes)\n",
    "\n",
    "        print(\"\\nData preprocessing completed. Ready for model training.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Critical error in data processing pipeline: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/model_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/model_trainer.py\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def inspect_data_types(X):\n",
    "    print(\"Data types of features:\")\n",
    "    print(X.dtypes)\n",
    "    object_columns = X.select_dtypes(include=['object']).columns\n",
    "    if not object_columns.empty:\n",
    "        print(\"Columns with object data types:\", object_columns.tolist())\n",
    "    else:\n",
    "        print(\"No columns with object data types.\")\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model.__class__.__name__}: {-grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_and_save_models(X_train, y_train, model_save_path, scaler, feature_names, encoders, player_encoder, numeric_cols):\n",
    "    # Inspect data types before training\n",
    "    inspect_data_types(X_train)\n",
    "\n",
    "    # Initialize models with default parameters\n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    xgb_model = xgb.XGBRegressor(random_state=42, enable_categorical=True)\n",
    "\n",
    "    # Define parameter grids for grid search\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    # Perform grid search\n",
    "    best_rf_model = perform_grid_search(rf_model, rf_param_grid, X_train, y_train)\n",
    "    best_xgb_model = perform_grid_search(xgb_model, xgb_param_grid, X_train, y_train)\n",
    "\n",
    "    # Train models with best parameters\n",
    "    best_rf_model.fit(X_train, y_train)\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Scale the features used for training\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Save models, scaler, feature names, encoders, and other artifacts\n",
    "    joblib.dump(best_rf_model, f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    joblib.dump(best_xgb_model, f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "    joblib.dump(scaler, f\"{model_save_path}/scaler.pkl\")\n",
    "    joblib.dump(feature_names, f\"{model_save_path}/feature_names.pkl\")\n",
    "    joblib.dump(encoders, f\"{model_save_path}/encoders.pkl\")\n",
    "    joblib.dump(injury_risk_mapping, f\"{model_save_path}/injury_risk_mapping.pkl\")\n",
    "    joblib.dump(numeric_cols, f\"{model_save_path}/numeric_cols.pkl\")\n",
    "\n",
    "    joblib.dump(player_encoder, f\"{model_save_path}/player_encoder.pkl\")\n",
    "    print(\"Models, scaler, feature names, encoders, and other artifacts trained and saved successfully.\")\n",
    "\n",
    "def evaluate_models(X_test, y_test, model_save_path):\n",
    "    # Load models, scaler, and feature names\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "\n",
    "    # Make predictions\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "    xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate models using multiple metrics\n",
    "    metrics = {'Random Forest': rf_predictions, 'XGBoost': xgb_predictions}\n",
    "\n",
    "    for model_name, predictions in metrics.items():\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "\n",
    "        print(f\"\\n{model_name} Evaluation:\")\n",
    "        print(f\"MSE: {mse}\")\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        print(f\"MAE: {mae}\")\n",
    "        print(f\"R-squared: {r2}\")\n",
    "        \n",
    "def filter_seasons(data, predict_season):\n",
    "    \"\"\"\n",
    "    Filters the dataset into prior seasons and the target season for prediction.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset containing season data.\n",
    "        predict_season (int): The season that you want to predict.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two DataFrames:\n",
    "            - prior_seasons_data: Data for seasons before the predict_season.\n",
    "            - target_season_data: Data for the predict_season.\n",
    "    \"\"\"\n",
    "    # Separate data into prior seasons and the target season\n",
    "    prior_seasons_data = data[data['Season'] < predict_season]\n",
    "    target_season_data = data[data['Season'] == predict_season]\n",
    "    \n",
    "    print(f\"Data filtered. Prior seasons shape: {prior_seasons_data.shape}, Target season shape: {target_season_data.shape}\")\n",
    "    \n",
    "    return target_season_data, prior_seasons_data\n",
    "\n",
    "# Data preprocessing\n",
    "def load_and_preprocess_data(file_path, predict_season):\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    _, prior_seasons_data = filter_seasons(data, predict_season)\n",
    "    prior_seasons_data = clean_data(prior_seasons_data)\n",
    "    prior_seasons_data = engineer_features(prior_seasons_data)\n",
    "    return prior_seasons_data\n",
    "\n",
    "# Feature selection\n",
    "def select_features(data, target_column, additional_features=[]):\n",
    "    top_features = ['PPG', 'APG', 'RPG', 'SPG', 'TOPG', 'Years of Service', 'PER', 'VORP', 'WSPG', 'OWSPG']\n",
    "    \n",
    "    # Add 'Injury_Risk', 'Position', and 'Team' to ensure they're included for encoding\n",
    "    top_features += ['Injury_Risk', 'Position', 'Team']\n",
    "    \n",
    "    # Add any additional features\n",
    "    top_features += additional_features\n",
    "    \n",
    "    # Ensure all selected features are in the dataset\n",
    "    available_features = [col for col in top_features if col in data.columns]\n",
    "    \n",
    "    print(\"Available features for modeling:\", available_features)  # Debug statement\n",
    "\n",
    "    X = data[available_features]\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'data/processed/nba_player_data_final_inflated.csv'\n",
    "    predict_season = 2023\n",
    "    target_column = 'SalaryPct'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    preprocessed_data = load_and_preprocess_data(file_path, predict_season)\n",
    "    print(\"Columns after preprocessing:\", preprocessed_data.columns)\n",
    "\n",
    "    # Select features\n",
    "    X, y = select_features(preprocessed_data, target_column)\n",
    "    print(\"Columns after feature selection:\", X.columns)\n",
    "\n",
    "    # Encode data\n",
    "    encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder = encode_data(X)\n",
    "    print(\"Columns after encoding:\", encoded_data.columns)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    model_save_path = 'data/models'\n",
    "    train_and_save_models(X_train, y_train, model_save_path, scaler, encoded_data.columns, encoders, injury_risk_mapping, numeric_cols)\n",
    "    evaluate_models(X_test, y_test, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/model_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/model_predictor.py\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def load_models_and_utils(model_save_path):\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "    scaler = joblib.load(f\"{model_save_path}/scaler.pkl\")\n",
    "    feature_names = joblib.load(f\"{model_save_path}/feature_names.pkl\")\n",
    "    encoders = joblib.load(f\"{model_save_path}/encoders.pkl\")\n",
    "    injury_risk_mapping = joblib.load(f\"{model_save_path}/injury_risk_mapping.pkl\")\n",
    "    numeric_cols = joblib.load(f\"{model_save_path}/numeric_cols.pkl\")\n",
    "    player_encoder = joblib.load(f\"{model_save_path}/player_encoder.pkl\")\n",
    "    return rf_model, xgb_model, scaler, feature_names, encoders, injury_risk_mapping, numeric_cols, player_encoder\n",
    "\n",
    "def predict(data, model_save_path):\n",
    "    rf_model, xgb_model, scaler, feature_names, encoders, _, _, player_encoder = load_models_and_utils(model_save_path)\n",
    "    \n",
    "    print(\"Original data shape:\", data.shape)\n",
    "    print(\"Original data columns:\", data.columns.tolist())\n",
    "\n",
    "    # Preserve player names\n",
    "    player_names = data['Player'] if 'Player' in data.columns else None\n",
    "    \n",
    "    # Drop the player column before encoding\n",
    "    data = data.drop(columns=['Player'], errors='ignore')\n",
    "    \n",
    "    # Encode the data using the loaded encoders\n",
    "    encoded_data, _, _, _, _, _ = encode_data(data, encoders, player_encoder)\n",
    "    \n",
    "    print(\"Encoded data shape:\", encoded_data.shape)\n",
    "    print(\"Encoded data columns:\", encoded_data.columns.tolist())\n",
    "    \n",
    "    # Handle missing features: Add missing columns and set them to zero\n",
    "    for col in feature_names:\n",
    "        if col not in encoded_data.columns:\n",
    "            encoded_data[col] = 0\n",
    "\n",
    "    # Ensure encoded_data only has feature_names columns\n",
    "    encoded_data = encoded_data[feature_names]\n",
    "    \n",
    "    print(\"Selected features shape:\", encoded_data.shape)\n",
    "    print(\"Selected features:\", encoded_data.columns.tolist())\n",
    "    print(\"Expected features:\", feature_names)\n",
    "    \n",
    "    # Scale the encoded data\n",
    "    encoded_data_scaled = scaler.transform(encoded_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_predictions = rf_model.predict(encoded_data_scaled)\n",
    "    xgb_predictions = xgb_model.predict(encoded_data_scaled)\n",
    "    \n",
    "    # Create a DataFrame for predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'RF_Predictions': rf_predictions,\n",
    "        'XGB_Predictions': xgb_predictions,\n",
    "        'Predicted_Salary': (rf_predictions + xgb_predictions) / 2\n",
    "    })\n",
    "    \n",
    "    # Attach player names back to the predictions\n",
    "    if player_names is not None:\n",
    "        predictions_df['Player'] = player_names.values\n",
    "\n",
    "    # Combine the predictions with the original data (excluding player names)\n",
    "    result = pd.concat([data.reset_index(drop=True), predictions_df], axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'data/processed/nba_player_data_final_inflated.csv'\n",
    "    predict_season = 2023\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    current_season_data, _ = filter_seasons(data, predict_season)\n",
    "    current_season_data = clean_data(current_season_data)\n",
    "    current_season_data = engineer_features(current_season_data)\n",
    "    model_save_path = 'data/models'\n",
    "    predictions_df = predict(current_season_data, model_save_path)  # Save predictions as predictions_df\n",
    "    print(predictions_df.head())\n",
    "    \n",
    "    # Save predictions_df for later use\n",
    "    predictions_df.to_csv('data/processed/predictions_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/app.py\n",
    "\n",
    "import os\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from nba_api.stats.static import teams, players\n",
    "\n",
    "# Import functions from other modules\n",
    "from data_loader_preprocessor import load_data, format_season, clean_data, engineer_features, encode_data\n",
    "from model_trainer import train_and_save_models, evaluate_models\n",
    "from model_predictor import predict\n",
    "\n",
    "# Importing Shot Chart Analysis functions\n",
    "from shot_chart.nba_helpers import get_team_abbreviation, categorize_shot, get_all_court_areas\n",
    "from shot_chart.nba_shots import fetch_shots_data, fetch_defensive_shots_data, fetch_shots_for_multiple_players\n",
    "from shot_chart.nba_plotting import plot_shot_chart_hexbin\n",
    "from shot_chart.nba_efficiency import create_mae_table, save_mae_table, load_mae_table, get_seasons_range, calculate_compatibility_between_players\n",
    "from shot_chart.shot_chart_main import run_scenario, preload_mae_tables, create_and_save_mae_table_specific, create_and_save_mae_table_all\n",
    "\n",
    "# Import functions from the small example app\n",
    "from advanced_metrics import plot_career_clusters, plot_injury_risk_vs_salary, plot_availability_vs_salary, plot_vorp_vs_salary, table_metric_salary, display_top_10_salary_per_metric, cluster_players_specialized, display_top_10_salary_per_metric_with_ws\n",
    "\n",
    "# Import New and improved Trade functions\n",
    "from trade_impact_section_st_app import trade_impact_simulator_app\n",
    "\n",
    "@st.cache_data\n",
    "def get_teams_list():\n",
    "    \"\"\"Get the list of NBA teams.\"\"\"\n",
    "    return [team['full_name'] for team in teams.get_teams()]\n",
    "\n",
    "@st.cache_data\n",
    "def get_players_list():\n",
    "    \"\"\"Get the list of NBA players.\"\"\"\n",
    "    return [player['full_name'] for player in players.get_players()]\n",
    "\n",
    "@st.cache_data\n",
    "def load_team_data():\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_df = pd.DataFrame(nba_teams)\n",
    "    return team_df[['id', 'full_name', 'abbreviation']]\n",
    "\n",
    "@st.cache_data\n",
    "def load_player_data(start_year, end_year):\n",
    "    player_data = pd.DataFrame()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        data = fetch_season_data_by_year(year)\n",
    "        if data is not None:\n",
    "            player_data = pd.concat([player_data, data], ignore_index=True)\n",
    "    return player_data\n",
    "\n",
    "def identify_overpaid_underpaid(predictions_df):\n",
    "    # Adjust Predicted_Salary calculation\n",
    "    predictions_df['Predicted_Salary'] = predictions_df['Predicted_Salary'] * predictions_df['Salary_Cap_Inflated']\n",
    "    \n",
    "    predictions_df['Salary_Difference'] = predictions_df['Salary'] - predictions_df['Predicted_Salary']\n",
    "    predictions_df['Overpaid'] = predictions_df['Salary_Difference'] > 0\n",
    "    predictions_df['Underpaid'] = predictions_df['Salary_Difference'] < 0\n",
    "    \n",
    "    overpaid = predictions_df[predictions_df['Overpaid']].sort_values('Salary_Difference', ascending=False)\n",
    "    underpaid = predictions_df[predictions_df['Underpaid']].sort_values('Salary_Difference')\n",
    "    \n",
    "    return overpaid.head(10), underpaid.head(10)\n",
    "\n",
    "# Utility functions\n",
    "def load_processed_data(file_path):\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    data = clean_data(data)\n",
    "    data = engineer_features(data)\n",
    "    return data\n",
    "\n",
    "def filter_data_by_season(data, season):\n",
    "    return data[data['Season'] == season]\n",
    "\n",
    "# Data visualization functions\n",
    "def plot_feature_distribution(data, feature):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.histplot(data[feature], kde=True, ax=ax)\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_heatmap(data):\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    corr = numeric_data.corr()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm', ax=ax)\n",
    "    ax.set_title('Correlation Heatmap')\n",
    "    return fig\n",
    "\n",
    "# Model metrics function\n",
    "def display_model_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    st.subheader(\"Model Performance Metrics\")\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    col1.metric(\"Mean Squared Error\", f\"{mse:.4f}\")\n",
    "    col2.metric(\"Root Mean Squared Error\", f\"{rmse:.4f}\")\n",
    "    col3.metric(\"Mean Absolute Error\", f\"{mae:.4f}\")\n",
    "    col4.metric(\"R-squared\", f\"{r2:.4f}\")\n",
    "\n",
    "def display_overpaid_underpaid(predictions_df):\n",
    "    st.subheader(\"Top 10 Overpaid and Underpaid Players\")\n",
    "\n",
    "    # Add filters\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        team_filter = st.multiselect(\"Filter by Team\", options=sorted(predictions_df['Team'].unique()))\n",
    "    with col2:\n",
    "        position_filter = st.multiselect(\"Filter by Position\", options=sorted(predictions_df['Position'].unique()))\n",
    "\n",
    "    # Apply filters\n",
    "    filtered_df = predictions_df\n",
    "    if team_filter:\n",
    "        filtered_df = filtered_df[filtered_df['Team'].isin(team_filter)]\n",
    "    if position_filter:\n",
    "        filtered_df = filtered_df[filtered_df['Position'].isin(position_filter)]\n",
    "\n",
    "    # Identify overpaid and underpaid players\n",
    "    overpaid, underpaid = identify_overpaid_underpaid(filtered_df)\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.subheader(\"Top 10 Overpaid Players\")\n",
    "        st.dataframe(overpaid[['Player', 'Team', 'Position', 'Salary', 'Predicted_Salary', 'Salary_Difference']])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Top 10 Underpaid Players\")\n",
    "        st.dataframe(underpaid[['Player', 'Team', 'Position', 'Salary', 'Predicted_Salary', 'Salary_Difference']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Shot Chart Analysis function\n",
    "def shot_chart_analysis():\n",
    "    st.header(\"Shot Chart Analysis\")\n",
    "\n",
    "    # Add guidelines and purpose explanation at the top\n",
    "    st.markdown(\"\"\"\n",
    "    ### Welcome to the NBA Shot Analysis App!\n",
    "    \n",
    "    This app allows you to analyze the offensive and defensive efficiency of NBA teams and players. \n",
    "    You can compare players or teams to identify the most efficient spots on the court, \n",
    "    analyze player compatibility based on shot area efficiency, and much more.\n",
    "    \n",
    "    **Options and Guidelines:**\n",
    "    - **Analysis Type**: Choose between offensive, defensive, or both types of analysis.\n",
    "    - **Team or Player**: Analyze a team or an individual player.\n",
    "    - **Court Areas**: Select specific court areas or analyze all areas.\n",
    "    - **Comparison**: Compare multiple players to see how their offensive efficiencies align or differ.\n",
    "    \"\"\")\n",
    "\n",
    "    analysis_type = st.selectbox(\"Select analysis type\", options=[\"offensive\", \"defensive\", \"both\"])\n",
    "\n",
    "    entity_type = st.selectbox(\"Analyze a Team or Player?\", options=[\"team\", \"player\"])\n",
    "\n",
    "    if entity_type == \"team\":\n",
    "        st.markdown(\"_**Team option is able to analyze both offense and defense by looking into the defense by shot detail from other teams' shot charts against the Opposing Team.**_\")\n",
    "        entity_name = st.selectbox(\"Select a Team\", options=get_teams_list())\n",
    "    else:\n",
    "        st.markdown(\"_**Player Option is only able to look at offense.**_\")\n",
    "        player_names = st.multiselect(\"Select Players to Analyze\", options=get_players_list())\n",
    "\n",
    "    season = st.selectbox(\"Select the season\", options=[\"2023-24\", \"2022-23\", \"2021-22\", \"2020-21\"])\n",
    "\n",
    "    opponent_type = st.selectbox(\"Compare against all teams or a specific team?\", options=[\"all\", \"specific\"])\n",
    "\n",
    "    opponent_name = None\n",
    "    if opponent_type == \"specific\":\n",
    "        opponent_name = st.selectbox(\"Select an Opponent Team\", options=get_teams_list())\n",
    "\n",
    "    court_areas = st.selectbox(\"Select court areas to analyze\", options=[\"all\", \"specific\"], index=0)\n",
    "\n",
    "    if court_areas == \"specific\":\n",
    "        court_areas = st.multiselect(\"Select specific court areas\", options=get_all_court_areas())\n",
    "    else:\n",
    "        court_areas = \"all\"\n",
    "\n",
    "    debug_mode = st.checkbox(\"Enable Debug Mode\", value=False)\n",
    "\n",
    "    if st.button(\"Run Analysis\"):\n",
    "        if entity_type == \"player\" and (not player_names or len(player_names) < 1):\n",
    "            st.error(\"Please select at least one player.\")\n",
    "        else:\n",
    "            if entity_type == \"player\":\n",
    "                if len(player_names) == 1:\n",
    "                    # Single player analysis\n",
    "                    run_scenario(\n",
    "                        entity_name=player_names[0],\n",
    "                        entity_type=entity_type,\n",
    "                        season=season,\n",
    "                        opponent_name=opponent_name,\n",
    "                        analysis_type=analysis_type,\n",
    "                        compare_players=False,\n",
    "                        player_names=None,\n",
    "                        court_areas=court_areas\n",
    "                    )\n",
    "                else:\n",
    "                    # Multiple players comparison\n",
    "                    player_shots = fetch_shots_for_multiple_players(player_names, season, court_areas, opponent_name, debug=debug_mode)\n",
    "\n",
    "                    for player, shots in player_shots.items():\n",
    "                        st.pyplot(plot_shot_chart_hexbin(shots['shots'], f'{player} Shot Chart', opponent=opponent_name if opponent_name else \"all teams\"))\n",
    "                        st.write(f\"Efficiency for {player}:\")\n",
    "                        st.write(shots['efficiency'])\n",
    "\n",
    "                    compatibility_df = calculate_compatibility_between_players(player_shots)\n",
    "                    st.write(\"Player Shooting Area Compatibility:\")\n",
    "                    st.write(compatibility_df)\n",
    "            else:\n",
    "                # Team analysis\n",
    "                run_scenario(\n",
    "                    entity_name=entity_name,\n",
    "                    entity_type=entity_type,\n",
    "                    season=season,\n",
    "                    opponent_name=opponent_name,\n",
    "                    analysis_type=analysis_type,\n",
    "                    compare_players=False,\n",
    "                    court_areas=court_areas\n",
    "                )\n",
    "\n",
    "    # Add explanation for shot chart MAE analysis\n",
    "    with st.expander(\"Understanding MAE in Player Analysis with context from their Shooting\"):\n",
    "        st.markdown(\"\"\"\n",
    "        **MAE** is a metric that measures the average magnitude of errors between predicted values and actual values, without considering their direction.\n",
    "        \n",
    "        In our context, MAE is used to measure the difference between the shooting efficiencies of two players across various areas on the court.\n",
    "        \n",
    "        **Steps to Analyze MAE:**\n",
    "        1. **Define Common Areas**: The court is divided into areas like \"Left Corner 3\", \"Top of Key\", \"Paint\", etc.\n",
    "        2. **Calculate Individual Efficiencies**: Fetch shot data for each player and calculate their shooting efficiency in these areas.\n",
    "        3. **Identify Common Areas**: When comparing players, identify the areas where both players have taken shots.\n",
    "        4. **Calculate MAE**: Compute the absolute difference between efficiencies in each common area and average them.\n",
    "        5. **Interpret Compatibility**:\n",
    "            - **High MAE**: Indicates players excel in different areas (more compatible).\n",
    "            - **Low MAE**: Indicates similar efficiencies in the same areas (less compatible).\n",
    "        \n",
    "        **Use this metric to assess player compatibility based on where they excel on the court!**\n",
    "        \"\"\")\n",
    "\n",
    "    with st.expander(\"Understanding MAE in Team (offensive or defensive) in comparison to other Teams\"):\n",
    "        st.markdown(\"\"\"\n",
    "        **MAE** is a metric that measures the average magnitude of errors between predicted values and actual values, without considering their direction.\n",
    "        \n",
    "        In the context of team analysis, MAE is used to measure the difference between the shooting efficiencies of one team's offense and the defensive efficiencies of other teams.\n",
    "        \n",
    "        **Steps to Analyze MAE for Team Comparison:**\n",
    "        1. **Calculate Offensive Efficiency**: Fetch shot data for the team of interest and calculate their shooting efficiency across various areas on the court.\n",
    "        2. **Calculate Defensive Efficiency of Opponents**: For each opponent team, calculate their defensive efficiency by analyzing how well they defend these same areas on the court.\n",
    "        3. **Calculate MAE**: Compute the MAE between the offensive efficiency of the team of interest and the defensive efficiencies of each opponent team across the defined court areas.\n",
    "        4. **Interpret the Results**:\n",
    "            - **Low MAE**: Indicates that the opponent team is effective at defending the areas where the team of interest typically excels. This suggests that the opponent is a \"bad fit\" for the team of interest, as they defend well against their strengths.\n",
    "            - **High MAE**: Indicates that the opponent team struggles to defend the areas where the team of interest typically excels. This suggests that the opponent is a \"good fit\" for the team of interest, as their defense is less effective against the team's offensive strengths.\n",
    "        \n",
    "        **Use this analysis to identify which teams are tough matchups (bad fits) versus easier matchups (good fits) based on how well they can defend your team's key offensive areas!**\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "# Advanced Metrics Analysis Function\n",
    "def advanced_metrics_analysis():\n",
    "    st.header(\"NBA Advanced Metrics and Salary Analysis\")\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv('data/processed/nba_player_data_final_inflated.csv')\n",
    "    \n",
    "    # Add a dropdown to select the season\n",
    "    seasons = sorted(data['Season'].unique(), reverse=True)\n",
    "    selected_season = st.selectbox(\"Select a Season\", seasons)\n",
    "    \n",
    "    # Filter the data by the selected season\n",
    "    data_season = data[data['Season'] == selected_season]\n",
    "    \n",
    "    # Cluster players based on the filtered data\n",
    "    data_season = cluster_players_specialized(data_season, n_clusters=7)\n",
    "    \n",
    "    st.header(\"Plots\")\n",
    "    \n",
    "    # Dropdown to select the plot\n",
    "    plot_choice = st.selectbox(\"Select a plot to view:\", \n",
    "                               [\"Career Clusters: Age vs Salary\", \n",
    "                                \"Injury Risk vs Salary\", \n",
    "                                \"Availability vs Salary\", \n",
    "                                \"VORP vs Salary\"])\n",
    "    \n",
    "    if plot_choice == \"Career Clusters: Age vs Salary\":\n",
    "        fig = plot_career_clusters(data_season)\n",
    "        st.pyplot(fig)\n",
    "    elif plot_choice == \"Injury Risk vs Salary\":\n",
    "        fig = plot_injury_risk_vs_salary(data_season)\n",
    "        st.pyplot(fig)\n",
    "    elif plot_choice == \"Availability vs Salary\":\n",
    "        fig = plot_availability_vs_salary(data_season)\n",
    "        st.pyplot(fig)\n",
    "    elif plot_choice == \"VORP vs Salary\":\n",
    "        fig = plot_vorp_vs_salary(data_season)\n",
    "        st.pyplot(fig)\n",
    "    \n",
    "    st.header(\"Top 10 Salary per Metric Tables\")\n",
    "    \n",
    "    # Calculate metrics table\n",
    "    metric_salary_table = table_metric_salary(data_season)\n",
    "    \n",
    "    # Dropdown to select the metric table\n",
    "    metric_choice = st.selectbox(\"Select a metric to view top 10:\", \n",
    "                                 [\"Salary_per_WS\", \n",
    "                                  \"Salary_per_VORP\", \n",
    "                                  \"Salary_per_OWS\", \n",
    "                                  \"Salary_per_DWS\"])\n",
    "    \n",
    "    # Display the selected top 10 table with WS included\n",
    "    top_10_table = display_top_10_salary_per_metric_with_ws(metric_salary_table, metric_choice)\n",
    "    st.write(f\"Top 10 {metric_choice}:\")\n",
    "    st.dataframe(top_10_table)\n",
    "\n",
    "# Main Streamlit app\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"NBA Salary Prediction, Trade Analysis, and Shot Chart Analysis\", layout=\"wide\")\n",
    "    st.title(\"NBA Salary Prediction, Trade Analysis, and Shot Chart Analysis\")\n",
    "\n",
    "    # Sidebar navigation\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    page = st.sidebar.radio(\n",
    "        \"Go to\",\n",
    "        [\n",
    "            \"Introduction\",\n",
    "            \"Data Analysis\",\n",
    "            \"Model Results\",\n",
    "            \"Salary Evaluation\",\n",
    "            \"Shot Chart Analysis\",\n",
    "            \"Advanced Metrics Analysis\",\n",
    "            \"Trade Impact Simulator\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load base data\n",
    "    print(os.getcwd())\n",
    "    data = load_processed_data('data/processed/nba_player_data_final_inflated.csv')\n",
    "\n",
    "    # Load existing predictions for 2023\n",
    "    initial_predictions_df = pd.read_csv('data/processed/predictions_df.csv')\n",
    "\n",
    "    # Season selection\n",
    "    seasons = sorted(data['Season'].unique(), reverse=True)\n",
    "    selected_season = st.selectbox(\"Select Season\", seasons)\n",
    "\n",
    "    # Load models at the beginning of main()\n",
    "    model_save_path = 'data/models'\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "\n",
    "    # Use initial predictions if 2023 is selected, otherwise retrain\n",
    "    if selected_season == 2023:\n",
    "        predictions_df = initial_predictions_df\n",
    "    else:\n",
    "        # Train model and make predictions\n",
    "        train_data = data[data['Season'] < selected_season]\n",
    "        test_data = data[data['Season'] == selected_season]\n",
    "\n",
    "        # Prepare the data for training\n",
    "        X_train = train_data.drop(['SalaryPct', 'Salary', 'Player'], axis=1)\n",
    "        y_train = train_data['SalaryPct']\n",
    "\n",
    "        # Encode the training data\n",
    "        X_train_encoded, _, encoders, scaler, numeric_cols, player_encoder = encode_data(X_train)\n",
    "\n",
    "        # Train and save models\n",
    "        train_and_save_models(X_train_encoded, y_train, model_save_path, scaler, X_train_encoded.columns, encoders, player_encoder, numeric_cols)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        predictions_df = predict(test_data, model_save_path)\n",
    "\n",
    "\n",
    "    if page == \"Introduction\":\n",
    "        st.title(\"Enhanced NBA Player Salary Analysis\")\n",
    "        st.write(\"Welcome to the NBA Salary Analysis and Prediction App! This project aims to provide comprehensive insights into NBA player salaries, advanced metrics, and future salary predictions based on historical data. Here's a detailed breakdown of the steps involved in creating this app:\")\n",
    "\n",
    "        st.subheader(\"Data Collection\")\n",
    "        \n",
    "        st.write(\"### Salary Data\")\n",
    "        st.write(\"- **Sources**:\")\n",
    "        st.write(\"  - [Basketball Reference Salary Cap History](https://www.basketball-reference.com/contracts/salary-cap-history.html)\")\n",
    "        st.write(\"- **Description**: Data on the NBA salary cap from various seasons, along with maximum salary details for players based on years of service.\")\n",
    "\n",
    "        st.write(\"### Add Injury Data (source will need to be updated**):\")\n",
    "        st.write(\"- **Source**: [Kaggle NBA Injury Stats 1951-2023](https://www.kaggle.com/datasets/loganlauton/nba-injury-stats-1951-2023/data)\")\n",
    "        st.write(\"- **Description**: This dataset provides detailed statistics on NBA injuries from 1951 to 2023, allowing for analysis of player availability and its impact on performance and salaries.\")\n",
    "\n",
    "        st.write(\"### Advanced Metrics\")\n",
    "        st.write(\"- **Source**: [Basketball Reference](https://www.basketball-reference.com)\")\n",
    "        st.write(\"- **Description**: Advanced player metrics such as Player Efficiency Rating (PER), True Shooting Percentage (TS%), and Value Over Replacement Player (VORP) were scraped using BeautifulSoup.\")\n",
    "\n",
    "        st.write(\"### Player Salaries and Team Data\")\n",
    "        st.write(\"- **Source**: [Hoopshype](https://hoopshype.com)\")\n",
    "        st.write(\"- **Description**: Player salary data was scraped for multiple seasons, with detailed information on individual player earnings and team salaries.\")\n",
    "\n",
    "        st.subheader(\"Data Processing\")\n",
    "\n",
    "        st.write(\"### Inflation Adjustment\")\n",
    "        st.write(\"- **Source**: [Adjusting for Inflation in Python](https://medium.com/analytics-vidhya/adjusting-for-inflation-when-analysing-historical-data-with-python-9d69a8dcbc27)\")\n",
    "        st.write(\"- **Description**: Adjusted historical salary data for inflation to provide a consistent basis for comparison.\")\n",
    "\n",
    "        st.write(\"### Data Aggregation\")\n",
    "        st.write(\"- Steps:\")\n",
    "        st.write(\"  1. Loaded salary data and combined it with team standings and advanced metrics.\")\n",
    "        st.write(\"  2. Merged multiple data sources to create a comprehensive dataset containing player performance, salaries, and advanced metrics.\")\n",
    "\n",
    "        st.subheader(\"Model Training and Prediction\")\n",
    "\n",
    "        st.write(\"### Data Preprocessing\")\n",
    "        st.write(\"- Implemented functions to handle missing values, perform feature engineering, and calculate key metrics such as points per game (PPG), assists per game (APG), and salary growth.\")\n",
    "\n",
    "        st.write(\"### Model Selection\")\n",
    "        st.write(\"- Utilized various machine learning models including Random Forest, Gradient Boosting, Ridge Regression, and others to predict future player salaries.\")\n",
    "        st.write(\"- Employed grid search for hyperparameter tuning and selected the best-performing models based on evaluation metrics like Mean Squared Error (MSE) and R score.\")\n",
    "\n",
    "        st.write(\"### Feature Importance and Clustering\")\n",
    "        st.write(\"- Analyzed feature importance to understand the key factors influencing player salaries.\")\n",
    "        st.write(\"- Clustered players into categories based on career trajectories, providing insights into player development and value.\")\n",
    "\n",
    "        st.subheader(\"App Development\")\n",
    "\n",
    "        st.write(\"### Streamlit App\")\n",
    "        st.write(\"- Built an interactive app using Streamlit to visualize data, perform exploratory data analysis, and make salary predictions.\")\n",
    "        st.write(\"- **Features**:\")\n",
    "        st.write(\"  - **Data Overview**: Display raw and processed data.\")\n",
    "        st.write(\"  - **Exploratory Data Analysis**: Visualize salary distributions, age vs. salary, and other key metrics.\")\n",
    "        st.write(\"  - **Advanced Analytics**: Analyze VORP to salary ratio, career trajectory clusters, and other advanced metrics.\")\n",
    "        st.write(\"  - **Salary Predictions**: Predict future salaries and compare actual vs. predicted values.\")\n",
    "        st.write(\"  - **Player Comparisons**: Compare selected players based on predicted salaries and performance metrics.\")\n",
    "        st.write(\"  - **Model Evaluation**: Evaluate different models and display their performance metrics and feature importance.\")\n",
    "\n",
    "        st.write(\"### Data Files\")\n",
    "        st.write(\"- Stored processed data and model files in a structured format to facilitate easy loading and analysis within the app.\")\n",
    "\n",
    "        st.subheader(\"Improvements:\")\n",
    "        \n",
    "        st.subheader(\"Conclusion\")\n",
    "\n",
    "        st.write(\"This app provides a robust platform for analyzing NBA player salaries, understanding the factors influencing earnings, and predicting future salaries based on historical data and advanced metrics. Explore the app to gain insights into player performance, salary trends, and much more.\")\n",
    "\n",
    "\n",
    "    elif page == \"Data Analysis\":\n",
    "        st.header(\"Data Analysis\")\n",
    "\n",
    "        # Filter data by selected season\n",
    "        season_data = filter_data_by_season(data, selected_season)\n",
    "\n",
    "        # Display basic statistics\n",
    "        st.subheader(\"Basic Statistics\")\n",
    "        st.write(season_data.describe())\n",
    "\n",
    "        # Feature distribution\n",
    "        st.subheader(\"Feature Distribution\")\n",
    "        feature = st.selectbox(\"Select Feature\", season_data.columns)\n",
    "        fig = plot_feature_distribution(season_data, feature)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # Correlation heatmap\n",
    "        st.subheader(\"Correlation Heatmap\")\n",
    "        fig = plot_correlation_heatmap(season_data)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # Data handling explanation\n",
    "        st.subheader(\"Data Handling\")\n",
    "        st.write(\"\"\"\n",
    "        We preprocessed the data to ensure it's suitable for our models:\n",
    "        1. Cleaned missing values and outliers\n",
    "        2. Engineered new features like PPG, APG, etc.\n",
    "        3. Encoded categorical variables (Position, Team, Injury Risk)\n",
    "        4. Scaled numerical features\n",
    "        \"\"\")\n",
    "\n",
    "    elif page == \"Model Results\":\n",
    "        st.header(\"Model Results\")\n",
    "\n",
    "        # Model selection\n",
    "        model_choice = st.selectbox(\"Select Model\", [\"Random Forest\", \"XGBoost\"])\n",
    "\n",
    "        if model_choice == \"Random Forest\":\n",
    "            model = rf_model\n",
    "            y_pred = predictions_df['RF_Predictions']\n",
    "        else:\n",
    "            model = xgb_model\n",
    "            y_pred = predictions_df['XGB_Predictions']\n",
    "\n",
    "        # Display model metrics\n",
    "        display_model_metrics(predictions_df['SalaryPct'], y_pred)\n",
    "\n",
    "        # Feature importance\n",
    "        st.subheader(\"Feature Importance\")\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': model.feature_names_in_,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"Features used in the model:\", model.feature_names_in_)  # Debug statement\n",
    "        print(\"Feature importance data:\", feature_importance.head())  # Debug statement\n",
    "        # Filter out categorical variables (e.g., Position_ and Team_ columns)\n",
    "        filtered_feature_importance = feature_importance[\n",
    "            ~feature_importance['feature'].str.startswith('Team_') &\n",
    "            ~feature_importance['feature'].str.startswith('Position_') &\n",
    "            ~feature_importance['feature'].str.startswith('Injury_Risk')\n",
    "        ]\n",
    "\n",
    "        # Before plotting feature importance\n",
    "        print(\"Filtered features for plotting:\", filtered_feature_importance['feature'].tolist())  # Debug statement\n",
    "        st.bar_chart(filtered_feature_importance.set_index('feature'))\n",
    "\n",
    "\n",
    "        # Model explanation\n",
    "        st.subheader(\"Model Explanation\")\n",
    "        st.write(f\"\"\"\n",
    "        The {model_choice} model was trained on historical NBA player data to predict salary percentages.\n",
    "        We used the following techniques to improve model performance:\n",
    "        1. Feature engineering to create relevant statistics\n",
    "        2. Proper encoding of categorical variables\n",
    "        3. Scaling of numerical features\n",
    "        4. Hyperparameter tuning using GridSearchCV\n",
    "        \"\"\")\n",
    "        \n",
    "    elif page == \"Salary Evaluation\":\n",
    "        st.header(\"Salary Evaluation\")\n",
    "        display_overpaid_underpaid(predictions_df)\n",
    "        st.write(\"\"\"\n",
    "        Using the Predicted Salary and Salary based on that season's stats, we can identify \n",
    "        overpaid and underpaid players. We find the difference to uncover potential value opportunities for different teams.\n",
    "        \"\"\")\n",
    "        \n",
    "\n",
    "    elif page == \"Shot Chart Analysis\":\n",
    "        shot_chart_analysis()\n",
    "\n",
    "    elif page == \"Advanced Metrics Analysis\":\n",
    "        advanced_metrics_analysis()\n",
    "\n",
    "    elif page == \"Trade Impact Simulator\":\n",
    "        st.header(\"Trade Impact Simulator\")\n",
    "        \n",
    "        # Assume selected_season is in YYYY format (e.g., 2023)\n",
    "        trade_impact_simulator_app(selected_season)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
