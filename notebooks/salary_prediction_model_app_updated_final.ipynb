{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/data_loader_preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/data_loader_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logger.info(f\"Data loaded. Shape: {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data from {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def format_season(data):\n",
    "    try:\n",
    "        data['Season'] = data['Season'].apply(lambda x: int(x.split('-')[0]))\n",
    "        logger.info(f\"Seasons in data: {data['Season'].unique()}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to format season data: {e}\")\n",
    "        raise\n",
    "\n",
    "def clean_data(data):\n",
    "    try:\n",
    "        data_clean = data.copy()\n",
    "        columns_to_drop = ['Injury_Periods', '2nd Apron', 'Wins', 'Losses']\n",
    "        data_clean.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "        \n",
    "        percentage_cols = ['3P%', '2P%', 'FT%', 'TS%']\n",
    "        for col in percentage_cols:\n",
    "            if col in data_clean.columns:\n",
    "                data_clean[col] = data_clean[col].fillna(data_clean[col].mean())\n",
    "        \n",
    "        data_clean = data_clean.dropna()\n",
    "        logger.info(f\"Data cleaned. Remaining shape: {data_clean.shape}\")\n",
    "        return data_clean\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to clean data: {e}\")\n",
    "        raise\n",
    "\n",
    "def engineer_features(data):\n",
    "    # Calculate per-game statistics to normalize performance data\n",
    "    per_game_cols = ['PTS', 'AST', 'TRB', 'STL', 'BLK', 'TOV']\n",
    "    for col in per_game_cols:\n",
    "        data[f'{col[0]}PG'] = data[col] / data['GP']\n",
    "    \n",
    "    # Derive additional features to capture important aspects of a player's performance\n",
    "    data['Availability'] = data['GP'] / 82\n",
    "    data['SalaryPct'] = data['Salary'] / data['Salary_Cap_Inflated']\n",
    "    data['Efficiency'] = (data['PTS'] + data['TRB'] + data['AST'] + data['STL'] + data['BLK']) / (data['FGA'] + data['FTA'] + data['TOV'] + 1)\n",
    "    data['ValueOverReplacement'] = data['VORP'] / (data['Salary'] + 1)\n",
    "    data['ExperienceSquared'] = data['Years of Service'] ** 2\n",
    "    data['Days_Injured_Percentage'] = data['Total_Days_Injured'] / data['GP']\n",
    "    data['WSPG'] = data['WS'] / data['GP']\n",
    "    data['DWSPG'] = data['DWS'] / data['GP']\n",
    "    data['OWSPG'] = data['OWS'] / data['GP']\n",
    "    data['PFPG'] = data['PF'] / data['GP']\n",
    "    data['ORPG'] = data['ORB'] / data['GP']\n",
    "    data['DRPG'] = data['DRB'] / data['GP']\n",
    "    \n",
    "    # Drop columns used in feature creation or deemed less relevant\n",
    "    columns_to_drop = ['GP', '2PA', 'OBPM', 'BPM', 'DBPM', '2P', 'GS', 'PTS', 'AST', 'TRB', 'STL', 'BLK',\n",
    "                       'TOV', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB', 'DRB', 'TRB',\n",
    "                       'TS%', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'Luxury Tax', '1st Apron', 'BAE',\n",
    "                       'Standard /Non-Taxpayer', 'Taxpayer', 'Team Room /Under Cap', 'WS', 'DWS', 'WS/48', 'PF', 'OWS', 'Injured']\n",
    "    data.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "    print(\"New features added.\")\n",
    "    return data\n",
    "\n",
    "def encode_injury_risk(data):\n",
    "    # Encode injury risk levels for model training\n",
    "    risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    data['Injury_Risk'] = data['Injury_Risk'].map(risk_mapping).fillna(1)  # Default to Medium if unknown\n",
    "    return data, risk_mapping\n",
    "\n",
    "def encode_categorical(data, columns):\n",
    "    # Encode categorical columns using one-hot encoding\n",
    "    encoders = {}\n",
    "    for col in columns:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded = encoder.fit_transform(data[[col]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]), index=data.index)\n",
    "        data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "        encoders[col] = encoder\n",
    "    return data, encoders\n",
    "\n",
    "\n",
    "def encode_data(data, encoders=None, player_encoder=None):\n",
    "    print(\"Columns before encoding:\", data.columns)\n",
    "\n",
    "    # Encode Injury_Risk\n",
    "    risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    data['Injury_Risk'] = data['Injury_Risk'].map(risk_mapping).fillna(1)  # Default to Medium if unknown\n",
    "\n",
    "    # Encode Player column if it's present\n",
    "    if 'Player' in data.columns:\n",
    "        if player_encoder is None:\n",
    "            player_encoder = LabelEncoder()\n",
    "            data['Player_Encoded'] = player_encoder.fit_transform(data['Player'])\n",
    "        else:\n",
    "            data['Player_Encoded'] = player_encoder.transform(data['Player'])\n",
    "        data.drop('Player', axis=1, inplace=True)  # Drop original Player column after encoding\n",
    "    \n",
    "    # Identify initial numeric columns\n",
    "    initial_numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Encode categorical variables (excluding Season)\n",
    "    categorical_cols = ['Position', 'Team']\n",
    "    if encoders is None:\n",
    "        encoders = {}\n",
    "        for col in categorical_cols:\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Updated line\n",
    "            encoded = encoder.fit_transform(data[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]), index=data.index)\n",
    "            data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "            encoders[col] = encoder\n",
    "    else:\n",
    "        for col in categorical_cols:\n",
    "            encoded = encoders[col].transform(data[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=encoders[col].get_feature_names_out([col]), index=data.index)\n",
    "            data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "\n",
    "    # Identify final numeric columns (excluding one-hot encoded columns and 'Season')\n",
    "    numeric_cols = [col for col in initial_numeric_cols if col not in ['Season', 'Injury_Risk', 'Player_Encoded']]\n",
    "\n",
    "    # Scale numeric features (excluding 'Player_Encoded')\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "    print(\"Encoded data shape:\", data.shape)\n",
    "    print(\"Columns after encoding:\", data.columns)\n",
    "\n",
    "    return data, risk_mapping, encoders, scaler, numeric_cols, player_encoder\n",
    "\n",
    "\n",
    "\n",
    "def scale_features(data, numeric_cols):\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    return data, scaler\n",
    "\n",
    "def decode_data(encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder):\n",
    "    decoded_data = encoded_data.copy()\n",
    "    \n",
    "    # Decode Injury_Risk\n",
    "    inv_injury_risk_mapping = {v: k for k, v in injury_risk_mapping.items()}\n",
    "    decoded_data['Injury_Risk'] = decoded_data['Injury_Risk'].map(inv_injury_risk_mapping)\n",
    "    \n",
    "    # Decode Player column\n",
    "    if 'Player_Encoded' in decoded_data.columns:\n",
    "        decoded_data['Player'] = player_encoder.inverse_transform(decoded_data['Player_Encoded'])\n",
    "        decoded_data.drop('Player_Encoded', axis=1, inplace=True)\n",
    "    \n",
    "    # Decode categorical variables\n",
    "    for col, encoder in encoders.items():\n",
    "        encoded_cols = [c for c in decoded_data.columns if c.startswith(f\"{col}_\")]\n",
    "        decoded_col = encoder.inverse_transform(decoded_data[encoded_cols])\n",
    "        decoded_data[col] = decoded_col.ravel()  # Flatten the 2D array to 1D\n",
    "        decoded_data.drop(encoded_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Inverse transform scaled features\n",
    "    decoded_data[numeric_cols] = scaler.inverse_transform(decoded_data[numeric_cols])\n",
    "    \n",
    "    return decoded_data\n",
    "\n",
    "def select_top_features(X, y, k=10):\n",
    "    # Select top features based on statistical significance\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "    top_features = X.columns[selector.get_support()].tolist()\n",
    "    print(f\"Top {k} features:\", top_features)\n",
    "    return top_features\n",
    "\n",
    "def calculate_tree_feature_importance(X, y):\n",
    "    # Calculate feature importance using a Random Forest Regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    importances = rf.feature_importances_\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "    plt.title('Top 20 Feature Importances from Random Forest')\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        file_path = 'data/processed/nba_player_data_final_inflated.csv'\n",
    "        data = load_data(file_path)\n",
    "        data = format_season(data)\n",
    "        data = clean_data(data)\n",
    "        data = engineer_features(data)\n",
    "\n",
    "        # Separate features and target\n",
    "        X = data.drop(['SalaryPct', 'Salary'], axis=1)\n",
    "        y = data['SalaryPct']\n",
    "\n",
    "        # Encode data\n",
    "        encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder = encode_data(X)\n",
    "        \n",
    "        logger.info(\"Data preprocessing completed. Ready for model training.\")\n",
    "        \n",
    "\n",
    "        print(\"\\nInjury Risk Mapping:\", injury_risk_mapping)\n",
    "        print(\"Encoded Injury Risk range:\", encoded_data['Injury_Risk'].min(), \"-\", encoded_data['Injury_Risk'].max())\n",
    "        print(\"\\nNumeric columns for scaling:\", numeric_cols)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        feature_importances = calculate_tree_feature_importance(encoded_data, y)\n",
    "        print(\"\\nTree-based feature importances:\")\n",
    "        print(feature_importances.head(20))\n",
    "\n",
    "        # Select top features\n",
    "        top_features = select_top_features(encoded_data, y)\n",
    "        print(\"\\nTop features selected using statistical methods:\", top_features)\n",
    "\n",
    "        # Decoding example\n",
    "        print(\"\\nDecoding Example:\")\n",
    "        decoded_data = decode_data(encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder)\n",
    "        \n",
    "        print(\"\\nFirst few rows of decoded data:\")\n",
    "        print(decoded_data[['Player', 'Injury_Risk', 'Position', 'Team', 'Season'] + top_features].head())\n",
    "\n",
    "        print(\"\\nData types after decoding:\")\n",
    "        print(decoded_data.dtypes)\n",
    "\n",
    "        print(\"\\nData preprocessing completed. Ready for model training.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Critical error in data processing pipeline: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/model_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/model_trainer.py\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def inspect_data_types(X):\n",
    "    print(\"Data types of features:\")\n",
    "    print(X.dtypes)\n",
    "    object_columns = X.select_dtypes(include=['object']).columns\n",
    "    if not object_columns.empty:\n",
    "        print(\"Columns with object data types:\", object_columns.tolist())\n",
    "    else:\n",
    "        print(\"No columns with object data types.\")\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model.__class__.__name__}: {-grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_and_save_models(X_train, y_train, model_save_path, scaler, feature_names, encoders, player_encoder, numeric_cols):\n",
    "    # Inspect data types before training\n",
    "    inspect_data_types(X_train)\n",
    "\n",
    "    # Initialize models with default parameters\n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    xgb_model = xgb.XGBRegressor(random_state=42, enable_categorical=True)\n",
    "\n",
    "    # Define parameter grids for grid search\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    # Perform grid search\n",
    "    best_rf_model = perform_grid_search(rf_model, rf_param_grid, X_train, y_train)\n",
    "    best_xgb_model = perform_grid_search(xgb_model, xgb_param_grid, X_train, y_train)\n",
    "\n",
    "    # Train models with best parameters\n",
    "    best_rf_model.fit(X_train, y_train)\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Scale the features used for training\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Save models, scaler, feature names, encoders, and other artifacts\n",
    "    joblib.dump(best_rf_model, f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    joblib.dump(best_xgb_model, f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "    joblib.dump(scaler, f\"{model_save_path}/scaler.pkl\")\n",
    "    joblib.dump(feature_names, f\"{model_save_path}/feature_names.pkl\")\n",
    "    joblib.dump(encoders, f\"{model_save_path}/encoders.pkl\")\n",
    "    joblib.dump(injury_risk_mapping, f\"{model_save_path}/injury_risk_mapping.pkl\")\n",
    "    joblib.dump(numeric_cols, f\"{model_save_path}/numeric_cols.pkl\")\n",
    "\n",
    "    joblib.dump(player_encoder, f\"{model_save_path}/player_encoder.pkl\")\n",
    "    print(\"Models, scaler, feature names, encoders, and other artifacts trained and saved successfully.\")\n",
    "\n",
    "def evaluate_models(X_test, y_test, model_save_path):\n",
    "    # Load models, scaler, and feature names\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "\n",
    "    # Make predictions\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "    xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate models using multiple metrics\n",
    "    metrics = {'Random Forest': rf_predictions, 'XGBoost': xgb_predictions}\n",
    "\n",
    "    for model_name, predictions in metrics.items():\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "\n",
    "        print(f\"\\n{model_name} Evaluation:\")\n",
    "        print(f\"MSE: {mse}\")\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        print(f\"MAE: {mae}\")\n",
    "        print(f\"R-squared: {r2}\")\n",
    "        \n",
    "def filter_seasons(data, predict_season):\n",
    "    \"\"\"\n",
    "    Filters the dataset into prior seasons and the target season for prediction.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset containing season data.\n",
    "        predict_season (int): The season that you want to predict.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two DataFrames:\n",
    "            - prior_seasons_data: Data for seasons before the predict_season.\n",
    "            - target_season_data: Data for the predict_season.\n",
    "    \"\"\"\n",
    "    # Separate data into prior seasons and the target season\n",
    "    prior_seasons_data = data[data['Season'] < predict_season]\n",
    "    target_season_data = data[data['Season'] == predict_season]\n",
    "    \n",
    "    print(f\"Data filtered. Prior seasons shape: {prior_seasons_data.shape}, Target season shape: {target_season_data.shape}\")\n",
    "    \n",
    "    return target_season_data, prior_seasons_data\n",
    "\n",
    "# Data preprocessing\n",
    "def load_and_preprocess_data(file_path, predict_season):\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    _, prior_seasons_data = filter_seasons(data, predict_season)\n",
    "    prior_seasons_data = clean_data(prior_seasons_data)\n",
    "    prior_seasons_data = engineer_features(prior_seasons_data)\n",
    "    return prior_seasons_data\n",
    "\n",
    "# Feature selection\n",
    "def select_features(data, target_column, additional_features=[]):\n",
    "    top_features = ['PPG', 'APG', 'RPG', 'SPG', 'TOPG', 'Years of Service', 'PER', 'VORP', 'WSPG', 'OWSPG']\n",
    "    \n",
    "    # Add 'Injury_Risk', 'Position', and 'Team' to ensure they're included for encoding\n",
    "    top_features += ['Injury_Risk', 'Position', 'Team']\n",
    "    \n",
    "    # Add any additional features\n",
    "    top_features += additional_features\n",
    "    \n",
    "    # Ensure all selected features are in the dataset\n",
    "    available_features = [col for col in top_features if col in data.columns]\n",
    "    \n",
    "    print(\"Available features for modeling:\", available_features)  # Debug statement\n",
    "\n",
    "    X = data[available_features]\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'data/processed/nba_player_data_final_inflated.csv'\n",
    "    predict_season = 2023\n",
    "    target_column = 'SalaryPct'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    preprocessed_data = load_and_preprocess_data(file_path, predict_season)\n",
    "    print(\"Columns after preprocessing:\", preprocessed_data.columns)\n",
    "\n",
    "    # Select features\n",
    "    X, y = select_features(preprocessed_data, target_column)\n",
    "    print(\"Columns after feature selection:\", X.columns)\n",
    "\n",
    "    # Encode data\n",
    "    encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder = encode_data(X)\n",
    "    print(\"Columns after encoding:\", encoded_data.columns)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    model_save_path = 'data/models'\n",
    "    train_and_save_models(X_train, y_train, model_save_path, scaler, encoded_data.columns, encoders, injury_risk_mapping, numeric_cols)\n",
    "    evaluate_models(X_test, y_test, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/model_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/model_predictor.py\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def load_models_and_utils(model_save_path):\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "    scaler = joblib.load(f\"{model_save_path}/scaler.pkl\")\n",
    "    feature_names = joblib.load(f\"{model_save_path}/feature_names.pkl\")\n",
    "    encoders = joblib.load(f\"{model_save_path}/encoders.pkl\")\n",
    "    injury_risk_mapping = joblib.load(f\"{model_save_path}/injury_risk_mapping.pkl\")\n",
    "    numeric_cols = joblib.load(f\"{model_save_path}/numeric_cols.pkl\")\n",
    "    player_encoder = joblib.load(f\"{model_save_path}/player_encoder.pkl\")\n",
    "    return rf_model, xgb_model, scaler, feature_names, encoders, injury_risk_mapping, numeric_cols, player_encoder\n",
    "\n",
    "def predict(data, model_save_path):\n",
    "    rf_model, xgb_model, scaler, feature_names, encoders, _, _, player_encoder = load_models_and_utils(model_save_path)\n",
    "    \n",
    "    print(\"Original data shape:\", data.shape)\n",
    "    print(\"Original data columns:\", data.columns.tolist())\n",
    "\n",
    "    # Preserve player names\n",
    "    player_names = data['Player'] if 'Player' in data.columns else None\n",
    "    \n",
    "    # Drop the player column before encoding\n",
    "    data = data.drop(columns=['Player'], errors='ignore')\n",
    "    \n",
    "    # Encode the data using the loaded encoders\n",
    "    encoded_data, _, _, _, _, _ = encode_data(data, encoders, player_encoder)\n",
    "    \n",
    "    print(\"Encoded data shape:\", encoded_data.shape)\n",
    "    print(\"Encoded data columns:\", encoded_data.columns.tolist())\n",
    "    \n",
    "    # Handle missing features: Add missing columns and set them to zero\n",
    "    for col in feature_names:\n",
    "        if col not in encoded_data.columns:\n",
    "            encoded_data[col] = 0\n",
    "\n",
    "    # Ensure encoded_data only has feature_names columns\n",
    "    encoded_data = encoded_data[feature_names]\n",
    "    \n",
    "    print(\"Selected features shape:\", encoded_data.shape)\n",
    "    print(\"Selected features:\", encoded_data.columns.tolist())\n",
    "    print(\"Expected features:\", feature_names)\n",
    "    \n",
    "    # Scale the encoded data\n",
    "    encoded_data_scaled = scaler.transform(encoded_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_predictions = rf_model.predict(encoded_data_scaled)\n",
    "    xgb_predictions = xgb_model.predict(encoded_data_scaled)\n",
    "    \n",
    "    # Create a DataFrame for predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'RF_Predictions': rf_predictions,\n",
    "        'XGB_Predictions': xgb_predictions,\n",
    "        'Predicted_Salary': (rf_predictions + xgb_predictions) / 2\n",
    "    })\n",
    "    \n",
    "    # Attach player names back to the predictions\n",
    "    if player_names is not None:\n",
    "        predictions_df['Player'] = player_names.values\n",
    "\n",
    "    # Combine the predictions with the original data (excluding player names)\n",
    "    result = pd.concat([data.reset_index(drop=True), predictions_df], axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'data/processed/nba_player_data_final_inflated.csv'\n",
    "    predict_season = 2023\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    current_season_data, _ = filter_seasons(data, predict_season)\n",
    "    current_season_data = clean_data(current_season_data)\n",
    "    current_season_data = engineer_features(current_season_data)\n",
    "    model_save_path = 'data/models'\n",
    "    predictions_df = predict(current_season_data, model_save_path)  # Save predictions as predictions_df\n",
    "    print(predictions_df.head())\n",
    "    \n",
    "    # Save predictions_df for later use\n",
    "    predictions_df.to_csv('data/processed/predictions_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hoopsrumors.com/2023/09/salary-matching-rules-for-trades-during-2023-24-season.html\n",
    "\n",
    "for trade rules\n",
    "\n",
    "\n",
    "FIRST_TAX_APRON = 172_346_000\n",
    "\n",
    "def check_salary_matching_rules(outgoing_salary, incoming_salary, team_salary_before_trade):\n",
    "    if team_salary_before_trade < FIRST_TAX_APRON:\n",
    "        if outgoing_salary <= 7_500_000:\n",
    "            max_incoming_salary = 2 * outgoing_salary + 250_000\n",
    "        elif outgoing_salary <= 29_000_000:\n",
    "            max_incoming_salary = outgoing_salary + 7_500_000\n",
    "        else:\n",
    "            max_incoming_salary = 1.25 * outgoing_salary + 250_000\n",
    "    else:\n",
    "        max_incoming_salary = 1.10 * outgoing_salary\n",
    "\n",
    "    return incoming_salary <= max_incoming_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/percentile_count_trade_impact.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/percentile_count_trade_impact.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from nba_api.stats.endpoints import leaguegamefinder, playergamelogs\n",
    "from nba_api.stats.static import teams, players\n",
    "\n",
    "# Constants\n",
    "RELEVANT_STATS = ['PTS', 'AST', 'TOV', 'STL', 'BLK', 'OREB', 'DREB', 'FGM', 'FG3M', 'FGA']\n",
    "PERCENTILE_THRESHOLDS = [1, 2, 3, 4, 5, 10, 25, 50]\n",
    "\n",
    "def load_team_data():\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_df = pd.DataFrame(nba_teams)\n",
    "    return team_df[['id', 'full_name', 'abbreviation']]\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "def fetch_player_id_by_name(player_name, debug=False):\n",
    "    \"\"\"Fetch player ID based on player name.\"\"\"\n",
    "    try:\n",
    "        player = players.find_players_by_full_name(player_name)[0]\n",
    "        if debug:\n",
    "            print(f\"Fetched ID for player {player_name}: {player['id']}\")\n",
    "        return player['id']\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error fetching ID for player {player_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_champion_for_percentile(season, debug=False):\n",
    "    \"\"\"Fetch the champion team for a given NBA season.\"\"\"\n",
    "    try:\n",
    "        games = leaguegamefinder.LeagueGameFinder(season_nullable=season, season_type_nullable='Playoffs').get_data_frames()[0]\n",
    "        games['GAME_DATE'] = pd.to_datetime(games['GAME_DATE'])\n",
    "        last_game = games.sort_values('GAME_DATE').iloc[-2:]\n",
    "        winner = last_game[last_game['WL'] == 'W'].iloc[0]\n",
    "        if debug:\n",
    "            print(f\"Champion for season {season}: {winner['TEAM_NAME']} ({winner['TEAM_ID']})\")\n",
    "        return winner['TEAM_NAME']\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error fetching champion for season {season}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_champions_for_percentile(start_year, end_year, debug=False):\n",
    "    \"\"\"Fetch champions for each season from start_year to end_year.\"\"\"\n",
    "    champions = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "        champ_name = get_champion_for_percentile(season, debug)\n",
    "        if champ_name:\n",
    "            champions.append({'Season': season, 'ChampionTeamName': champ_name})\n",
    "        elif debug:\n",
    "            print(f\"Champion data not available for season {season}\")\n",
    "        time.sleep(1)  # To avoid overwhelming the API\n",
    "    if debug:\n",
    "        print(f\"Champions data: {champions}\")\n",
    "    return pd.DataFrame(champions)\n",
    "\n",
    "def calculate_average_top_percentiles(top_percentile_counts_df, debug=False):\n",
    "    \"\"\"Calculate the average percentiles for all champion teams, grouped by season.\"\"\"\n",
    "    average_percentiles = {}\n",
    "\n",
    "    for col in RELEVANT_STATS:\n",
    "        for threshold in PERCENTILE_THRESHOLDS:\n",
    "            count_key = f'{col}_Top_{threshold}_count'\n",
    "            avg_key = f'{col}_Avg_Top_{threshold}_percentile'\n",
    "            \n",
    "            # Calculate the mean of counts grouped by 'Season' and then average these means\n",
    "            avg_value = top_percentile_counts_df.groupby('Season')[count_key].mean().mean()\n",
    "            \n",
    "            avg_value = avg_value if pd.notnull(avg_value) else 0\n",
    "            average_percentiles[avg_key] = avg_value\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"{col} Avg Top {threshold}% Count across seasons: {avg_value}\")\n",
    "    \n",
    "    return pd.DataFrame([average_percentiles])\n",
    "\n",
    "\n",
    "def calculate_champion_percentiles(league_percentiles, champions, debug=False):\n",
    "    \"\"\"Extract percentiles for players in champion teams based on league percentiles.\"\"\"\n",
    "    champion_data = league_percentiles[league_percentiles['TEAM_NAME'].isin(champions['ChampionTeamName'])].copy()\n",
    "    \n",
    "    # Merge with champions to get the Season associated with each champion team\n",
    "    champion_data = pd.merge(champion_data, champions, left_on='TEAM_NAME', right_on='ChampionTeamName')\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Champion Data Percentiles with Season:\")\n",
    "        print(champion_data[['TEAM_NAME', 'Season', 'PLAYER_NAME']].head())\n",
    "    \n",
    "    return champion_data\n",
    "\n",
    "\n",
    "def fetch_all_player_data(seasons, debug=False):\n",
    "    \"\"\"Fetch player game logs data for all players across multiple seasons.\"\"\"\n",
    "    all_data = pd.DataFrame()\n",
    "    for season in seasons:\n",
    "        try:\n",
    "            player_logs = playergamelogs.PlayerGameLogs(season_nullable=season).get_data_frames()[0]\n",
    "            player_logs['SEASON'] = season\n",
    "            all_data = pd.concat([all_data, player_logs], ignore_index=True)\n",
    "            if debug:\n",
    "                print(f\"Fetched {len(player_logs)} player logs for the league in season {season}\")\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Error fetching player data for the league in season {season}: {e}\")\n",
    "    if debug:\n",
    "        print(f\"Total logs fetched: {len(all_data)}\")\n",
    "    return all_data\n",
    "\n",
    "def calculate_player_stats(player_data, debug=False):\n",
    "    \"\"\"Calculate average player statistics from game logs.\"\"\"\n",
    "    # Calculate stats per game for players\n",
    "    player_stats = player_data.groupby(['SEASON', 'TEAM_NAME', 'PLAYER_NAME'])[RELEVANT_STATS].mean().reset_index()\n",
    "    \n",
    "    # Rename columns to include '_per_game'\n",
    "    for stat in RELEVANT_STATS:\n",
    "        player_stats.rename(columns={stat: f'{stat}_per_game'}, inplace=True)\n",
    "\n",
    "    if debug:\n",
    "        print(\"Sample player stats (entire league):\")\n",
    "        print(player_stats.head())  # Show head of the player stats\n",
    "    return player_stats\n",
    "\n",
    "def calculate_player_percentiles(stats_df, debug=False):\n",
    "    \"\"\"Calculate percentile ranks for each stat in the DataFrame by season.\"\"\"\n",
    "    percentiles = {}\n",
    "\n",
    "    for col in RELEVANT_STATS:\n",
    "        col_per_game = f'{col}_per_game'\n",
    "        if col_per_game in stats_df.columns:\n",
    "            # Calculate percentiles across the entire dataset\n",
    "            stats_df[f'{col}_percentile'] = stats_df[col_per_game].rank(pct=True, method='min')\n",
    "            # Ensure no NaN values before calculating percentiles\n",
    "            if not stats_df[col_per_game].isna().any():\n",
    "                percentiles[col] = np.percentile(stats_df[col_per_game], [100 - t for t in PERCENTILE_THRESHOLDS])\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"NaN values found in {col_per_game} column.\")\n",
    "            if debug:\n",
    "                print(f\"Calculated percentiles for {col_per_game}:\")\n",
    "                print(stats_df[['TEAM_NAME', 'PLAYER_NAME', col_per_game, f'{col}_percentile']].head())\n",
    "    return stats_df, percentiles\n",
    "\n",
    "def count_top_percentiles(player_percentiles, percentiles, team_name, season, debug=False):\n",
    "    \"\"\"Count how many players in a specific team fall within top percentiles, filtered by season.\"\"\"\n",
    "    top_counts = {f'{stat}_Top_{threshold}_count': 0 for stat in RELEVANT_STATS for threshold in PERCENTILE_THRESHOLDS}\n",
    "    \n",
    "    # Filter the data by team and season\n",
    "    team_data = player_percentiles[(player_percentiles['TEAM_NAME'] == team_name) & (player_percentiles['SEASON'] == season)]\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n{team_name} player percentiles data for season {season}:\\n{team_data[['PLAYER_NAME', 'FG3M_per_game', 'FG3M_percentile']]}\")\n",
    "    \n",
    "    for col in RELEVANT_STATS:\n",
    "        col_per_game = f'{col}_per_game'\n",
    "        if col in percentiles:  # Ensure we have valid percentiles calculated\n",
    "            for idx, threshold in enumerate(PERCENTILE_THRESHOLDS):\n",
    "                count_key = f'{col}_Top_{threshold}_count'\n",
    "                top_counts[count_key] = (team_data[col_per_game] >= percentiles[col][idx]).sum()\n",
    "\n",
    "                if debug and col == 'FG3M':\n",
    "                    print(f\"{col} Top {threshold}% Count for season {season}: {top_counts[count_key]}\")\n",
    "                    print(f\"Players in Top {threshold}% for {col} in season {season}: {team_data[team_data[col_per_game] >= percentiles[col][idx]][['PLAYER_NAME', col_per_game, f'{col}_percentile']]}\")\n",
    "\n",
    "    return top_counts\n",
    "\n",
    "\n",
    "def simulate_trade(player_stats, players_from_team_a, players_from_team_b, team_a_name, team_b_name, debug=False):\n",
    "    \"\"\"Simulate a trade by swapping players between two teams.\"\"\"\n",
    "    if debug:\n",
    "        print(\"\\nBefore trade simulation:\")\n",
    "        print(player_stats[(player_stats['PLAYER_NAME'].isin(players_from_team_a + players_from_team_b))][['PLAYER_NAME', 'TEAM_NAME']])\n",
    "    \n",
    "    # Swap players between the two teams\n",
    "    player_stats.loc[player_stats['PLAYER_NAME'].isin(players_from_team_a), 'TEAM_NAME'] = team_b_name\n",
    "    player_stats.loc[player_stats['PLAYER_NAME'].isin(players_from_team_b), 'TEAM_NAME'] = team_a_name\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nAfter trade simulation:\")\n",
    "        print(player_stats[(player_stats['PLAYER_NAME'].isin(players_from_team_a + players_from_team_b))][['PLAYER_NAME', 'TEAM_NAME']])\n",
    "    \n",
    "    return player_stats\n",
    "\n",
    "def create_comparison_table(before_trade, after_trade, average_percentiles, team_name):\n",
    "    \"\"\"Create a comparison table for a team before and after the trade.\"\"\"\n",
    "    data = {'Team': [team_name] * len(PERCENTILE_THRESHOLDS), 'Percentile': PERCENTILE_THRESHOLDS}\n",
    "    \n",
    "    for stat in RELEVANT_STATS:\n",
    "        before_counts = [before_trade[f'{stat}_Top_{threshold}_count'] for threshold in PERCENTILE_THRESHOLDS]\n",
    "        after_counts = [after_trade[f'{stat}_Top_{threshold}_count'] for threshold in PERCENTILE_THRESHOLDS]\n",
    "        champ_avg = [average_percentiles[f'{stat}_Avg_Top_{threshold}_percentile'][0] for threshold in PERCENTILE_THRESHOLDS]\n",
    "        \n",
    "        data[f'{stat}_Before'] = before_counts\n",
    "        data[f'{stat}_After'] = after_counts\n",
    "        data[f'{stat}_Champ_Avg'] = champ_avg\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.set_index('Percentile', inplace=True)\n",
    "    return df\n",
    "\n",
    "def fetch_and_process_season_data(seasons, debug=False):\n",
    "    # Fetch player data for all specified seasons\n",
    "    all_player_data = fetch_all_player_data(seasons, debug)\n",
    "    \n",
    "    # Calculate player-level stats\n",
    "    player_stats = calculate_player_stats(all_player_data, debug)\n",
    "    \n",
    "    # Calculate percentiles for all players in the league\n",
    "    league_percentiles, league_percentiles_ref = calculate_player_percentiles(player_stats, debug)\n",
    "    \n",
    "    return player_stats, league_percentiles, league_percentiles_ref\n",
    "\n",
    "def get_champion_percentiles(seasons, debug=False):\n",
    "    start_year = int(seasons[0].split('-')[0])\n",
    "    end_year = int(seasons[-1].split('-')[0])\n",
    "\n",
    "    champion_info = get_champions_for_percentile(start_year, end_year, debug)\n",
    "    player_stats, league_percentiles, league_percentiles_ref = fetch_and_process_season_data(seasons, debug)\n",
    "    \n",
    "    # Calculate champion percentiles including the Season column\n",
    "    champion_percentiles = calculate_champion_percentiles(league_percentiles, champion_info, debug)\n",
    "    \n",
    "    # Group by TEAM_NAME and Season, then calculate top percentiles\n",
    "    top_percentile_counts = champion_percentiles.groupby(['TEAM_NAME', 'Season']).apply(\n",
    "        lambda x: count_top_percentiles(x, league_percentiles_ref, x.iloc[0]['TEAM_NAME'], x.iloc[0]['Season'], debug)\n",
    "    ).apply(pd.Series).reset_index()\n",
    "\n",
    "    # Calculate average percentiles across all seasons for each champion team\n",
    "    average_top_percentiles_df = calculate_average_top_percentiles(top_percentile_counts, debug)\n",
    "    \n",
    "    return average_top_percentiles_df\n",
    "\n",
    "\n",
    "def compare_teams_before_after_trade(season, team_a_name, team_b_name, players_from_team_a, players_from_team_b, debug=False):\n",
    "    player_stats, league_percentiles, league_percentiles_ref = fetch_and_process_season_data([season], debug)\n",
    "    \n",
    "    # Count top percentiles before the trade\n",
    "    team_a_top_percentile_counts = count_top_percentiles(league_percentiles, league_percentiles_ref, team_a_name, season, debug)\n",
    "    team_b_top_percentile_counts = count_top_percentiles(league_percentiles, league_percentiles_ref, team_b_name, season, debug)\n",
    "    \n",
    "    # Simulate the trade\n",
    "    player_stats = simulate_trade(player_stats, players_from_team_a, players_from_team_b, team_a_name, team_b_name, debug)\n",
    "    \n",
    "    # Recalculate percentiles after the trade\n",
    "    league_percentiles_after_trade, _ = calculate_player_percentiles(player_stats, debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nAfter trade percentiles calculation:\")\n",
    "        print(league_percentiles_after_trade[['TEAM_NAME', 'PLAYER_NAME', 'FG3M_per_game', 'FG3M_percentile']])\n",
    "    \n",
    "    # Count top percentiles after the trade\n",
    "    team_a_top_percentile_counts_after = count_top_percentiles(league_percentiles_after_trade, league_percentiles_ref, team_a_name, season, debug)\n",
    "    team_b_top_percentile_counts_after = count_top_percentiles(league_percentiles_after_trade, league_percentiles_ref, team_b_name, season, debug)\n",
    "    \n",
    "    return team_a_top_percentile_counts, team_a_top_percentile_counts_after, team_b_top_percentile_counts, team_b_top_percentile_counts_after\n",
    "\n",
    "\n",
    "def generate_comparison_tables(season, team_a_name, team_b_name, players_from_team_a, players_from_team_b, average_top_percentiles_df, debug=False):\n",
    "    team_a_top_before, team_a_top_after, team_b_top_before, team_b_top_after = compare_teams_before_after_trade(\n",
    "        season, team_a_name, team_b_name, players_from_team_a, players_from_team_b, debug\n",
    "    )\n",
    "    \n",
    "    # Create comparison tables with champion average percentiles\n",
    "    celtics_comparison_table = create_comparison_table(team_a_top_before, team_a_top_after, average_top_percentiles_df, team_a_name)\n",
    "    warriors_comparison_table = create_comparison_table(team_b_top_before, team_b_top_after, average_top_percentiles_df, team_b_name)\n",
    "    \n",
    "    return celtics_comparison_table, warriors_comparison_table\n",
    "\n",
    "\n",
    "def main(debug=False):\n",
    "    seasons = [\"2019-20\", \"2020-21\",\"2021-22\", \"2022-23\", \"2023-24\"]\n",
    "\n",
    "    # Fetch champion percentiles and calculate averages\n",
    "    average_top_percentiles_df = get_champion_percentiles(seasons, debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nAverage Champion Percentiles:\")\n",
    "        print(average_top_percentiles_df)\n",
    "    \n",
    "    team_a_name = \"Boston Celtics\"\n",
    "    team_b_name = \"Golden State Warriors\"\n",
    "    players_from_boston = [\"Jaylen Brown\", \"Jayson Tatum\"]\n",
    "    players_from_warriors = [\"Jordan Poole\", \"Kevon Looney\"]\n",
    "    \n",
    "    # Generate comparison tables before and after the trade\n",
    "    celtics_comparison_table, warriors_comparison_table = generate_comparison_tables(\n",
    "        seasons[-1], team_a_name, team_b_name, players_from_boston, players_from_warriors, average_top_percentiles_df, debug\n",
    "    )\n",
    "    \n",
    "    # Display tables\n",
    "    print(\"\\nBoston Celtics Comparison Table:\")\n",
    "    print(celtics_comparison_table)\n",
    "    \n",
    "    print(\"\\nGolden State Warriors Comparison Table:\")\n",
    "    print(warriors_comparison_table)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/overall_team_trade_impact.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/overall_team_trade_impact.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nba_api.stats.endpoints import playergamelogs, leaguegamefinder\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "from nba_api.stats.endpoints import commonplayerinfo\n",
    "from nba_api.stats.static import teams, players\n",
    "\n",
    "# Constants\n",
    "RELEVANT_STATS = ['PTS', 'AST', 'TOV', 'STL', 'BLK', 'OREB', 'DREB', 'FGM', 'FG3M', 'FGA']\n",
    "\n",
    "def load_team_data():\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_df = pd.DataFrame(nba_teams)\n",
    "    return team_df[['id', 'full_name', 'abbreviation']]\n",
    "\n",
    "def fetch_player_id_by_name(player_name, debug=False):\n",
    "    \"\"\"Fetch player ID based on player name.\"\"\"\n",
    "    try:\n",
    "        player = players.find_players_by_full_name(player_name)[0]\n",
    "        if debug:\n",
    "            print(f\"Fetched ID for player {player_name}: {player['id']}\")\n",
    "        return player['id']\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error fetching ID for player {player_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_player_info(player_id, debug=False):\n",
    "    \"\"\"Fetch player information based on player ID.\"\"\"\n",
    "    try:\n",
    "        player_info = commonplayerinfo.CommonPlayerInfo(player_id=player_id).get_data_frames()[0]\n",
    "        if debug:\n",
    "            print(f\"Fetched info for player ID {player_id}: {player_info['DISPLAY_FIRST_LAST'].values[0]}\")\n",
    "        return player_info\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error fetching info for player ID {player_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_season_data_by_year(year):\n",
    "    \"\"\"Fetch player game logs data for a given starting year of the NBA season.\"\"\"\n",
    "    season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "    player_logs = playergamelogs.PlayerGameLogs(season_nullable=season).get_data_frames()[0]\n",
    "    player_logs['SEASON'] = season\n",
    "    player_logs['GAME_DATE'] = pd.to_datetime(player_logs['GAME_DATE'])\n",
    "    return player_logs\n",
    "\n",
    "def calculate_team_averages(season_data):\n",
    "    \"\"\"Calculate team averages for the relevant stats.\"\"\"\n",
    "    team_stats = (\n",
    "        season_data.groupby(['SEASON', 'TEAM_NAME'])[RELEVANT_STATS]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    games_played = season_data.groupby(['SEASON', 'TEAM_NAME'])['GAME_ID'].nunique().reset_index(name='GAMES_PLAYED')\n",
    "    team_stats = pd.merge(team_stats, games_played, on=['SEASON', 'TEAM_NAME'])\n",
    "\n",
    "    for stat in RELEVANT_STATS:\n",
    "        team_stats[f'{stat}_per_game'] = team_stats[stat] / team_stats['GAMES_PLAYED']\n",
    "    \n",
    "    team_stats['eFG%_per_game'] = (\n",
    "        (team_stats['FGM_per_game'] + 0.5 * team_stats['FG3M_per_game']) / team_stats['FGA_per_game']\n",
    "    )\n",
    "    \n",
    "    return team_stats\n",
    "\n",
    "def calculate_percentiles(stats_df, debug=False):\n",
    "    \"\"\"Calculate percentiles for the stats within each season.\"\"\"\n",
    "    for season in stats_df['SEASON'].unique():\n",
    "        season_data = stats_df[stats_df['SEASON'] == season]\n",
    "        for stat in RELEVANT_STATS + ['eFG%']:\n",
    "            stat_per_game = f'{stat}_per_game'\n",
    "            if stat_per_game in season_data.columns:\n",
    "                stats_df.loc[season_data.index, f'{stat}_percentile'] = season_data[stat_per_game].rank(pct=True)\n",
    "                if debug:\n",
    "                    print(f\"Calculated percentiles for {stat} in season {season}:\")\n",
    "                    print(stats_df.loc[season_data.index, [stat_per_game, f'{stat}_percentile']].head())\n",
    "    return stats_df\n",
    "\n",
    "def get_champion(season, debug=False):\n",
    "    \"\"\"Fetch the champion team for a given NBA season.\"\"\"\n",
    "    try:\n",
    "        games = leaguegamefinder.LeagueGameFinder(season_nullable=season, season_type_nullable='Playoffs').get_data_frames()[0]\n",
    "        games['GAME_DATE'] = pd.to_datetime(games['GAME_DATE'])\n",
    "        last_game = games.sort_values('GAME_DATE').iloc[-2:]\n",
    "        winner = last_game[last_game['WL'] == 'W'].iloc[0]\n",
    "        if debug:\n",
    "            print(f\"Champion for season {season}: {winner['TEAM_NAME']} ({winner['TEAM_ID']})\")\n",
    "        return winner['TEAM_NAME']\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error fetching champion for season {season}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_champions(start_year, end_year, debug=False):\n",
    "    \"\"\"Fetch champions for each season from start_year to end_year.\"\"\"\n",
    "    champions = {}\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "        champ_name = get_champion(season, debug)\n",
    "        if champ_name:\n",
    "            champions[season] = {'ChampionTeamName': champ_name}\n",
    "        elif debug:\n",
    "            print(f\"Champion data not available for season {season}\")\n",
    "        time.sleep(1)  # To avoid overwhelming the API\n",
    "    if debug:\n",
    "        print(f\"Champions data: {champions}\")\n",
    "    return champions\n",
    "\n",
    "def get_champion_team_stats(start_year, end_year):\n",
    "    \"\"\"Fetch and process champion team stats for the given range of years.\"\"\"\n",
    "    all_team_stats = pd.DataFrame()\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season_data = fetch_season_data_by_year(year)\n",
    "        team_stats = calculate_team_averages(season_data)\n",
    "        team_stats = calculate_percentiles(team_stats)\n",
    "        \n",
    "        # Identify the champion team\n",
    "        champ_name = get_champion(f\"{year}-{str(year+1)[-2:]}\")\n",
    "        if champ_name:\n",
    "            champ_stats = team_stats[team_stats['TEAM_NAME'] == champ_name]\n",
    "            all_team_stats = pd.concat([all_team_stats, champ_stats])\n",
    "\n",
    "    # Calculate the average stats for the champions (only numeric columns)\n",
    "    numeric_cols = all_team_stats.select_dtypes(include=[np.number]).columns\n",
    "    avg_stats = all_team_stats[numeric_cols].mean()\n",
    "\n",
    "    # Create a DataFrame for the average stats and append to the results\n",
    "    avg_stats_df = pd.DataFrame([avg_stats])\n",
    "    avg_stats_df['SEASON'] = 'Average'\n",
    "    avg_stats_df['TEAM_NAME'] = 'Average Champion'\n",
    "    \n",
    "    # Append the average row to the champion team data\n",
    "    all_team_stats = pd.concat([all_team_stats, avg_stats_df], ignore_index=True)\n",
    "    \n",
    "    return all_team_stats\n",
    "\n",
    "def calculate_team_stats(player_data, period, debug=False):\n",
    "    \"\"\"Calculate team-level statistics, including averages.\"\"\"\n",
    "    if debug:\n",
    "        print(f\"Calculating {period} team-level statistics.\")\n",
    "        print(\"Initial player_data head:\")\n",
    "        print(player_data.head())\n",
    "\n",
    "    # Calculate team-level stats by summing player stats for each team and season\n",
    "    team_stats = (\n",
    "        player_data.groupby(['SEASON', 'TEAM_NAME'])[RELEVANT_STATS]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate the number of games played by each team\n",
    "    games_played = player_data.groupby(['SEASON', 'TEAM_NAME'])['GAME_ID'].nunique().reset_index(name='GAMES_PLAYED')\n",
    "\n",
    "    # Merge games played with team stats\n",
    "    team_stats = pd.merge(team_stats, games_played, on=['SEASON', 'TEAM_NAME'])\n",
    "\n",
    "    # Calculate stats per game\n",
    "    for stat in RELEVANT_STATS:\n",
    "        team_stats[f'{stat}_per_game'] = team_stats[stat] / team_stats['GAMES_PLAYED']\n",
    "\n",
    "    # Add period column\n",
    "    team_stats['PERIOD'] = period\n",
    "\n",
    "    if debug:\n",
    "        print(f\"{period} team-level statistics head:\")\n",
    "        print(team_stats.head())\n",
    "\n",
    "    return team_stats\n",
    "\n",
    "def calculate_post_trade_team_stats(player_data, traded_players, trade_date, season_data, debug=False):\n",
    "    \"\"\"Calculate post-trade team-level statistics, using entire season if necessary.\"\"\"\n",
    "    if debug:\n",
    "        print(\"Calculating post-trade team-level statistics.\")\n",
    "\n",
    "    # Convert trade_date to datetime\n",
    "    trade_date = pd.to_datetime(trade_date)\n",
    "\n",
    "    # Determine the start of the season based on the SEASON column\n",
    "    season_start_year = int(player_data['SEASON'].iloc[0].split('-')[0])\n",
    "    season_start_date = pd.to_datetime(f\"{season_start_year}-10-01\")  # NBA season typically starts in October\n",
    "\n",
    "    # Determine whether to use entire season data or data after trade date\n",
    "    if trade_date < season_start_date:\n",
    "        if debug:\n",
    "            print(f\"Warning: Trade date {trade_date} is earlier than the start of the season {season_start_date}. Using entire season data.\")\n",
    "        post_trade_data = season_data  # Use the entire season data\n",
    "    else:\n",
    "        post_trade_data = player_data[player_data['GAME_DATE'] >= trade_date].copy()\n",
    "\n",
    "    if debug:\n",
    "        print(\"Post-trade player data head:\")\n",
    "        print(post_trade_data.head())\n",
    "\n",
    "    # Calculate post-trade stats\n",
    "    post_trade_stats = calculate_team_stats(post_trade_data, 'Post-trade', debug)\n",
    "\n",
    "    # Calculate traded players' post-trade averages\n",
    "    traded_player_stats = {}\n",
    "    for player_name, new_team_name in traded_players.items():\n",
    "        player_id = fetch_player_id_by_name(player_name, debug)\n",
    "        player_post_trade_stats = post_trade_data[post_trade_data['PLAYER_ID'] == player_id][RELEVANT_STATS].mean()\n",
    "        traded_player_stats[player_name] = player_post_trade_stats.to_dict()\n",
    "        if debug:\n",
    "            print(f\"{player_name} averages post-trade (to {new_team_name}): {traded_player_stats[player_name]}\")\n",
    "\n",
    "    # Adjust post-trade stats based on traded players\n",
    "    for player_name, new_team_name in traded_players.items():\n",
    "        player_id = fetch_player_id_by_name(player_name, debug)\n",
    "        old_team_name = player_data[player_data['PLAYER_ID'] == player_id]['TEAM_NAME'].iloc[0]\n",
    "        post_trade_games = post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == new_team_name, 'GAMES_PLAYED'].values[0]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nAdjusting stats for trade: {player_name} from {old_team_name} to {new_team_name}\")\n",
    "\n",
    "        # Remove player's stats from old team\n",
    "        for stat in RELEVANT_STATS:\n",
    "            # Ensure the column is a float before performing the operation\n",
    "            post_trade_stats[stat] = post_trade_stats[stat].astype(float)\n",
    "            if debug:\n",
    "                print(f\"  Before adjustment - {old_team_name} {stat}: {post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == old_team_name, stat].values[0]}\")\n",
    "            post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == old_team_name, stat] -= traded_player_stats[player_name][stat] * post_trade_games\n",
    "            if debug:\n",
    "                print(f\"  After adjustment - {old_team_name} {stat}: {post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == old_team_name, stat].values[0]}\")\n",
    "\n",
    "        # Add player's stats to new team\n",
    "        for stat in RELEVANT_STATS:\n",
    "            if debug:\n",
    "                print(f\"  Before adjustment - {new_team_name} {stat}: {post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == new_team_name, stat].values[0]}\")\n",
    "            post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == new_team_name, stat] += traded_player_stats[player_name][stat] * post_trade_games\n",
    "            if debug:\n",
    "                print(f\"  After adjustment - {new_team_name} {stat}: {post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == new_team_name, stat].values[0]}\")\n",
    "\n",
    "    # Recalculate per-game stats\n",
    "    for stat in RELEVANT_STATS:\n",
    "        post_trade_stats[f'{stat}_per_game'] = post_trade_stats[stat] / post_trade_stats['GAMES_PLAYED']\n",
    "\n",
    "    if debug:\n",
    "        print(\"Post-trade team stats calculated successfully.\")\n",
    "        print(\"Post-trade team stats head:\")\n",
    "        print(post_trade_stats.head())\n",
    "\n",
    "    return post_trade_stats\n",
    "\n",
    "\n",
    "def compare_team_performance(percentiles, champion_team_data, traded_teams, champion_filter='Average Champion', debug=False):\n",
    "    \"\"\"Generate a comparison table for team performance before and after trades.\"\"\"\n",
    "    if debug:\n",
    "        print(\"Comparing team performance:\")\n",
    "        print(\"Percentiles data head:\")\n",
    "        print(percentiles.head())\n",
    "    \n",
    "    # Filter for the selected champion\n",
    "    if champion_filter == 'Average Champion':\n",
    "        champion_row = champion_team_data[champion_team_data['TEAM_NAME'] == 'Average Champion'].iloc[0]\n",
    "    else:\n",
    "        champion_row = champion_team_data[(champion_team_data['SEASON'] == champion_filter) & (champion_team_data['TEAM_NAME'] != 'Average Champion')].iloc[0]\n",
    "\n",
    "    comparison_data = []\n",
    "    \n",
    "    for team in traded_teams:\n",
    "        pre_trade_stats = percentiles[(percentiles['TEAM_NAME'] == team) & (percentiles['PERIOD'] == 'Pre-trade')]\n",
    "        post_trade_stats = percentiles[(percentiles['TEAM_NAME'] == team) & (percentiles['PERIOD'] == 'Post-trade')]\n",
    "        \n",
    "        if not pre_trade_stats.empty and not post_trade_stats.empty:\n",
    "            team_comparison = {'Team': team}\n",
    "            for stat in RELEVANT_STATS + ['eFG%']:\n",
    "                team_comparison[f'{stat} Pre-trade'] = pre_trade_stats[f'{stat}_per_game'].values[0]\n",
    "                team_comparison[f'{stat} Pre-trade Percentile'] = pre_trade_stats[f'{stat}_percentile'].values[0]\n",
    "                team_comparison[f'{stat} Post-trade'] = post_trade_stats[f'{stat}_per_game'].values[0]\n",
    "                team_comparison[f'{stat} Post-trade Percentile'] = post_trade_stats[f'{stat}_percentile'].values[0]\n",
    "                team_comparison[f'{stat} Champion'] = champion_row[f'{stat}_per_game']\n",
    "                team_comparison[f'{stat} Champion Percentile'] = champion_row[f'{stat}_percentile']\n",
    "            \n",
    "            comparison_data.append(team_comparison)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"No data available for comparison for {team}.\")\n",
    "                print(\"Pre-trade stats head:\")\n",
    "                print(pre_trade_stats.head())\n",
    "                print(\"Post-trade stats head:\")\n",
    "                print(post_trade_stats.head())\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nComparison Results:\")\n",
    "        print(tabulate(comparison_df, headers='keys', tablefmt='grid'))\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def validate_post_trade_stats(player_data, trade_date, traded_teams, post_trade_stats, debug=False):\n",
    "    \"\"\"Validate the post-trade statistics calculation.\"\"\"\n",
    "    trade_date = pd.to_datetime(trade_date)\n",
    "    post_trade_data = player_data[player_data['GAME_DATE'] >= trade_date]\n",
    "\n",
    "    validation_results = {}\n",
    "\n",
    "    for team in traded_teams:\n",
    "        team_data = post_trade_data[post_trade_data['TEAM_NAME'] == team]\n",
    "        \n",
    "        total_points = team_data['PTS'].sum()\n",
    "        games_played = team_data['GAME_ID'].nunique()\n",
    "        calculated_ppg = total_points / games_played if games_played > 0 else 0\n",
    "\n",
    "        reported_ppg = post_trade_stats[post_trade_stats['TEAM_NAME'] == team]['PTS_per_game'].values[0]\n",
    "\n",
    "        validation_results[team] = {\n",
    "            'Calculated PPG': calculated_ppg,\n",
    "            'Reported PPG': reported_ppg,\n",
    "            'Difference': calculated_ppg - reported_ppg,\n",
    "            'Games Played': games_played\n",
    "        }\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nPost-Trade Statistics Validation:\")\n",
    "        print(tabulate(pd.DataFrame(validation_results).T, headers='keys', tablefmt='grid'))\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "def fetch_and_process_champion_data(start_year, end_year, debug=False):\n",
    "    \"\"\"Fetch and process champion data for the given range of years.\"\"\"\n",
    "    champions = get_champions(start_year, end_year, debug)\n",
    "    champion_team_data = get_champion_team_stats(start_year, end_year)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nChampion Team Stats with Average:\")\n",
    "        print(tabulate(champion_team_data, headers='keys', tablefmt='grid'))\n",
    "    \n",
    "    return champion_team_data\n",
    "\n",
    "def fetch_and_process_player_data(start_year, end_year, debug=False):\n",
    "    \"\"\"Fetch and combine player data for the given range of years.\"\"\"\n",
    "    player_data = pd.DataFrame()\n",
    "    season_data = pd.DataFrame()  # To store the full season data\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        data = fetch_season_data_by_year(year)\n",
    "        if data is not None:\n",
    "            player_data = pd.concat([player_data, data], ignore_index=True)\n",
    "            season_data = player_data  # Assuming season_data should hold the entire season's data\n",
    "    \n",
    "    if player_data.empty:\n",
    "        raise ValueError(\"Failed to fetch player data.\")\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nFetched Player Data:\")\n",
    "        print(player_data.head())\n",
    "    \n",
    "    return player_data, season_data\n",
    "\n",
    "def calculate_combined_team_stats(player_data, trade_date, traded_players, season_data, debug=False):\n",
    "    \"\"\"Calculate combined pre-trade and post-trade team statistics.\"\"\"\n",
    "    trade_date = pd.to_datetime(trade_date)\n",
    "\n",
    "    if player_data['GAME_DATE'].min() > trade_date:\n",
    "        pre_trade_team_stats = calculate_team_stats(player_data, 'Pre-trade', debug)\n",
    "    else:\n",
    "        pre_trade_team_stats = calculate_team_stats(player_data[player_data['GAME_DATE'] < trade_date], 'Pre-trade', debug)\n",
    "        \n",
    "    post_trade_team_stats = calculate_post_trade_team_stats(player_data, traded_players, trade_date, season_data, debug)\n",
    "    \n",
    "    combined_stats = pd.concat([pre_trade_team_stats, post_trade_team_stats], ignore_index=True)\n",
    "    \n",
    "    combined_stats['eFG%_per_game'] = (\n",
    "        (combined_stats['FGM_per_game'] + 0.5 * combined_stats['FG3M_per_game']) / combined_stats['FGA_per_game']\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nCombined Team Stats:\")\n",
    "        print(tabulate(combined_stats, headers='keys', tablefmt='grid'))\n",
    "    \n",
    "    return combined_stats\n",
    "\n",
    "def trade_impact_analysis(start_year, end_year, trade_date, traded_players, champion_filter='Average Champion', debug=False):\n",
    "    \"\"\"Perform trade impact analysis and return the comparison table.\"\"\"\n",
    "    # Fetch and process data\n",
    "    champion_team_data = fetch_and_process_champion_data(start_year, end_year, debug)\n",
    "    player_data, season_data = fetch_and_process_player_data(start_year, end_year, debug)\n",
    "    \n",
    "    # Calculate combined team stats\n",
    "    combined_stats = calculate_combined_team_stats(player_data, trade_date, traded_players, season_data, debug)\n",
    "    \n",
    "    # Calculate percentiles for combined stats\n",
    "    percentiles = calculate_percentiles(combined_stats, debug)\n",
    "    \n",
    "    # Compare pre-trade and post-trade stats for traded teams\n",
    "    traded_teams = list(set([team_name for _, team_name in traded_players.items()]))\n",
    "    comparison_table = compare_team_performance(percentiles, champion_team_data, traded_teams, champion_filter, debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nTrade Impact Comparison:\")\n",
    "        print(tabulate(comparison_table, headers='keys', tablefmt='grid'))\n",
    "    \n",
    "    return comparison_table\n",
    "\n",
    "def main(debug=True):\n",
    "    start_year = 2023\n",
    "    end_year = 2023\n",
    "    trade_date = '2023-4-20'  # Example trade date\n",
    "    \n",
    "    # Traded players with new team names\n",
    "    traded_players = {\n",
    "        'Jayson Tatum': 'Boston Celtics',  # Example Player and new team\n",
    "        'Devin Booker': 'Phoenix Suns'     # Example Player and new team\n",
    "    }\n",
    "    \n",
    "    # Perform trade impact analysis\n",
    "    comparison_table = trade_impact_analysis(\n",
    "        start_year, end_year, trade_date, traded_players, \n",
    "        champion_filter='Average Champion', debug=debug\n",
    "    )\n",
    "    \n",
    "    # Print the comparison table\n",
    "    print(comparison_table)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/nba_rules_trade_impact.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/nba_rules_trade_impact.py\n",
    "# https://www.hoopsrumors.com/2023/09/salary-matching-rules-for-trades-during-2023-24-season.html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Constants for the 2023/24 season\n",
    "FIRST_TAX_APRON_2023 = 172_346_000\n",
    "SALARY_CAP_2023 = 136_021_000\n",
    "\n",
    "# Percentages based on rules\n",
    "UP_TO_7500K_MULTIPLIER = 2.0\n",
    "UP_TO_7500K_BONUS = 250_000 / SALARY_CAP_2023\n",
    "\n",
    "BETWEEN_7501K_AND_29M_BONUS = 7_500_000 / SALARY_CAP_2023\n",
    "\n",
    "ABOVE_29M_MULTIPLIER = 1.25\n",
    "ABOVE_29M_BONUS = 250_000 / SALARY_CAP_2023\n",
    "\n",
    "ABOVE_FIRST_APRON_MULTIPLIER = 1.10\n",
    "\n",
    "def check_salary_matching_rules(outgoing_salary, incoming_salary, team_salary_before_trade, salary_cap, first_tax_apron, debug=False):\n",
    "    debug_info = []\n",
    "    if debug:\n",
    "        debug_info.append(f\"Debug: Checking salary matching rules:\")\n",
    "        debug_info.append(f\"  Outgoing Salary: ${outgoing_salary:,.2f}\")\n",
    "        debug_info.append(f\"  Incoming Salary: ${incoming_salary:,.2f}\")\n",
    "        debug_info.append(f\"  Team Salary Before Trade: ${team_salary_before_trade:,.2f}\")\n",
    "        debug_info.append(f\"  Salary Cap: ${salary_cap:,.2f}\")\n",
    "        debug_info.append(f\"  First Tax Apron: ${first_tax_apron:,.2f}\")\n",
    "\n",
    "    if team_salary_before_trade < first_tax_apron:\n",
    "        if outgoing_salary <= 7_500_000:\n",
    "            max_incoming_salary = (UP_TO_7500K_MULTIPLIER * outgoing_salary + UP_TO_7500K_BONUS * salary_cap)\n",
    "            rule = \"200% of outgoing + 250,000 (up to 7,500,000)\"\n",
    "            percentage_limit = (UP_TO_7500K_MULTIPLIER * outgoing_salary + UP_TO_7500K_BONUS * salary_cap) / outgoing_salary\n",
    "        elif outgoing_salary <= 29_000_000:\n",
    "            max_incoming_salary = outgoing_salary + BETWEEN_7501K_AND_29M_BONUS * salary_cap\n",
    "            rule = \"outgoing + 7,500,000 (7,500,001 to 29,000,000)\"\n",
    "            percentage_limit = (outgoing_salary + BETWEEN_7501K_AND_29M_BONUS * salary_cap) / outgoing_salary\n",
    "        else:\n",
    "            max_incoming_salary = (ABOVE_29M_MULTIPLIER * outgoing_salary + ABOVE_29M_BONUS * salary_cap)\n",
    "            rule = \"125% of outgoing + 250,000 (above 29,000,000)\"\n",
    "            percentage_limit = (ABOVE_29M_MULTIPLIER * outgoing_salary + ABOVE_29M_BONUS * salary_cap) / outgoing_salary\n",
    "    else:\n",
    "        max_incoming_salary = ABOVE_FIRST_APRON_MULTIPLIER * outgoing_salary\n",
    "        rule = \"110% of outgoing (above first tax apron)\"\n",
    "        percentage_limit = ABOVE_FIRST_APRON_MULTIPLIER\n",
    "\n",
    "    if debug:\n",
    "        debug_info.append(f\"  Max Incoming Salary Allowed: ${max_incoming_salary:,.2f}\")\n",
    "        debug_info.append(f\"  Rule Applied: {rule}\")\n",
    "        debug_info.append(f\"  Percentage Limit: {percentage_limit:.2f}\")\n",
    "\n",
    "    return incoming_salary <= max_incoming_salary, max_incoming_salary, rule, percentage_limit, \"\\n\".join(debug_info)\n",
    "\n",
    "def analyze_trade_scenario(players1, players2, predictions_df, season, debug=False):\n",
    "    debug_info = []\n",
    "    \n",
    "    # Filter the dataframe for the specified season\n",
    "    season_data = predictions_df[predictions_df['Season'] == season]\n",
    "\n",
    "    # Ensure all players in each list are from the same team\n",
    "    teams1 = season_data[season_data['Player'].isin(players1)]['Team'].unique()\n",
    "    teams2 = season_data[season_data['Player'].isin(players2)]['Team'].unique()\n",
    "\n",
    "    if len(teams1) != 1 or len(teams2) != 1:\n",
    "        return None, \"Error: All players in each list must be from the same team.\"\n",
    "\n",
    "    team1 = teams1[0]\n",
    "    team2 = teams2[0]\n",
    "\n",
    "    if team1 == team2:\n",
    "        return None, \"Error: The two teams involved in the trade must be different.\"\n",
    "\n",
    "    # Calculate total salaries for each group of players\n",
    "    outgoing_salary_team1 = season_data[season_data['Player'].isin(players1)]['Salary'].sum()\n",
    "    incoming_salary_team1 = season_data[season_data['Player'].isin(players2)]['Salary'].sum()\n",
    "\n",
    "    outgoing_salary_team2 = season_data[season_data['Player'].isin(players2)]['Salary'].sum()\n",
    "    incoming_salary_team2 = season_data[season_data['Player'].isin(players1)]['Salary'].sum()\n",
    "\n",
    "    # Check salary matching rules for both teams\n",
    "    team1_salary_before_trade = season_data[season_data['Team'] == team1]['Salary'].sum()\n",
    "    team2_salary_before_trade = season_data[season_data['Team'] == team2]['Salary'].sum()\n",
    "\n",
    "    # Determine tax apron status\n",
    "    team1_tax_apron_status = \"Below\" if team1_salary_before_trade < FIRST_TAX_APRON_2023 else \"Above\"\n",
    "    team2_tax_apron_status = \"Below\" if team2_salary_before_trade < FIRST_TAX_APRON_2023 else \"Above\"\n",
    "\n",
    "    trade_works_for_team1, team1_max_incoming_salary, team1_rule, team1_percentage_limit, team1_debug = check_salary_matching_rules(\n",
    "        outgoing_salary_team1, incoming_salary_team1, team1_salary_before_trade, SALARY_CAP_2023, FIRST_TAX_APRON_2023, debug\n",
    "    )\n",
    "    trade_works_for_team2, team2_max_incoming_salary, team2_rule, team2_percentage_limit, team2_debug = check_salary_matching_rules(\n",
    "        outgoing_salary_team2, incoming_salary_team2, team2_salary_before_trade, SALARY_CAP_2023, FIRST_TAX_APRON_2023, debug\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        debug_info.append(team1_debug)\n",
    "        debug_info.append(team2_debug)\n",
    "        debug_info.append(\"\\nDebug: Trade Analysis Results:\")\n",
    "        debug_info.append(f\"Team 1 ({team1}):\")\n",
    "        debug_info.append(f\"  Total Outgoing Salary: ${outgoing_salary_team1:,.2f}\")\n",
    "        debug_info.append(f\"  Max Incoming Salary Allowed: ${team1_max_incoming_salary:,.2f} (Rule: {team1_rule})\")\n",
    "        debug_info.append(f\"  Percentage Limit: {team1_percentage_limit:.2f}\")\n",
    "        debug_info.append(f\"Team 2 ({team2}):\")\n",
    "        debug_info.append(f\"  Total Outgoing Salary: ${outgoing_salary_team2:,.2f}\")\n",
    "        debug_info.append(f\"  Max Incoming Salary Allowed: ${team2_max_incoming_salary:,.2f} (Rule: {team2_rule})\")\n",
    "        debug_info.append(f\"  Percentage Limit: {team2_percentage_limit:.2f}\")\n",
    "\n",
    "    trade_status = True\n",
    "    if not trade_works_for_team1:\n",
    "        debug_info.append(f\"Trade Works for Team 1: No\")\n",
    "        debug_info.append(f\"  Trade fails for Team 1 because incoming salary exceeds max allowed under rule: {team1_rule}\")\n",
    "        debug_info.append(f\"  Team 1 is {team1_tax_apron_status} the First Tax Apron.\")\n",
    "        trade_status = False\n",
    "    else:\n",
    "        debug_info.append(f\"Trade Works for Team 1: Yes\")\n",
    "\n",
    "    if not trade_works_for_team2:\n",
    "        debug_info.append(f\"Trade Works for Team 2: No\")\n",
    "        debug_info.append(f\"  Trade fails for Team 2 because incoming salary exceeds max allowed under rule: {team2_rule}\")\n",
    "        debug_info.append(f\"  Team 2 is {team2_tax_apron_status} the First Tax Apron.\")\n",
    "        trade_status = False\n",
    "    else:\n",
    "        debug_info.append(f\"Trade Works for Team 2: Yes\")\n",
    "\n",
    "    if trade_status:\n",
    "        debug_info.append(\"The trade is valid according to salary matching rules.\")\n",
    "    else:\n",
    "        debug_info.append(\"The trade does not satisfy salary matching rules.\")\n",
    "\n",
    "    return trade_status, \"\\n\".join(debug_info)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the real predictions dataframe\n",
    "    predictions_df = pd.read_csv('data/processed/predictions_df.csv')\n",
    "\n",
    "    # Specify two lists of players for the trade scenario\n",
    "    players1 = [\"Anthony Davis\", \"LeBron James\"]\n",
    "    players2 = [\"Jayson Tatum\", \"Jaylen Brown\"]\n",
    "\n",
    "    # Analyze the trade scenario for the specified season with debugging enabled\n",
    "    season = 2023\n",
    "    print(f\"Analyzing trade for the {season} season:\")\n",
    "    results, debug_output = analyze_trade_scenario(players1, players2, predictions_df, season, debug=True)\n",
    "    print(\"results =\",debug_output)\n",
    "    print(\"results =\", results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/app_test_trade_impact.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/app_test_trade_impact.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nba_api.stats.endpoints import leaguegamefinder, playergamelogs, commonplayerinfo\n",
    "from nba_api.stats.static import teams, players\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "import streamlit as st\n",
    "\n",
    "# Importing the functions from the respective scripts\n",
    "from percentile_count_trade_impact import (\n",
    "    fetch_all_player_data,\n",
    "    calculate_player_stats,\n",
    "    calculate_player_percentiles,  \n",
    "    simulate_trade,\n",
    "    create_comparison_table,\n",
    "    get_champion_percentiles,\n",
    "    generate_comparison_tables\n",
    ")\n",
    "\n",
    "from overall_team_trade_impact import (\n",
    "    fetch_and_process_player_data,\n",
    "    calculate_combined_team_stats,\n",
    "    trade_impact_analysis\n",
    ")\n",
    "\n",
    "from nba_rules_trade_impact import (\n",
    "    check_salary_matching_rules,\n",
    "    analyze_trade_scenario\n",
    ")\n",
    "\n",
    "# Function to fetch players for a specific team\n",
    "def get_players_for_team(team_name, season=\"2023-24\"):\n",
    "    \"\"\"Fetch players for a given team name.\"\"\"\n",
    "    team_id = teams.find_teams_by_full_name(team_name)[0]['id']\n",
    "    team_players = playergamelogs.PlayerGameLogs(season_nullable=season).get_data_frames()[0]\n",
    "    team_players = team_players[team_players['TEAM_ID'] == team_id]\n",
    "    return sorted(team_players['PLAYER_NAME'].unique())\n",
    "\n",
    "def analyze_player_salaries(players, predictions_df):\n",
    "    \"\"\"Analyze if the selected players are overpaid or underpaid based on predicted salaries.\"\"\"\n",
    "    player_salary_analysis = []\n",
    "\n",
    "    for player in players:\n",
    "        player_data = predictions_df[predictions_df['Player'] == player]\n",
    "        if not player_data.empty:\n",
    "            actual_salary = player_data['Salary'].values[0]\n",
    "            salary_cap = player_data['Salary_Cap_Inflated'].values[0]\n",
    "            predicted_salary = player_data['Predicted_Salary'].values[0] * salary_cap\n",
    "            difference = actual_salary - predicted_salary\n",
    "            status = \"Overpaid\" if difference > 0 else \"Underpaid\" if difference < 0 else \"Fairly Paid\"\n",
    "            player_salary_analysis.append({\n",
    "                'Player': player,\n",
    "                'Actual Salary': actual_salary,\n",
    "                'Predicted Salary': predicted_salary,\n",
    "                'Difference': difference,\n",
    "                'Status': status\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(player_salary_analysis)\n",
    "\n",
    "\n",
    "# Main function to analyze trade impact\n",
    "def analyze_trade_impact(\n",
    "    traded_players, \n",
    "    trade_date, \n",
    "    percentile_seasons=None, \n",
    "    debug=False\n",
    "):\n",
    "    if percentile_seasons is None:\n",
    "        percentile_seasons = [\"2014-15\", \"2015-16\", \"2016-17\", \"2017-18\", \"2018-19\", \"2019-20\", \"2020-21\", \"2021-22\", \"2022-23\", \"2023-24\"]\n",
    "\n",
    "    teams_involved = list(set(traded_players.values()))\n",
    "    if len(teams_involved) != 2:\n",
    "        raise ValueError(\"This function supports trades involving exactly two teams.\")\n",
    "\n",
    "    team_a_name = teams_involved[0]\n",
    "    team_b_name = teams_involved[1]\n",
    "    players_from_team_a = [player for player, team_name in traded_players.items() if team_name == team_a_name]\n",
    "    players_from_team_b = [player for player, team_name in traded_players.items() if team_name == team_b_name]\n",
    "\n",
    "    overall_trade_start_year = int(min(season.split('-')[0] for season in percentile_seasons))\n",
    "    overall_trade_end_year = int(trade_date.split('-')[0])\n",
    "\n",
    "    # Step 1: Champion Percentile Analysis for Specific Seasons\n",
    "    average_top_percentiles_df = get_champion_percentiles(percentile_seasons, debug)\n",
    "\n",
    "    # Generate comparison tables before and after the trade\n",
    "    celtics_comparison_table, warriors_comparison_table = generate_comparison_tables(\n",
    "        percentile_seasons[-1], team_a_name, team_b_name, players_from_team_a, players_from_team_b, average_top_percentiles_df, debug\n",
    "    )\n",
    "\n",
    "    # Step 2: Overall Trade Impact Analysis\n",
    "    comparison_table = trade_impact_analysis(\n",
    "        overall_trade_start_year, overall_trade_end_year, trade_date, traded_players, \n",
    "        champion_filter='Average Champion', debug=debug\n",
    "    )\n",
    "\n",
    "    # Step 3: Trade Scenario Analysis Based on Salary Matching Rules\n",
    "    predictions_df = pd.read_csv('data/processed/predictions_df.csv')\n",
    "    salary_check_season = overall_trade_end_year\n",
    "    trade_scenario_results, trade_scenario_debug = analyze_trade_scenario(players_from_team_a, players_from_team_b, predictions_df, salary_check_season, debug=debug)\n",
    "\n",
    "    # Overpaid/Underpaid analysis\n",
    "    predictions_df = pd.read_csv('data/processed/predictions_df.csv')\n",
    "    all_players = list(traded_players.keys())\n",
    "    salary_analysis_df = analyze_player_salaries(all_players, predictions_df)\n",
    "\n",
    "    return {\n",
    "        'celtics_comparison_table': celtics_comparison_table,\n",
    "        'warriors_comparison_table': warriors_comparison_table,\n",
    "        'overall_comparison': comparison_table,\n",
    "        'trade_scenario_results': trade_scenario_results,\n",
    "        'trade_scenario_debug': trade_scenario_debug,\n",
    "        'salary_analysis': salary_analysis_df\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage with input variables\n",
    "\n",
    "    traded_players = {\n",
    "        'Jaylen Brown': 'Boston Celtics',\n",
    "        'Jayson Tatum': 'Boston Celtics',\n",
    "        'Stephen Curry': 'Golden State Warriors',\n",
    "        'Klay Thompson': 'Golden State Warriors'\n",
    "    }\n",
    "\n",
    "    trade_date = '2023-12-20'\n",
    "    percentile_seasons = [\"2022-23\", \"2023-24\"]\n",
    "    \n",
    "    results = analyze_trade_impact(traded_players, trade_date, percentile_seasons, debug=True)\n",
    "\n",
    "    # Print the results including the debug output\n",
    "    print(results['trade_scenario_debug'])\n",
    "    print(\"results =\", results)\n",
    "\n",
    "\n",
    "# Example usage in a Streamlit app:\n",
    "\n",
    "# import streamlit as st\n",
    "\n",
    "# # User inputs for the Streamlit app\n",
    "# st.title(\"Trade Impact Analysis\")\n",
    "\n",
    "# # Step 1: Select Teams\n",
    "# st.header(\"Select Teams Involved in the Trade\")\n",
    "# team_names = sorted([team['full_name'] for team in teams.get_teams()])\n",
    "# selected_teams = st.multiselect(\"Select up to 4 teams\", team_names, default=[\"Boston Celtics\", \"Golden State Warriors\"], max_selections=4)\n",
    "\n",
    "# # Step 2: Select Players from each Team\n",
    "# traded_players = {}\n",
    "# for team in selected_teams:\n",
    "#     st.subheader(f\"Select Players from {team}\")\n",
    "#     players = get_players_for_team(team)\n",
    "#     selected_players = st.multiselect(f\"Select players from {team}\", players, max_selections=4)\n",
    "#     for player in selected_players:\n",
    "#         traded_players[player] = team\n",
    "\n",
    "# # Step 3: Select Trade Date\n",
    "# trade_date = st.date_input(\"Trade Date\", value=pd.to_datetime(\"2023-12-20\"))\n",
    "\n",
    "# # Step 4: Select Number of Seasons for Percentile Analysis\n",
    "# num_seasons = st.slider(\"Number of Seasons for Analysis\", 5, 20, 10)\n",
    "# start_season = st.text_input(\"Starting Season (e.g., 2014-15)\", \"2014-15\")\n",
    "# percentile_seasons = [f\"{int(start_season.split('-')[0]) + i}-{str(int(start_season.split('-')[0]) + i + 1)[-2:]}\" for i in range(num_seasons)]\n",
    "\n",
    "# # Step 5: Run the Analysis\n",
    "# if st.button(\"Run Analysis\"):\n",
    "#     analyze_trade_impact(traded_players, trade_date.strftime('%Y-%m-%d'), percentile_seasons, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/app.py\n",
    "\n",
    "import os\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "from nba_api.stats.static import teams, players\n",
    "\n",
    "# Import functions from other modules\n",
    "from data_loader_preprocessor import load_data, format_season, clean_data, engineer_features, encode_data\n",
    "from model_trainer import train_and_save_models, evaluate_models\n",
    "from model_predictor import predict\n",
    "\n",
    "# Import functions from app_test_trade_impact.py\n",
    "from app_test_trade_impact import analyze_trade_impact, get_players_for_team\n",
    "\n",
    "# Importing Shot Chart Analysis functions\n",
    "from shot_chart.nba_helpers import get_team_abbreviation, categorize_shot, get_all_court_areas\n",
    "from shot_chart.nba_shots import fetch_shots_data, fetch_defensive_shots_data, fetch_shots_for_multiple_players\n",
    "from shot_chart.nba_plotting import plot_shot_chart_hexbin\n",
    "from shot_chart.nba_efficiency import create_mae_table, save_mae_table, load_mae_table, get_seasons_range, calculate_compatibility_between_players\n",
    "from shot_chart.shot_chart_main import run_scenario, preload_mae_tables, create_and_save_mae_table_specific, create_and_save_mae_table_all\n",
    "\n",
    "@st.cache_data\n",
    "def get_teams_list():\n",
    "    \"\"\"Get the list of NBA teams.\"\"\"\n",
    "    return [team['full_name'] for team in teams.get_teams()]\n",
    "\n",
    "@st.cache_data\n",
    "def get_players_list():\n",
    "    \"\"\"Get the list of NBA players.\"\"\"\n",
    "    return [player['full_name'] for player in players.get_players()]\n",
    "\n",
    "@st.cache_data\n",
    "def load_team_data():\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_df = pd.DataFrame(nba_teams)\n",
    "    return team_df[['id', 'full_name', 'abbreviation']]\n",
    "\n",
    "@st.cache_data\n",
    "def load_player_data(start_year, end_year):\n",
    "    player_data = pd.DataFrame()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        data = fetch_season_data_by_year(year)\n",
    "        if data is not None:\n",
    "            player_data = pd.concat([player_data, data], ignore_index=True)\n",
    "    return player_data\n",
    "\n",
    "def identify_overpaid_underpaid(predictions_df):\n",
    "    # Adjust Predicted_Salary calculation\n",
    "    predictions_df['Predicted_Salary'] = predictions_df['Predicted_Salary'] * predictions_df['Salary_Cap_Inflated']\n",
    "    \n",
    "    predictions_df['Salary_Difference'] = predictions_df['Salary'] - predictions_df['Predicted_Salary']\n",
    "    predictions_df['Overpaid'] = predictions_df['Salary_Difference'] > 0\n",
    "    predictions_df['Underpaid'] = predictions_df['Salary_Difference'] < 0\n",
    "    \n",
    "    overpaid = predictions_df[predictions_df['Overpaid']].sort_values('Salary_Difference', ascending=False)\n",
    "    underpaid = predictions_df[predictions_df['Underpaid']].sort_values('Salary_Difference')\n",
    "    \n",
    "    return overpaid.head(10), underpaid.head(10)\n",
    "\n",
    "# Utility functions\n",
    "def load_processed_data(file_path):\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    data = clean_data(data)\n",
    "    data = engineer_features(data)\n",
    "    return data\n",
    "\n",
    "def filter_data_by_season(data, season):\n",
    "    return data[data['Season'] == season]\n",
    "\n",
    "# Data visualization functions\n",
    "def plot_feature_distribution(data, feature):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.histplot(data[feature], kde=True, ax=ax)\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_heatmap(data):\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    corr = numeric_data.corr()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm', ax=ax)\n",
    "    ax.set_title('Correlation Heatmap')\n",
    "    return fig\n",
    "\n",
    "# Model metrics function\n",
    "def display_model_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    st.subheader(\"Model Performance Metrics\")\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    col1.metric(\"Mean Squared Error\", f\"{mse:.4f}\")\n",
    "    col2.metric(\"Root Mean Squared Error\", f\"{rmse:.4f}\")\n",
    "    col3.metric(\"Mean Absolute Error\", f\"{mae:.4f}\")\n",
    "    col4.metric(\"R-squared\", f\"{r2:.4f}\")\n",
    "\n",
    "def display_overpaid_underpaid(predictions_df):\n",
    "    st.subheader(\"Top 10 Overpaid and Underpaid Players\")\n",
    "\n",
    "    # Add filters\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        team_filter = st.multiselect(\"Filter by Team\", options=sorted(predictions_df['Team'].unique()))\n",
    "    with col2:\n",
    "        position_filter = st.multiselect(\"Filter by Position\", options=sorted(predictions_df['Position'].unique()))\n",
    "\n",
    "    # Apply filters\n",
    "    filtered_df = predictions_df\n",
    "    if team_filter:\n",
    "        filtered_df = filtered_df[filtered_df['Team'].isin(team_filter)]\n",
    "    if position_filter:\n",
    "        filtered_df = filtered_df[filtered_df['Position'].isin(position_filter)]\n",
    "\n",
    "    # Identify overpaid and underpaid players\n",
    "    overpaid, underpaid = identify_overpaid_underpaid(filtered_df)\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.subheader(\"Top 10 Overpaid Players\")\n",
    "        st.dataframe(overpaid[['Player', 'Team', 'Position', 'Salary', 'Predicted_Salary', 'Salary_Difference']])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Top 10 Underpaid Players\")\n",
    "        st.dataframe(underpaid[['Player', 'Team', 'Position', 'Salary', 'Predicted_Salary', 'Salary_Difference']])\n",
    "\n",
    "\n",
    "# Trade Impact Simulator function\n",
    "def display_trade_impact(results, team_a_name, team_b_name):\n",
    "    st.subheader(f\"Percentile Counts Impact on {team_a_name} Compared to Average Champion Percentiles\")\n",
    "    st.table(results['celtics_comparison_table'])\n",
    "\n",
    "    st.subheader(f\"Percentile Counts Impact on {team_b_name} Compared to Average Champion Percentiles\")\n",
    "    st.table(results['warriors_comparison_table'])\n",
    "\n",
    "    st.subheader(\"Pre vs Post Trade Impact\")\n",
    "    st.table(results['overall_comparison'])\n",
    "\n",
    "    st.subheader(\"Salary Analysis: Overpaid vs Underpaid\")\n",
    "    st.table(results['salary_analysis'])\n",
    "\n",
    "    st.subheader(\"Salary Cap Clearance: (debug shows Salary cap vs Salary Help)\")\n",
    "    st.write(results['trade_scenario_results'])\n",
    "\n",
    "\n",
    "\n",
    "def trade_impact_simulator():\n",
    "    st.header(\"Trade Impact Simulator\")\n",
    "\n",
    "    teams = load_team_data()\n",
    "    team_a_name = st.selectbox(\"Select Team A\", teams['full_name'])\n",
    "    team_b_name = st.selectbox(\"Select Team B\", teams['full_name'][teams['full_name'] != team_a_name].tolist())\n",
    "\n",
    "    players_from_team_a = st.multiselect(f\"Select Players from {team_a_name}\", get_players_for_team(team_a_name))\n",
    "    players_from_team_b = st.multiselect(f\"Select Players from {team_b_name}\", get_players_for_team(team_b_name))\n",
    "\n",
    "    trade_date = st.date_input(\"Trade Date\", value=pd.to_datetime(\"2023-12-20\"))\n",
    "\n",
    "    percentile_seasons = st.multiselect(\"Select Average Champion Comparison Percentiles Seasons (default: 10):\", \n",
    "                                        [\"2014-15\", \"2015-16\", \"2016-17\", \"2017-18\", \n",
    "                                         \"2018-19\", \"2019-20\", \"2020-21\", \"2021-22\", \"2022-23\", \"2023-24\"],\n",
    "                                        default=[\"2014-15\", \"2015-16\", \"2016-17\", \"2017-18\", \n",
    "                                         \"2018-19\", \"2019-20\", \"2020-21\", \"2021-22\", \"2022-23\", \"2023-24\"])\n",
    "\n",
    "    if st.button(\"Simulate Trade Impact\"):\n",
    "        if players_from_team_a and players_from_team_b:\n",
    "            traded_players = {player: team_a_name for player in players_from_team_a}\n",
    "            traded_players.update({player: team_b_name for player in players_from_team_b})\n",
    "\n",
    "            # Call the analysis function and capture the results\n",
    "            results = analyze_trade_impact(traded_players, trade_date.strftime('%Y-%m-%d'), percentile_seasons, debug=True)\n",
    "\n",
    "            if results:\n",
    "                # Display all results in the app\n",
    "                display_trade_impact(results, team_a_name, team_b_name)\n",
    "\n",
    "                # MAE Analysis section\n",
    "                st.subheader(\"Player Shooting Area Compatibility (MAE Analysis)\")\n",
    "\n",
    "                # Combine the players from both teams\n",
    "                all_players = players_from_team_a + players_from_team_b\n",
    "\n",
    "                # Fetch shots for the selected players\n",
    "                player_shots = fetch_shots_for_multiple_players(all_players, season='2023-24', court_areas='all')\n",
    "\n",
    "                # Calculate compatibility\n",
    "                compatibility_df = calculate_compatibility_between_players(player_shots)\n",
    "\n",
    "                # Display the MAE table\n",
    "                st.write(compatibility_df)\n",
    "\n",
    "                # Salary Cap Clearance section\n",
    "                st.subheader(\"Salary Cap Clearance: (debug shows Salary cap vs Salary Help)\")\n",
    "                st.write(results['trade_scenario_results'])\n",
    "\n",
    "                # Display debug output\n",
    "                st.subheader(\"Debug Information\")\n",
    "                st.text_area(\"Detailed Debug Output\", results['trade_scenario_debug'], height=300)\n",
    "\n",
    "                st.success(\"Trade Impact Simulation Completed\")\n",
    "            else:\n",
    "                st.error(\"Trade impact analysis failed. Please check the inputs.\")\n",
    "        else:\n",
    "            st.error(\"Please select players from both teams to simulate the trade impact.\")\n",
    "\n",
    "\n",
    "# Shot Chart Analysis function\n",
    "def shot_chart_analysis():\n",
    "    st.header(\"Shot Chart Analysis\")\n",
    "\n",
    "    # Add guidelines and purpose explanation at the top\n",
    "    st.markdown(\"\"\"\n",
    "    ### Welcome to the NBA Shot Analysis App!\n",
    "    \n",
    "    This app allows you to analyze the offensive and defensive efficiency of NBA teams and players. \n",
    "    You can compare players or teams to identify the most efficient spots on the court, \n",
    "    analyze player compatibility based on shot area efficiency, and much more.\n",
    "    \n",
    "    **Options and Guidelines:**\n",
    "    - **Analysis Type**: Choose between offensive, defensive, or both types of analysis.\n",
    "    - **Team or Player**: Analyze a team or an individual player.\n",
    "    - **Court Areas**: Select specific court areas or analyze all areas.\n",
    "    - **Comparison**: Compare multiple players to see how their offensive efficiencies align or differ.\n",
    "    \"\"\")\n",
    "\n",
    "    analysis_type = st.selectbox(\"Select analysis type\", options=[\"offensive\", \"defensive\", \"both\"])\n",
    "\n",
    "    entity_type = st.selectbox(\"Analyze a Team or Player?\", options=[\"team\", \"player\"])\n",
    "\n",
    "    if entity_type == \"team\":\n",
    "        st.markdown(\"_**Team option is able to analyze both offense and defense by looking into the defense by shot detail from other teams' shot charts against the Opposing Team.**_\")\n",
    "        entity_name = st.selectbox(\"Select a Team\", options=get_teams_list())\n",
    "    else:\n",
    "        st.markdown(\"_**Player Option is only able to look at offense.**_\")\n",
    "        player_names = st.multiselect(\"Select Players to Analyze\", options=get_players_list())\n",
    "\n",
    "    season = st.selectbox(\"Select the season\", options=[\"2023-24\", \"2022-23\", \"2021-22\", \"2020-21\"])\n",
    "\n",
    "    opponent_type = st.selectbox(\"Compare against all teams or a specific team?\", options=[\"all\", \"specific\"])\n",
    "\n",
    "    opponent_name = None\n",
    "    if opponent_type == \"specific\":\n",
    "        opponent_name = st.selectbox(\"Select an Opponent Team\", options=get_teams_list())\n",
    "\n",
    "    court_areas = st.selectbox(\"Select court areas to analyze\", options=[\"all\", \"specific\"], index=0)\n",
    "\n",
    "    if court_areas == \"specific\":\n",
    "        court_areas = st.multiselect(\"Select specific court areas\", options=get_all_court_areas())\n",
    "    else:\n",
    "        court_areas = \"all\"\n",
    "\n",
    "    debug_mode = st.checkbox(\"Enable Debug Mode\", value=False)\n",
    "\n",
    "    if st.button(\"Run Analysis\"):\n",
    "        if entity_type == \"player\" and (not player_names or len(player_names) < 1):\n",
    "            st.error(\"Please select at least one player.\")\n",
    "        else:\n",
    "            if entity_type == \"player\":\n",
    "                if len(player_names) == 1:\n",
    "                    # Single player analysis\n",
    "                    run_scenario(\n",
    "                        entity_name=player_names[0],\n",
    "                        entity_type=entity_type,\n",
    "                        season=season,\n",
    "                        opponent_name=opponent_name,\n",
    "                        analysis_type=analysis_type,\n",
    "                        compare_players=False,\n",
    "                        player_names=None,\n",
    "                        court_areas=court_areas\n",
    "                    )\n",
    "                else:\n",
    "                    # Multiple players comparison\n",
    "                    player_shots = fetch_shots_for_multiple_players(player_names, season, court_areas, opponent_name, debug=debug_mode)\n",
    "\n",
    "                    for player, shots in player_shots.items():\n",
    "                        st.pyplot(plot_shot_chart_hexbin(shots['shots'], f'{player} Shot Chart', opponent=opponent_name if opponent_name else \"all teams\"))\n",
    "                        st.write(f\"Efficiency for {player}:\")\n",
    "                        st.write(shots['efficiency'])\n",
    "\n",
    "                    compatibility_df = calculate_compatibility_between_players(player_shots)\n",
    "                    st.write(\"Player Shooting Area Compatibility:\")\n",
    "                    st.write(compatibility_df)\n",
    "            else:\n",
    "                # Team analysis\n",
    "                run_scenario(\n",
    "                    entity_name=entity_name,\n",
    "                    entity_type=entity_type,\n",
    "                    season=season,\n",
    "                    opponent_name=opponent_name,\n",
    "                    analysis_type=analysis_type,\n",
    "                    compare_players=False,\n",
    "                    court_areas=court_areas\n",
    "                )\n",
    "\n",
    "    # Add explanation for shot chart MAE analysis\n",
    "    with st.expander(\"Understanding MAE in Player Analysis with context from their Shooting\"):\n",
    "        st.markdown(\"\"\"\n",
    "        **MAE** is a metric that measures the average magnitude of errors between predicted values and actual values, without considering their direction.\n",
    "        \n",
    "        In our context, MAE is used to measure the difference between the shooting efficiencies of two players across various areas on the court.\n",
    "        \n",
    "        **Steps to Analyze MAE:**\n",
    "        1. **Define Common Areas**: The court is divided into areas like \"Left Corner 3\", \"Top of Key\", \"Paint\", etc.\n",
    "        2. **Calculate Individual Efficiencies**: Fetch shot data for each player and calculate their shooting efficiency in these areas.\n",
    "        3. **Identify Common Areas**: When comparing players, identify the areas where both players have taken shots.\n",
    "        4. **Calculate MAE**: Compute the absolute difference between efficiencies in each common area and average them.\n",
    "        5. **Interpret Compatibility**:\n",
    "            - **High MAE**: Indicates players excel in different areas (more compatible).\n",
    "            - **Low MAE**: Indicates similar efficiencies in the same areas (less compatible).\n",
    "        \n",
    "        **Use this metric to assess player compatibility based on where they excel on the court!**\n",
    "        \"\"\")\n",
    "\n",
    "    with st.expander(\"Understanding MAE in Team (offensive or defensive) in comparison to other Teams\"):\n",
    "        st.markdown(\"\"\"\n",
    "        **MAE** is a metric that measures the average magnitude of errors between predicted values and actual values, without considering their direction.\n",
    "        \n",
    "        In the context of team analysis, MAE is used to measure the difference between the shooting efficiencies of one team's offense and the defensive efficiencies of other teams.\n",
    "        \n",
    "        **Steps to Analyze MAE for Team Comparison:**\n",
    "        1. **Calculate Offensive Efficiency**: Fetch shot data for the team of interest and calculate their shooting efficiency across various areas on the court.\n",
    "        2. **Calculate Defensive Efficiency of Opponents**: For each opponent team, calculate their defensive efficiency by analyzing how well they defend these same areas on the court.\n",
    "        3. **Calculate MAE**: Compute the MAE between the offensive efficiency of the team of interest and the defensive efficiencies of each opponent team across the defined court areas.\n",
    "        4. **Interpret the Results**:\n",
    "            - **Low MAE**: Indicates that the opponent team is effective at defending the areas where the team of interest typically excels. This suggests that the opponent is a \"bad fit\" for the team of interest, as they defend well against their strengths.\n",
    "            - **High MAE**: Indicates that the opponent team struggles to defend the areas where the team of interest typically excels. This suggests that the opponent is a \"good fit\" for the team of interest, as their defense is less effective against the team's offensive strengths.\n",
    "        \n",
    "        **Use this analysis to identify which teams are tough matchups (bad fits) versus easier matchups (good fits) based on how well they can defend your team's key offensive areas!**\n",
    "        \"\"\")\n",
    "\n",
    "# Main Streamlit app\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"NBA Salary Prediction, Trade Analysis, and Shot Chart Analysis\", layout=\"wide\")\n",
    "    st.title(\"NBA Salary Prediction, Trade Analysis, and Shot Chart Analysis\")\n",
    "\n",
    "    # Sidebar navigation\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    page = st.sidebar.radio(\"Go to\", [\"Data Analysis\", \"Model Results\", \"Salary Evaluation\", \"Trade Impact Simulator\", \"Shot Chart Analysis\"])\n",
    "\n",
    "    # Load base data\n",
    "    print(os.getcwd())\n",
    "    data = load_processed_data('data/processed/nba_player_data_final_inflated.csv')\n",
    "\n",
    "    # Load existing predictions for 2023\n",
    "    initial_predictions_df = pd.read_csv('data/processed/predictions_df.csv')\n",
    "\n",
    "    # Season selection\n",
    "    seasons = sorted(data['Season'].unique(), reverse=True)\n",
    "    selected_season = st.selectbox(\"Select Season\", seasons)\n",
    "\n",
    "    # Load models at the beginning of main()\n",
    "    model_save_path = 'data/models'\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "\n",
    "    # Use initial predictions if 2023 is selected, otherwise retrain\n",
    "    if selected_season == 2023:\n",
    "        predictions_df = initial_predictions_df\n",
    "    else:\n",
    "        # Train model and make predictions\n",
    "        train_data = data[data['Season'] < selected_season]\n",
    "        test_data = data[data['Season'] == selected_season]\n",
    "\n",
    "        # Prepare the data for training\n",
    "        X_train = train_data.drop(['SalaryPct', 'Salary', 'Player'], axis=1)\n",
    "        y_train = train_data['SalaryPct']\n",
    "\n",
    "        # Encode the training data\n",
    "        X_train_encoded, _, encoders, scaler, numeric_cols, player_encoder = encode_data(X_train)\n",
    "\n",
    "        # Train and save models\n",
    "        train_and_save_models(X_train_encoded, y_train, model_save_path, scaler, X_train_encoded.columns, encoders, player_encoder, numeric_cols)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        predictions_df = predict(test_data, model_save_path)\n",
    "\n",
    "    if page == \"Data Analysis\":\n",
    "        st.header(\"Data Analysis\")\n",
    "\n",
    "        # Filter data by selected season\n",
    "        season_data = filter_data_by_season(data, selected_season)\n",
    "\n",
    "        # Display basic statistics\n",
    "        st.subheader(\"Basic Statistics\")\n",
    "        st.write(season_data.describe())\n",
    "\n",
    "        # Feature distribution\n",
    "        st.subheader(\"Feature Distribution\")\n",
    "        feature = st.selectbox(\"Select Feature\", season_data.columns)\n",
    "        fig = plot_feature_distribution(season_data, feature)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # Correlation heatmap\n",
    "        st.subheader(\"Correlation Heatmap\")\n",
    "        fig = plot_correlation_heatmap(season_data)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # Data handling explanation\n",
    "        st.subheader(\"Data Handling\")\n",
    "        st.write(\"\"\"\n",
    "        We preprocessed the data to ensure it's suitable for our models:\n",
    "        1. Cleaned missing values and outliers\n",
    "        2. Engineered new features like PPG, APG, etc.\n",
    "        3. Encoded categorical variables (Position, Team, Injury Risk)\n",
    "        4. Scaled numerical features\n",
    "        \"\"\")\n",
    "\n",
    "    elif page == \"Model Results\":\n",
    "        st.header(\"Model Results\")\n",
    "\n",
    "        # Model selection\n",
    "        model_choice = st.selectbox(\"Select Model\", [\"Random Forest\", \"XGBoost\"])\n",
    "\n",
    "        if model_choice == \"Random Forest\":\n",
    "            model = rf_model\n",
    "            y_pred = predictions_df['RF_Predictions']\n",
    "        else:\n",
    "            model = xgb_model\n",
    "            y_pred = predictions_df['XGB_Predictions']\n",
    "\n",
    "        # Display model metrics\n",
    "        display_model_metrics(predictions_df['SalaryPct'], y_pred)\n",
    "\n",
    "        # Feature importance\n",
    "        st.subheader(\"Feature Importance\")\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': model.feature_names_in_,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        print(\"Features used in the model:\", model.feature_names_in_)  # Debug statement\n",
    "        print(\"Feature importance data:\", feature_importance.head())  # Debug statement\n",
    "        # Filter out categorical variables (e.g., Position_ and Team_ columns)\n",
    "        filtered_feature_importance = feature_importance[\n",
    "            ~feature_importance['feature'].str.startswith('Team_') &\n",
    "            ~feature_importance['feature'].str.startswith('Position_') &\n",
    "            ~feature_importance['feature'].str.startswith('Injury_Risk')\n",
    "        ]\n",
    "\n",
    "        # Before plotting feature importance\n",
    "        print(\"Filtered features for plotting:\", filtered_feature_importance['feature'].tolist())  # Debug statement\n",
    "        st.bar_chart(filtered_feature_importance.set_index('feature'))\n",
    "\n",
    "\n",
    "        # Model explanation\n",
    "        st.subheader(\"Model Explanation\")\n",
    "        st.write(f\"\"\"\n",
    "        The {model_choice} model was trained on historical NBA player data to predict salary percentages.\n",
    "        We used the following techniques to improve model performance:\n",
    "        1. Feature engineering to create relevant statistics\n",
    "        2. Proper encoding of categorical variables\n",
    "        3. Scaling of numerical features\n",
    "        4. Hyperparameter tuning using GridSearchCV\n",
    "        \"\"\")\n",
    "        \n",
    "    elif page == \"Salary Evaluation\":\n",
    "        st.header(\"Salary Evaluation\")\n",
    "        display_overpaid_underpaid(predictions_df)\n",
    "        st.write(\"\"\"\n",
    "        Using the Predicted Salary and Salary based on that season's stats, we can identify \n",
    "        overpaid and underpaid players. We find the difference to uncover potential value opportunities for different teams.\n",
    "        \"\"\")\n",
    "        \n",
    "    elif page == \"Trade Impact Simulator\":\n",
    "        trade_impact_simulator()\n",
    "        \n",
    "        # Trade analysis explanation\n",
    "        st.subheader(\"Trade Analysis Explanation\")\n",
    "        st.write(\"\"\"\n",
    "        Our trade analysis compares team statistics before and after the proposed trade.\n",
    "        We consider:\n",
    "        1. Changes in key performance metrics (PPG, RPG, APG, etc.)\n",
    "        2. Salary implications and cap space impact\n",
    "        3. Comparison to league averages and recent championship teams\n",
    "        4. Distribution of top performers in various statistical categories\n",
    "        5. Overpaid/Underpaid player analysis\n",
    "        \"\"\")\n",
    "\n",
    "    elif page == \"Shot Chart Analysis\":\n",
    "        shot_chart_analysis()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
