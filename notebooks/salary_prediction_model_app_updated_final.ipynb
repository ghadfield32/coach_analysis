{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/updated/data_loader_preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/updated/data_loader_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Data loaded. Shape:\", data.shape)\n",
    "    return data\n",
    "\n",
    "def format_season(data):\n",
    "    # Convert season format to a single year for easier numerical analysis\n",
    "    data['Season'] = data['Season'].apply(lambda x: int(x.split('-')[0]))\n",
    "    print(\"Seasons in data:\", data['Season'].unique())\n",
    "    return data\n",
    "\n",
    "def clean_data(data):\n",
    "    # Drop columns that may not contribute significantly to the model\n",
    "    data_clean = data.copy()\n",
    "    columns_to_drop = ['Injury_Periods', '2nd Apron', 'Wins', 'Losses']\n",
    "    data_clean.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "    \n",
    "    # Handle missing percentage data by filling with column mean\n",
    "    percentage_cols = ['3P%', '2P%', 'FT%', 'TS%']\n",
    "    for col in percentage_cols:\n",
    "        if col in data_clean.columns:\n",
    "            data_clean[col] = data_clean[col].fillna(data_clean[col].mean())\n",
    "    \n",
    "    # Drop remaining NaNs\n",
    "    data_clean = data_clean.dropna()\n",
    "    print(\"Data cleaned. Remaining shape:\", data_clean.shape)\n",
    "    return data_clean\n",
    "\n",
    "def engineer_features(data):\n",
    "    # Calculate per-game statistics to normalize performance data\n",
    "    per_game_cols = ['PTS', 'AST', 'TRB', 'STL', 'BLK', 'TOV']\n",
    "    for col in per_game_cols:\n",
    "        data[f'{col[0]}PG'] = data[col] / data['GP']\n",
    "    \n",
    "    # Derive additional features to capture important aspects of a player's performance\n",
    "    data['Availability'] = data['GP'] / 82\n",
    "    data['SalaryPct'] = data['Salary'] / data['Salary_Cap_Inflated']\n",
    "    data['Efficiency'] = (data['PTS'] + data['TRB'] + data['AST'] + data['STL'] + data['BLK']) / (data['FGA'] + data['FTA'] + data['TOV'] + 1)\n",
    "    data['ValueOverReplacement'] = data['VORP'] / (data['Salary'] + 1)\n",
    "    data['ExperienceSquared'] = data['Years of Service'] ** 2\n",
    "    data['Days_Injured_Percentage'] = data['Total_Days_Injured'] / data['GP']\n",
    "    data['WSPG'] = data['WS'] / data['GP']\n",
    "    data['DWSPG'] = data['DWS'] / data['GP']\n",
    "    data['OWSPG'] = data['OWS'] / data['GP']\n",
    "    data['PFPG'] = data['PF'] / data['GP']\n",
    "    data['ORPG'] = data['ORB'] / data['GP']\n",
    "    data['DRPG'] = data['DRB'] / data['GP']\n",
    "    \n",
    "    # Drop columns used in feature creation or deemed less relevant\n",
    "    columns_to_drop = ['GP', '2PA', 'OBPM', 'BPM', 'DBPM', '2P', 'GS', 'PTS', 'AST', 'TRB', 'STL', 'BLK',\n",
    "                       'TOV', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB', 'DRB', 'TRB',\n",
    "                       'TS%', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'Luxury Tax', '1st Apron', 'BAE',\n",
    "                       'Standard /Non-Taxpayer', 'Taxpayer', 'Team Room /Under Cap', 'WS', 'DWS', 'WS/48', 'PF', 'OWS', 'Injured']\n",
    "    data.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "    print(\"New features added.\")\n",
    "    return data\n",
    "\n",
    "def encode_injury_risk(data):\n",
    "    # Encode injury risk levels for model training\n",
    "    risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    data['Injury_Risk'] = data['Injury_Risk'].map(risk_mapping).fillna(1)  # Default to Medium if unknown\n",
    "    return data, risk_mapping\n",
    "\n",
    "def encode_categorical(data, columns):\n",
    "    # Encode categorical columns using one-hot encoding\n",
    "    encoders = {}\n",
    "    for col in columns:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded = encoder.fit_transform(data[[col]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]), index=data.index)\n",
    "        data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "        encoders[col] = encoder\n",
    "    return data, encoders\n",
    "\n",
    "\n",
    "def encode_data(data, encoders=None, player_encoder=None):\n",
    "    print(\"Columns before encoding:\", data.columns)\n",
    "\n",
    "    # Encode Injury_Risk\n",
    "    risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    data['Injury_Risk'] = data['Injury_Risk'].map(risk_mapping).fillna(1)  # Default to Medium if unknown\n",
    "\n",
    "    # Encode Player column if it's present\n",
    "    if 'Player' in data.columns:\n",
    "        if player_encoder is None:\n",
    "            player_encoder = LabelEncoder()\n",
    "            data['Player_Encoded'] = player_encoder.fit_transform(data['Player'])\n",
    "        else:\n",
    "            data['Player_Encoded'] = player_encoder.transform(data['Player'])\n",
    "        data.drop('Player', axis=1, inplace=True)  # Drop original Player column after encoding\n",
    "    \n",
    "    # Identify initial numeric columns\n",
    "    initial_numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # Encode categorical variables (excluding Season)\n",
    "    categorical_cols = ['Position', 'Team']\n",
    "    if encoders is None:\n",
    "        encoders = {}\n",
    "        for col in categorical_cols:\n",
    "            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # Updated line\n",
    "            encoded = encoder.fit_transform(data[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]), index=data.index)\n",
    "            data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "            encoders[col] = encoder\n",
    "    else:\n",
    "        for col in categorical_cols:\n",
    "            encoded = encoders[col].transform(data[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=encoders[col].get_feature_names_out([col]), index=data.index)\n",
    "            data = pd.concat([data.drop(col, axis=1), encoded_df], axis=1)\n",
    "\n",
    "    # Identify final numeric columns (excluding one-hot encoded columns and 'Season')\n",
    "    numeric_cols = [col for col in initial_numeric_cols if col not in ['Season', 'Injury_Risk', 'Player_Encoded']]\n",
    "\n",
    "    # Scale numeric features (excluding 'Player_Encoded')\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "    print(\"Encoded data shape:\", data.shape)\n",
    "    print(\"Columns after encoding:\", data.columns)\n",
    "\n",
    "    return data, risk_mapping, encoders, scaler, numeric_cols, player_encoder\n",
    "\n",
    "\n",
    "\n",
    "def scale_features(data, numeric_cols):\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    return data, scaler\n",
    "\n",
    "def decode_data(encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder):\n",
    "    decoded_data = encoded_data.copy()\n",
    "    \n",
    "    # Decode Injury_Risk\n",
    "    inv_injury_risk_mapping = {v: k for k, v in injury_risk_mapping.items()}\n",
    "    decoded_data['Injury_Risk'] = decoded_data['Injury_Risk'].map(inv_injury_risk_mapping)\n",
    "    \n",
    "    # Decode Player column\n",
    "    if 'Player_Encoded' in decoded_data.columns:\n",
    "        decoded_data['Player'] = player_encoder.inverse_transform(decoded_data['Player_Encoded'])\n",
    "        decoded_data.drop('Player_Encoded', axis=1, inplace=True)\n",
    "    \n",
    "    # Decode categorical variables\n",
    "    for col, encoder in encoders.items():\n",
    "        encoded_cols = [c for c in decoded_data.columns if c.startswith(f\"{col}_\")]\n",
    "        decoded_col = encoder.inverse_transform(decoded_data[encoded_cols])\n",
    "        decoded_data[col] = decoded_col.ravel()  # Flatten the 2D array to 1D\n",
    "        decoded_data.drop(encoded_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Inverse transform scaled features\n",
    "    decoded_data[numeric_cols] = scaler.inverse_transform(decoded_data[numeric_cols])\n",
    "    \n",
    "    return decoded_data\n",
    "\n",
    "def select_top_features(X, y, k=10):\n",
    "    # Select top features based on statistical significance\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    selector.fit(X, y)\n",
    "    top_features = X.columns[selector.get_support()].tolist()\n",
    "    print(f\"Top {k} features:\", top_features)\n",
    "    return top_features\n",
    "\n",
    "def calculate_tree_feature_importance(X, y):\n",
    "    # Calculate feature importance using a Random Forest Regressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    importances = rf.feature_importances_\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "    plt.title('Top 20 Feature Importances from Random Forest')\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '../data/processed/nba_player_data_final_inflated.csv'\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    data = clean_data(data)\n",
    "    data = engineer_features(data)\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop(['SalaryPct', 'Salary'], axis=1)\n",
    "    y = data['SalaryPct']\n",
    "\n",
    "    # Encode data\n",
    "    encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder = encode_data(X)\n",
    "\n",
    "    print(\"\\nInjury Risk Mapping:\", injury_risk_mapping)\n",
    "    print(\"Encoded Injury Risk range:\", encoded_data['Injury_Risk'].min(), \"-\", encoded_data['Injury_Risk'].max())\n",
    "    print(\"\\nNumeric columns for scaling:\", numeric_cols)\n",
    "\n",
    "    # Calculate feature importance\n",
    "    feature_importances = calculate_tree_feature_importance(encoded_data, y)\n",
    "    print(\"\\nTree-based feature importances:\")\n",
    "    print(feature_importances.head(20))\n",
    "\n",
    "    # Select top features\n",
    "    top_features = select_top_features(encoded_data, y)\n",
    "    print(\"\\nTop features selected using statistical methods:\", top_features)\n",
    "\n",
    "    # Decoding example\n",
    "    print(\"\\nDecoding Example:\")\n",
    "    decoded_data = decode_data(encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder)\n",
    "    \n",
    "    print(\"\\nFirst few rows of decoded data:\")\n",
    "    print(decoded_data[['Player', 'Injury_Risk', 'Position', 'Team', 'Season'] + top_features].head())\n",
    "\n",
    "    print(\"\\nData types after decoding:\")\n",
    "    print(decoded_data.dtypes)\n",
    "\n",
    "    print(\"\\nData preprocessing completed. Ready for model training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/updated/model_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/updated/model_trainer.py\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def inspect_data_types(X):\n",
    "    print(\"Data types of features:\")\n",
    "    print(X.dtypes)\n",
    "    object_columns = X.select_dtypes(include=['object']).columns\n",
    "    if not object_columns.empty:\n",
    "        print(\"Columns with object data types:\", object_columns.tolist())\n",
    "    else:\n",
    "        print(\"No columns with object data types.\")\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model.__class__.__name__}: {-grid_search.best_score_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_and_save_models(X_train, y_train, model_save_path, scaler, feature_names, encoders, player_encoder, numeric_cols):\n",
    "    # Inspect data types before training\n",
    "    inspect_data_types(X_train)\n",
    "\n",
    "    # Initialize models with default parameters\n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    xgb_model = xgb.XGBRegressor(random_state=42, enable_categorical=True)\n",
    "\n",
    "    # Define parameter grids for grid search\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    # Perform grid search\n",
    "    best_rf_model = perform_grid_search(rf_model, rf_param_grid, X_train, y_train)\n",
    "    best_xgb_model = perform_grid_search(xgb_model, xgb_param_grid, X_train, y_train)\n",
    "\n",
    "    # Train models with best parameters\n",
    "    best_rf_model.fit(X_train, y_train)\n",
    "    best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Scale the features used for training\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Save models, scaler, feature names, encoders, and other artifacts\n",
    "    joblib.dump(best_rf_model, f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    joblib.dump(best_xgb_model, f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "    joblib.dump(scaler, f\"{model_save_path}/scaler.pkl\")\n",
    "    joblib.dump(feature_names, f\"{model_save_path}/feature_names.pkl\")\n",
    "    joblib.dump(encoders, f\"{model_save_path}/encoders.pkl\")\n",
    "    joblib.dump(injury_risk_mapping, f\"{model_save_path}/injury_risk_mapping.pkl\")\n",
    "    joblib.dump(numeric_cols, f\"{model_save_path}/numeric_cols.pkl\")\n",
    "\n",
    "    joblib.dump(player_encoder, f\"{model_save_path}/player_encoder.pkl\")\n",
    "    print(\"Models, scaler, feature names, encoders, and other artifacts trained and saved successfully.\")\n",
    "\n",
    "def evaluate_models(X_test, y_test, model_save_path):\n",
    "    # Load models, scaler, and feature names\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "\n",
    "    # Make predictions\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "    xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate models using multiple metrics\n",
    "    metrics = {'Random Forest': rf_predictions, 'XGBoost': xgb_predictions}\n",
    "\n",
    "    for model_name, predictions in metrics.items():\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "\n",
    "        print(f\"\\n{model_name} Evaluation:\")\n",
    "        print(f\"MSE: {mse}\")\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "        print(f\"MAE: {mae}\")\n",
    "        print(f\"R-squared: {r2}\")\n",
    "        \n",
    "def filter_seasons(data, predict_season):\n",
    "    \"\"\"\n",
    "    Filters the dataset into prior seasons and the target season for prediction.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset containing season data.\n",
    "        predict_season (int): The season that you want to predict.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two DataFrames:\n",
    "            - prior_seasons_data: Data for seasons before the predict_season.\n",
    "            - target_season_data: Data for the predict_season.\n",
    "    \"\"\"\n",
    "    # Separate data into prior seasons and the target season\n",
    "    prior_seasons_data = data[data['Season'] < predict_season]\n",
    "    target_season_data = data[data['Season'] == predict_season]\n",
    "    \n",
    "    print(f\"Data filtered. Prior seasons shape: {prior_seasons_data.shape}, Target season shape: {target_season_data.shape}\")\n",
    "    \n",
    "    return target_season_data, prior_seasons_data\n",
    "\n",
    "# Data preprocessing\n",
    "def load_and_preprocess_data(file_path, predict_season):\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    _, prior_seasons_data = filter_seasons(data, predict_season)\n",
    "    prior_seasons_data = clean_data(prior_seasons_data)\n",
    "    prior_seasons_data = engineer_features(prior_seasons_data)\n",
    "    return prior_seasons_data\n",
    "\n",
    "# Feature selection\n",
    "def select_features(data, target_column, additional_features=[]):\n",
    "    top_features = ['PPG', 'APG', 'RPG', 'SPG', 'TOPG', 'Years of Service', 'PER', 'VORP', 'WSPG', 'OWSPG']\n",
    "    \n",
    "    # Add 'Injury_Risk', 'Position', and 'Team' to ensure they're included for encoding\n",
    "    top_features += ['Injury_Risk', 'Position', 'Team']\n",
    "    \n",
    "    # Add any additional features\n",
    "    top_features += additional_features\n",
    "    \n",
    "    # Ensure all selected features are in the dataset\n",
    "    available_features = [col for col in top_features if col in data.columns]\n",
    "    \n",
    "    print(\"Available features for modeling:\", available_features)  # Debug statement\n",
    "\n",
    "    X = data[available_features]\n",
    "    y = data[target_column]\n",
    "    return X, y\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '../data/processed/nba_player_data_final_inflated.csv'\n",
    "    predict_season = 2023\n",
    "    target_column = 'SalaryPct'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    preprocessed_data = load_and_preprocess_data(file_path, predict_season)\n",
    "    print(\"Columns after preprocessing:\", preprocessed_data.columns)\n",
    "\n",
    "    # Select features\n",
    "    X, y = select_features(preprocessed_data, target_column)\n",
    "    print(\"Columns after feature selection:\", X.columns)\n",
    "\n",
    "    # Encode data\n",
    "    encoded_data, injury_risk_mapping, encoders, scaler, numeric_cols, player_encoder = encode_data(X)\n",
    "    print(\"Columns after encoding:\", encoded_data.columns)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    model_save_path = '../data/models'\n",
    "    train_and_save_models(X_train, y_train, model_save_path, scaler, encoded_data.columns, encoders, injury_risk_mapping, numeric_cols)\n",
    "    evaluate_models(X_test, y_test, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/updated/model_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/updated/model_predictor.py\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def load_models_and_utils(model_save_path):\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "    scaler = joblib.load(f\"{model_save_path}/scaler.pkl\")\n",
    "    feature_names = joblib.load(f\"{model_save_path}/feature_names.pkl\")\n",
    "    encoders = joblib.load(f\"{model_save_path}/encoders.pkl\")\n",
    "    injury_risk_mapping = joblib.load(f\"{model_save_path}/injury_risk_mapping.pkl\")\n",
    "    numeric_cols = joblib.load(f\"{model_save_path}/numeric_cols.pkl\")\n",
    "    player_encoder = joblib.load(f\"{model_save_path}/player_encoder.pkl\")\n",
    "    return rf_model, xgb_model, scaler, feature_names, encoders, injury_risk_mapping, numeric_cols, player_encoder\n",
    "\n",
    "def predict(data, model_save_path):\n",
    "    rf_model, xgb_model, scaler, feature_names, encoders, _, _, player_encoder = load_models_and_utils(model_save_path)\n",
    "    \n",
    "    print(\"Original data shape:\", data.shape)\n",
    "    print(\"Original data columns:\", data.columns.tolist())\n",
    "\n",
    "    # Preserve player names\n",
    "    player_names = data['Player'] if 'Player' in data.columns else None\n",
    "    \n",
    "    # Drop the player column before encoding\n",
    "    data = data.drop(columns=['Player'], errors='ignore')\n",
    "    \n",
    "    # Encode the data using the loaded encoders\n",
    "    encoded_data, _, _, _, _, _ = encode_data(data, encoders, player_encoder)\n",
    "    \n",
    "    print(\"Encoded data shape:\", encoded_data.shape)\n",
    "    print(\"Encoded data columns:\", encoded_data.columns.tolist())\n",
    "    \n",
    "    # Handle missing features: Add missing columns and set them to zero\n",
    "    for col in feature_names:\n",
    "        if col not in encoded_data.columns:\n",
    "            encoded_data[col] = 0\n",
    "\n",
    "    # Ensure encoded_data only has feature_names columns\n",
    "    encoded_data = encoded_data[feature_names]\n",
    "    \n",
    "    print(\"Selected features shape:\", encoded_data.shape)\n",
    "    print(\"Selected features:\", encoded_data.columns.tolist())\n",
    "    print(\"Expected features:\", feature_names)\n",
    "    \n",
    "    # Scale the encoded data\n",
    "    encoded_data_scaled = scaler.transform(encoded_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_predictions = rf_model.predict(encoded_data_scaled)\n",
    "    xgb_predictions = xgb_model.predict(encoded_data_scaled)\n",
    "    \n",
    "    # Create a DataFrame for predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'RF_Predictions': rf_predictions,\n",
    "        'XGB_Predictions': xgb_predictions,\n",
    "        'Predicted_Salary': (rf_predictions + xgb_predictions) / 2\n",
    "    })\n",
    "    \n",
    "    # Attach player names back to the predictions\n",
    "    if player_names is not None:\n",
    "        predictions_df['Player'] = player_names.values\n",
    "\n",
    "    # Combine the predictions with the original data (excluding player names)\n",
    "    result = pd.concat([data.reset_index(drop=True), predictions_df], axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '../data/processed/nba_player_data_final_inflated.csv'\n",
    "    predict_season = 2023\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    current_season_data, _ = filter_seasons(data, predict_season)\n",
    "    current_season_data = clean_data(current_season_data)\n",
    "    current_season_data = engineer_features(current_season_data)\n",
    "    model_save_path = '../data/models'\n",
    "    predictions_df = predict(current_season_data, model_save_path)  # Save predictions as predictions_df\n",
    "    print(predictions_df.head())\n",
    "    \n",
    "    # Save predictions_df for later use\n",
    "    predictions_df.to_csv('../data/processed/predictions_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.hoopsrumors.com/2023/09/salary-matching-rules-for-trades-during-2023-24-season.html\n",
    "\n",
    "for trade rules\n",
    "\n",
    "\n",
    "FIRST_TAX_APRON = 172_346_000\n",
    "\n",
    "def check_salary_matching_rules(outgoing_salary, incoming_salary, team_salary_before_trade):\n",
    "    if team_salary_before_trade < FIRST_TAX_APRON:\n",
    "        if outgoing_salary <= 7_500_000:\n",
    "            max_incoming_salary = 2 * outgoing_salary + 250_000\n",
    "        elif outgoing_salary <= 29_000_000:\n",
    "            max_incoming_salary = outgoing_salary + 7_500_000\n",
    "        else:\n",
    "            max_incoming_salary = 1.25 * outgoing_salary + 250_000\n",
    "    else:\n",
    "        max_incoming_salary = 1.10 * outgoing_salary\n",
    "\n",
    "    return incoming_salary <= max_incoming_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for season 2022-23\n",
      "Fetching data for season 2023-24\n",
      "Champions Player Data (Past 10 Seasons):\n",
      "    PLAYER_ID               PLAYER_NAME     TEAM_ID       TEAM_NAME  \\\n",
      "0      201145                Jeff Green  1610612743  Denver Nuggets   \n",
      "1      201599            DeAndre Jordan  1610612743  Denver Nuggets   \n",
      "2      202397                 Ish Smith  1610612743  Denver Nuggets   \n",
      "3      202704            Reggie Jackson  1610612743  Denver Nuggets   \n",
      "4      203484  Kentavious Caldwell-Pope  1610612743  Denver Nuggets   \n",
      "5      203932              Aaron Gordon  1610612743  Denver Nuggets   \n",
      "6      203999              Nikola Jokic  1610612743  Denver Nuggets   \n",
      "7     1627750              Jamal Murray  1610612743  Denver Nuggets   \n",
      "8     1628418             Thomas Bryant  1610612743  Denver Nuggets   \n",
      "9     1628427             Vlatko Cancar  1610612743  Denver Nuggets   \n",
      "10    1628432                Davon Reed  1610612743  Denver Nuggets   \n",
      "11    1628971               Bruce Brown  1610612743  Denver Nuggets   \n",
      "12    1629008        Michael Porter Jr.  1610612743  Denver Nuggets   \n",
      "13    1630192                Zeke Nnaji  1610612743  Denver Nuggets   \n",
      "14    1630538              Bones Hyland  1610612743  Denver Nuggets   \n",
      "15    1631128           Christian Braun  1610612743  Denver Nuggets   \n",
      "16    1631212             Peyton Watson  1610612743  Denver Nuggets   \n",
      "17    1631298                Jack White  1610612743  Denver Nuggets   \n",
      "18     201143                Al Horford  1610612738  Boston Celtics   \n",
      "19     201950              Jrue Holiday  1610612738  Boston Celtics   \n",
      "20     204001        Kristaps Porzingis  1610612738  Boston Celtics   \n",
      "21    1627759              Jaylen Brown  1610612738  Boston Celtics   \n",
      "22    1628369              Jayson Tatum  1610612738  Boston Celtics   \n",
      "23    1628401             Derrick White  1610612738  Boston Celtics   \n",
      "24    1628436               Luke Kornet  1610612738  Boston Celtics   \n",
      "25    1629004            Svi Mykhailiuk  1610612738  Boston Celtics   \n",
      "26    1629052            Oshae Brissett  1610612738  Boston Celtics   \n",
      "27    1629674             Neemias Queta  1610612738  Boston Celtics   \n",
      "28    1630202          Payton Pritchard  1610612738  Boston Celtics   \n",
      "29    1630205             Lamar Stevens  1610612738  Boston Celtics   \n",
      "30    1630214            Xavier Tillman  1610612738  Boston Celtics   \n",
      "31    1630531            Jaden Springer  1610612738  Boston Celtics   \n",
      "32    1630573                Sam Hauser  1610612738  Boston Celtics   \n",
      "33    1630625             Dalano Banton  1610612738  Boston Celtics   \n",
      "34    1631120                JD Davison  1610612738  Boston Celtics   \n",
      "35    1641775              Jordan Walsh  1610612738  Boston Celtics   \n",
      "36    1641809             Drew Peterson  1610612738  Boston Celtics   \n",
      "\n",
      "          PTS       AST       TOV       STL       BLK      OREB      DREB  \\\n",
      "0    7.803571  1.232143  0.821429  0.321429  0.303571  0.678571  1.892857   \n",
      "1    5.102564  0.871795  1.230769  0.307692  0.589744  1.333333  3.846154   \n",
      "2    2.511628  2.325581  1.023256  0.186047  0.162791  0.116279  1.139535   \n",
      "3    7.937500  3.125000  1.187500  0.562500  0.062500  0.187500  1.562500   \n",
      "4   10.815789  2.407895  1.131579  1.473684  0.460526  0.460526  2.276316   \n",
      "5   16.308824  2.985294  1.441176  0.794118  0.750000  2.411765  4.147059   \n",
      "6   24.492754  9.826087  3.579710  1.260870  0.681159  2.420290  9.420290   \n",
      "7   19.969231  6.153846  2.230769  1.015385  0.246154  0.738462  3.215385   \n",
      "8    4.611111  0.111111  0.555556  0.111111  0.388889  1.055556  2.277778   \n",
      "9    4.950000  1.333333  0.616667  0.366667  0.233333  0.400000  1.733333   \n",
      "10   2.314286  0.542857  0.571429  0.371429  0.114286  0.257143  1.314286   \n",
      "11  11.537500  3.350000  1.537500  1.087500  0.637500  0.787500  3.312500   \n",
      "12  17.419355  1.048387  1.096774  0.596774  0.467742  1.032258  4.467742   \n",
      "13   5.226415  0.339623  0.584906  0.320755  0.433962  1.226415  1.377358   \n",
      "14  12.095238  2.976190  1.595238  0.666667  0.309524  0.238095  1.809524   \n",
      "15   4.736842  0.776316  0.460526  0.539474  0.223684  0.631579  1.750000   \n",
      "16   3.260870  0.478261  0.347826  0.086957  0.478261  0.347826  1.260870   \n",
      "17   1.235294  0.235294  0.117647  0.176471  0.117647  0.352941  0.647059   \n",
      "18   8.646154  2.584615  0.738462  0.584615  0.953846  1.261538  5.092308   \n",
      "19  12.463768  4.826087  1.797101  0.884058  0.768116  1.217391  4.188406   \n",
      "20  20.087719  2.017544  1.561404  0.736842  1.947368  1.701754  5.473684   \n",
      "21  23.000000  3.557143  2.371429  1.185714  0.528571  1.200000  4.328571   \n",
      "22  26.851351  4.918919  2.540541  1.013514  0.581081  0.905405  7.216216   \n",
      "23  15.164384  5.164384  1.534247  1.013699  1.191781  0.698630  3.547945   \n",
      "24   5.301587  1.063492  0.333333  0.365079  0.968254  1.873016  2.269841   \n",
      "25   3.951220  0.853659  0.292683  0.268293  0.024390  0.268293  0.975610   \n",
      "26   3.654545  0.800000  0.363636  0.345455  0.145455  1.109091  1.800000   \n",
      "27   5.500000  0.714286  0.464286  0.464286  0.750000  1.892857  2.464286   \n",
      "28   9.597561  3.426829  0.743902  0.475610  0.073171  0.853659  2.378049   \n",
      "29   2.789474  0.421053  0.473684  0.315789  0.263158  0.631579  1.000000   \n",
      "30   4.000000  1.000000  0.250000  0.450000  0.450000  0.700000  2.000000   \n",
      "31   2.058824  0.529412  0.470588  0.647059  0.235294  0.470588  0.705882   \n",
      "32   9.012658  1.037975  0.405063  0.506329  0.316456  0.569620  2.924051   \n",
      "33   2.333333  0.791667  0.416667  0.208333  0.125000  0.500000  0.958333   \n",
      "34   2.000000  1.250000  0.250000  0.125000  0.125000  0.250000  1.000000   \n",
      "35   1.666667  0.555556  0.333333  0.555556  0.111111  0.555556  1.666667   \n",
      "36   3.666667  0.333333  0.333333  0.666667  0.000000  0.000000  0.333333   \n",
      "\n",
      "         FGM      FG3M        FGA      eFG%   SEASON  \n",
      "0   2.857143  0.535714   5.857143  0.533537  2022-23  \n",
      "1   2.256410  0.025641   2.948718  0.769565  2022-23  \n",
      "2   1.209302  0.046512   3.046512  0.404580  2022-23  \n",
      "3   3.062500  1.187500   8.000000  0.457031  2022-23  \n",
      "4   3.842105  1.776316   8.315789  0.568829  2022-23  \n",
      "5   6.308824  0.882353  11.191176  0.603154  2022-23  \n",
      "6   9.362319  0.826087  14.811594  0.659980  2022-23  \n",
      "7   7.276923  2.646154  16.015385  0.536984  2022-23  \n",
      "8   1.833333  0.222222   3.777778  0.514706  2022-23  \n",
      "9   1.800000  0.716667   3.783333  0.570485  2022-23  \n",
      "10  0.714286  0.457143   2.285714  0.412500  2022-23  \n",
      "11  4.475000  1.137500   9.262500  0.544534  2022-23  \n",
      "12  6.419355  3.032258  13.177419  0.602203  2022-23  \n",
      "13  2.075472  0.320755   3.698113  0.604592  2022-23  \n",
      "14  4.119048  2.166667  10.333333  0.503456  2022-23  \n",
      "15  1.881579  0.447368   3.802632  0.553633  2022-23  \n",
      "16  1.260870  0.260870   2.565217  0.542373  2022-23  \n",
      "17  0.470588  0.176471   1.117647  0.500000  2022-23  \n",
      "18  3.292308  1.661538   6.446154  0.639618  2023-24  \n",
      "19  4.797101  2.000000   9.985507  0.580552  2023-24  \n",
      "20  6.807018  1.929825  13.192982  0.589096  2023-24  \n",
      "21  8.957143  2.071429  17.942857  0.556927  2023-24  \n",
      "22  9.081081  3.094595  19.270270  0.551543  2023-24  \n",
      "23  5.301370  2.684932  11.493151  0.578069  2023-24  \n",
      "24  2.253968  0.015873   3.222222  0.701970  2023-24  \n",
      "25  1.390244  1.024390   3.341463  0.569343  2023-24  \n",
      "26  1.236364  0.272727   2.781818  0.493464  2023-24  \n",
      "27  2.392857  0.000000   3.714286  0.644231  2023-24  \n",
      "28  3.621951  1.792683   7.743902  0.583465  2023-24  \n",
      "29  1.105263  0.157895   2.368421  0.500000  2023-24  \n",
      "30  1.700000  0.400000   3.300000  0.575758  2023-24  \n",
      "31  0.764706  0.117647   1.764706  0.466667  2023-24  \n",
      "32  3.151899  2.493671   7.063291  0.622760  2023-24  \n",
      "33  0.791667  0.083333   2.125000  0.392157  2023-24  \n",
      "34  0.625000  0.375000   1.500000  0.541667  2023-24  \n",
      "35  0.666667  0.222222   1.666667  0.466667  2023-24  \n",
      "36  1.333333  1.000000   2.000000  0.916667  2023-24  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 246\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mprint\u001b[39m(traded_players_salary[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Salary\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 199\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(champions_player_data)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# 3. Get current season player-level data\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m current_season_player_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_current_season_player_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_seasons_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_season\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCurrent Season Player Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(current_season_player_data)\n",
      "Cell \u001b[0;32mIn[6], line 160\u001b[0m, in \u001b[0;36mget_current_season_player_data\u001b[0;34m(all_seasons_data, current_season)\u001b[0m\n\u001b[1;32m    158\u001b[0m     player_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEASON\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m current_season\n\u001b[1;32m    159\u001b[0m     all_current_players\u001b[38;5;241m.\u001b[39mappend(player_stats)\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# To avoid overwhelming the API\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(all_current_players, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nba_api.stats.endpoints import leaguegamefinder, playergamelogs\n",
    "import time\n",
    "\n",
    "RELEVANT_STATS = ['PTS', 'AST', 'TOV', 'STL', 'BLK', 'OREB', 'DREB', 'FGM', 'FG3M', 'FGA']\n",
    "PERCENTILE_THRESHOLDS = [99, 98, 97, 96, 95, 90, 75, 50]\n",
    "\n",
    "def get_champion(season):\n",
    "    games = leaguegamefinder.LeagueGameFinder(season_nullable=season, season_type_nullable='Playoffs').get_data_frames()[0]\n",
    "    games['GAME_DATE'] = pd.to_datetime(games['GAME_DATE'])\n",
    "    last_game = games.sort_values('GAME_DATE').iloc[-2:]\n",
    "    winner = last_game[last_game['WL'] == 'W'].iloc[0]\n",
    "    return winner['TEAM_ID'], winner['TEAM_NAME']\n",
    "\n",
    "def get_champions(start_year, end_year):\n",
    "    champions = {}\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "        champ_id, champ_name = get_champion(season)\n",
    "        champions[season] = {'ChampionTeamID': champ_id, 'ChampionTeamName': champ_name}\n",
    "        time.sleep(1)  # To avoid overwhelming the API\n",
    "    return champions\n",
    "\n",
    "def get_season_from_date(date):\n",
    "    year = int(date[:4])\n",
    "    month = int(date[5:7])\n",
    "    if month >= 10:\n",
    "        return f\"{year}-{str(year+1)[2:]}\"\n",
    "    else:\n",
    "        return f\"{year-1}-{str(year)[2:]}\"\n",
    "\n",
    "def analyze_leaguegamefinder_endpoint(start_season, end_season):\n",
    "    all_seasons_data = []\n",
    "    \n",
    "    for season in range(int(start_season[:4]), int(end_season[:4]) + 1):\n",
    "        season_str = f\"{season}-{str(season+1)[2:]}\"\n",
    "        print(f\"Fetching data for season {season_str}\")\n",
    "        \n",
    "        games = leaguegamefinder.LeagueGameFinder(\n",
    "            season_nullable=season_str,\n",
    "            season_type_nullable='Regular Season'\n",
    "        ).get_data_frames()[0]\n",
    "        \n",
    "        games['SEASON'] = games['GAME_DATE'].apply(get_season_from_date)\n",
    "        all_seasons_data.append(games)\n",
    "        \n",
    "        time.sleep(1)  # To avoid overwhelming the API\n",
    "    \n",
    "    return pd.concat(all_seasons_data, ignore_index=True)\n",
    "\n",
    "def calculate_per_game_stats(games_df):\n",
    "    per_game_stats = games_df.groupby(['SEASON', 'TEAM_ID', 'TEAM_NAME'])[RELEVANT_STATS].mean().reset_index()\n",
    "    \n",
    "    # Calculate eFG%\n",
    "    per_game_stats['eFG%'] = (per_game_stats['FGM'] + 0.5 * per_game_stats['FG3M']) / per_game_stats['FGA']\n",
    "    \n",
    "    return per_game_stats\n",
    "\n",
    "def calculate_percentiles(stats_df):\n",
    "    percentile_cols = RELEVANT_STATS + ['eFG%']\n",
    "    \n",
    "    for col in percentile_cols:\n",
    "        stats_df[f'{col}_percentile'] = stats_df.groupby('SEASON')[col].rank(pct=True)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def get_current_season_stats(all_seasons_data, current_season):\n",
    "    current_season_data = all_seasons_data[all_seasons_data['SEASON'] == current_season]\n",
    "    per_game_stats = calculate_per_game_stats(current_season_data)\n",
    "    percentile_stats = calculate_percentiles(per_game_stats)\n",
    "    \n",
    "    # Calculate league average\n",
    "    league_avg = per_game_stats[RELEVANT_STATS + ['eFG%']].mean()\n",
    "    league_avg['TEAM_NAME'] = 'League Average'\n",
    "    league_avg['SEASON'] = current_season\n",
    "    league_avg['TEAM_ID'] = 'AVG'\n",
    "    league_avg = pd.DataFrame(league_avg).transpose()\n",
    "    \n",
    "    # Combine team stats with league average\n",
    "    combined_stats = pd.concat([percentile_stats, league_avg], ignore_index=True)\n",
    "    return combined_stats\n",
    "\n",
    "def get_champions_stats(all_seasons_data, start_season, end_season):\n",
    "    champions = {}\n",
    "    for year in range(int(start_season[:4]), int(end_season[:4]) + 1):\n",
    "        season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "        champ_id, champ_name = get_champion(season)\n",
    "        champions[season] = {'ChampionTeamID': champ_id, 'ChampionTeamName': champ_name}\n",
    "    \n",
    "    champions_data = all_seasons_data[all_seasons_data.apply(lambda row: row['TEAM_ID'] == champions.get(row['SEASON'], {}).get('ChampionTeamID'), axis=1)]\n",
    "    champions_stats = calculate_per_game_stats(champions_data)\n",
    "    return calculate_percentiles(champions_stats)\n",
    "\n",
    "def compare_stats(current_stats, champions_stats, league_avg):\n",
    "    # Compare current stats to champions average and league average\n",
    "    champs_avg = champions_stats[RELEVANT_STATS + ['eFG%']].mean()\n",
    "    \n",
    "    comparison = current_stats.copy()\n",
    "    for stat in RELEVANT_STATS + ['eFG%']:\n",
    "        comparison[f'{stat}_vs_champs'] = comparison[stat] - champs_avg[stat]\n",
    "        comparison[f'{stat}_vs_league'] = comparison[stat] - league_avg[stat]\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "def get_team_data(all_seasons_data, team_names, current_season):\n",
    "    team_data = all_seasons_data[(all_seasons_data['SEASON'] == current_season) & (all_seasons_data['TEAM_NAME'].isin(team_names))]\n",
    "    return calculate_per_game_stats(team_data)\n",
    "\n",
    "def simulate_trade(all_seasons_data, team_from, team_to, trade_impact, current_season):\n",
    "    before_trade = get_team_data(all_seasons_data, [team_from, team_to], current_season)\n",
    "    \n",
    "    # Simulate the trade by adjusting team stats\n",
    "    after_trade = before_trade.copy()\n",
    "    numeric_columns = before_trade.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for stat in numeric_columns:\n",
    "        if stat in trade_impact:\n",
    "            after_trade.loc[after_trade['TEAM_NAME'] == team_from, stat] -= trade_impact[stat]\n",
    "            after_trade.loc[after_trade['TEAM_NAME'] == team_to, stat] += trade_impact[stat]\n",
    "    \n",
    "    # Recalculate eFG% for both teams\n",
    "    for team in [team_from, team_to]:\n",
    "        team_data = after_trade[after_trade['TEAM_NAME'] == team]\n",
    "        after_trade.loc[after_trade['TEAM_NAME'] == team, 'eFG%'] = (\n",
    "            (team_data['FGM'] + 0.5 * team_data['FG3M']) / team_data['FGA']\n",
    "        ).values[0]\n",
    "    \n",
    "    return before_trade, after_trade\n",
    "\n",
    "def get_player_game_logs(team_id, season):\n",
    "    player_logs = playergamelogs.PlayerGameLogs(team_id_nullable=team_id, season_nullable=season).get_data_frames()[0]\n",
    "    return player_logs\n",
    "\n",
    "def process_player_data(player_logs):\n",
    "    player_stats = player_logs.groupby(['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_NAME'])[RELEVANT_STATS].mean().reset_index()\n",
    "    player_stats['eFG%'] = (player_stats['FGM'] + 0.5 * player_stats['FG3M']) / player_stats['FGA']\n",
    "    return player_stats\n",
    "\n",
    "def get_champions_player_data(champions, start_season, end_season):\n",
    "    all_champion_players = []\n",
    "    for season in range(int(start_season[:4]), int(end_season[:4]) + 1):\n",
    "        season_str = f\"{season}-{str(season+1)[2:]}\"\n",
    "        champ_id = champions[season_str]['ChampionTeamID']\n",
    "        player_logs = get_player_game_logs(champ_id, season_str)\n",
    "        player_stats = process_player_data(player_logs)\n",
    "        player_stats['SEASON'] = season_str\n",
    "        all_champion_players.append(player_stats)\n",
    "        time.sleep(1)  # To avoid overwhelming the API\n",
    "    return pd.concat(all_champion_players, ignore_index=True)\n",
    "\n",
    "def get_current_season_player_data(all_seasons_data, current_season):\n",
    "    current_teams = all_seasons_data[all_seasons_data['SEASON'] == current_season]['TEAM_ID'].unique()\n",
    "    all_current_players = []\n",
    "    for team_id in current_teams:\n",
    "        player_logs = get_player_game_logs(team_id, current_season)\n",
    "        player_stats = process_player_data(player_logs)\n",
    "        player_stats['SEASON'] = current_season\n",
    "        all_current_players.append(player_stats)\n",
    "        time.sleep(1)  # To avoid overwhelming the API\n",
    "    return pd.concat(all_current_players, ignore_index=True)\n",
    "\n",
    "def simulate_trade_with_players(team_from_data, team_to_data, traded_players):\n",
    "    before_trade = pd.concat([team_from_data, team_to_data])\n",
    "    \n",
    "    # Move traded players between teams\n",
    "    traded_from = team_from_data[team_from_data['PLAYER_NAME'].isin(traded_players)].copy()\n",
    "    traded_to = team_to_data[team_to_data['PLAYER_NAME'].isin(traded_players)].copy()\n",
    "    \n",
    "    team_from_after = team_from_data[~team_from_data['PLAYER_NAME'].isin(traded_players)]\n",
    "    team_to_after = pd.concat([team_to_data[~team_to_data['PLAYER_NAME'].isin(traded_players)], traded_from])\n",
    "    \n",
    "    after_trade = pd.concat([team_from_after, team_to_after])\n",
    "    \n",
    "    return before_trade, after_trade\n",
    "\n",
    "def analyze_trade_impact(before_trade, after_trade):\n",
    "    team_totals_before = before_trade.groupby('TEAM_NAME')[RELEVANT_STATS + ['eFG%']].sum().reset_index()\n",
    "    team_totals_after = after_trade.groupby('TEAM_NAME')[RELEVANT_STATS + ['eFG%']].sum().reset_index()\n",
    "    \n",
    "    trade_impact = team_totals_after.set_index('TEAM_NAME').subtract(team_totals_before.set_index('TEAM_NAME')).reset_index()\n",
    "    return trade_impact\n",
    "\n",
    "def main():\n",
    "    start_season = \"2022-23\"\n",
    "    end_season = \"2023-24\"\n",
    "    current_season = end_season\n",
    "    all_seasons_data = analyze_leaguegamefinder_endpoint(start_season, end_season)\n",
    "    \n",
    "    # 1. Get champions for the past 10 seasons\n",
    "    champions = get_champions(int(start_season[:4]), int(end_season[:4]))\n",
    "    \n",
    "    # 2. Get player-level data for champions\n",
    "    champions_player_data = get_champions_player_data(champions, start_season, end_season)\n",
    "    print(\"Champions Player Data (Past 10 Seasons):\")\n",
    "    print(champions_player_data)\n",
    "    \n",
    "    # 3. Get current season player-level data\n",
    "    current_season_player_data = get_current_season_player_data(all_seasons_data, current_season)\n",
    "    print(\"\\nCurrent Season Player Data:\")\n",
    "    print(current_season_player_data)\n",
    "    \n",
    "    # 4. Load predictions dataframe\n",
    "    predictions_df = pd.read_csv('../data/processed/predictions_df.csv')\n",
    "    print(\"\\nPredictions DataFrame (first few rows):\")\n",
    "    print(predictions_df.head())\n",
    "    \n",
    "    # 5. Simulate trade\n",
    "    team_from = \"Los Angeles Lakers\"\n",
    "    team_to = \"Boston Celtics\"\n",
    "    traded_players = [\"Anthony Davis\", \"Jayson Tatum\"]  # Example players\n",
    "    \n",
    "    team_from_data = current_season_player_data[current_season_player_data['TEAM_NAME'] == team_from]\n",
    "    team_to_data = current_season_player_data[current_season_player_data['TEAM_NAME'] == team_to]\n",
    "    \n",
    "    print(\"\\nTeam Data Before Trade:\")\n",
    "    print(pd.concat([team_from_data, team_to_data]))\n",
    "    \n",
    "    before_trade, after_trade = simulate_trade_with_players(team_from_data, team_to_data, traded_players)\n",
    "    \n",
    "    print(\"\\nTeam Data After Trade:\")\n",
    "    print(after_trade)\n",
    "    \n",
    "    # 6. Analyze trade impact\n",
    "    trade_impact = analyze_trade_impact(before_trade, after_trade)\n",
    "    print(\"\\nTrade Impact (Difference in Team Stats):\")\n",
    "    print(trade_impact)\n",
    "    \n",
    "    # 7. Compare traded players to champions\n",
    "    traded_player_stats = before_trade[before_trade['PLAYER_NAME'].isin(traded_players)]\n",
    "    champion_avg = champions_player_data.groupby('SEASON')[RELEVANT_STATS + ['eFG%']].mean().mean()\n",
    "    \n",
    "    print(\"\\nTraded Players vs. Champions Average:\")\n",
    "    for _, player in traded_player_stats.iterrows():\n",
    "        print(f\"\\n{player['PLAYER_NAME']}:\")\n",
    "        for stat in RELEVANT_STATS + ['eFG%']:\n",
    "            diff = player[stat] - champion_avg[stat]\n",
    "            print(f\"{stat}: {player[stat]:.2f} (Diff from Champs Avg: {diff:.2f})\")\n",
    "    \n",
    "    # 8. Analyze salary based on predictions\n",
    "    traded_players_salary = predictions_df[predictions_df['Player'].isin(traded_players)]\n",
    "    print(\"\\nSalary Analysis for Traded Players:\")\n",
    "    print(traded_players_salary[['Player', 'Salary', 'Predicted_Salary']])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/updated/overall_team_trade_impact.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/updated/overall_team_trade_impact.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nba_api.stats.endpoints import playergamelogs, leaguegamefinder\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "from nba_api.stats.endpoints import commonplayerinfo\n",
    "from nba_api.stats.static import teams\n",
    "\n",
    "# Constants\n",
    "RELEVANT_STATS = ['PTS', 'AST', 'TOV', 'STL', 'BLK', 'OREB', 'DREB', 'FGM', 'FG3M', 'FGA']\n",
    "\n",
    "def load_team_data():\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_df = pd.DataFrame(nba_teams)\n",
    "    return team_df[['id', 'full_name', 'abbreviation']]\n",
    "\n",
    "\n",
    "def fetch_player_info(player_id, debug=False):\n",
    "    \"\"\"Fetch player information based on player ID.\"\"\"\n",
    "    try:\n",
    "        player_info = commonplayerinfo.CommonPlayerInfo(player_id=player_id).get_data_frames()[0]\n",
    "        if debug:\n",
    "            print(f\"Fetched info for player ID {player_id}: {player_info['DISPLAY_FIRST_LAST'].values[0]}\")\n",
    "        return player_info\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error fetching info for player ID {player_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_season_data_by_year(year, debug=False):\n",
    "    \"\"\"Fetch player game logs data for a given starting year of the NBA season.\"\"\"\n",
    "    season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "    if debug:\n",
    "        print(f\"Fetching player data for season {season}\")\n",
    "    try:\n",
    "        player_logs = playergamelogs.PlayerGameLogs(season_nullable=season).get_data_frames()[0]\n",
    "        player_logs['SEASON'] = season\n",
    "        player_logs['GAME_DATE'] = pd.to_datetime(player_logs['GAME_DATE'])\n",
    "        if debug:\n",
    "            print(f\"Player data for season {season} contains {player_logs.shape[0]} rows.\")\n",
    "        return player_logs\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error fetching player data for season {season}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Helper Functions\n",
    "def get_champion(season, debug=False):\n",
    "    \"\"\"Fetch the champion team for a given NBA season.\"\"\"\n",
    "    try:\n",
    "        games = leaguegamefinder.LeagueGameFinder(season_nullable=season, season_type_nullable='Playoffs').get_data_frames()[0]\n",
    "        games['GAME_DATE'] = pd.to_datetime(games['GAME_DATE'])\n",
    "        last_game = games.sort_values('GAME_DATE').iloc[-2:]\n",
    "        winner = last_game[last_game['WL'] == 'W'].iloc[0]\n",
    "        if debug:\n",
    "            print(f\"Champion for season {season}: {winner['TEAM_NAME']} ({winner['TEAM_ID']})\")\n",
    "        return winner['TEAM_NAME']\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Error fetching champion for season {season}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_champions(start_year, end_year, debug=False):\n",
    "    \"\"\"Fetch champions for each season from start_year to end_year.\"\"\"\n",
    "    champions = {}\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "        champ_name = get_champion(season, debug)\n",
    "        if champ_name:\n",
    "            champions[season] = {'ChampionTeamName': champ_name}\n",
    "        elif debug:\n",
    "            print(f\"Champion data not available for season {season}\")\n",
    "        time.sleep(1)  # To avoid overwhelming the API\n",
    "    if debug:\n",
    "        print(f\"Champions data: {champions}\")\n",
    "    return champions\n",
    "\n",
    "def calculate_percentiles(stats_df, debug=False):\n",
    "    \"\"\"Calculate percentiles for stats after averages are computed.\"\"\"\n",
    "    # Group by season and calculate percentiles for each season separately\n",
    "    for season in stats_df['SEASON'].unique():\n",
    "        season_data = stats_df[stats_df['SEASON'] == season]\n",
    "        for stat in RELEVANT_STATS + ['eFG%']:\n",
    "            stat_per_game = f'{stat}_per_game'\n",
    "            if stat_per_game in season_data.columns:\n",
    "                stats_df.loc[season_data.index, f'{stat}_percentile'] = season_data[stat_per_game].rank(pct=True)\n",
    "                if debug:\n",
    "                    print(f\"Calculated percentiles for {stat} in season {season}:\")\n",
    "                    print(stats_df.loc[season_data.index, [stat_per_game, f'{stat}_percentile']].head())\n",
    "    return stats_df\n",
    "\n",
    "def calculate_team_stats(player_data, period, debug=False):\n",
    "    \"\"\"Calculate team-level statistics, including averages.\"\"\"\n",
    "    if debug:\n",
    "        print(f\"Calculating {period} team-level statistics.\")\n",
    "        print(\"Initial player_data head:\")\n",
    "        print(player_data.head())\n",
    "\n",
    "    # Calculate team-level stats by summing player stats for each team and season\n",
    "    team_stats = (\n",
    "        player_data.groupby(['SEASON', 'TEAM_NAME'])[RELEVANT_STATS]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate the number of games played by each team\n",
    "    games_played = player_data.groupby(['SEASON', 'TEAM_NAME'])['GAME_ID'].nunique().reset_index(name='GAMES_PLAYED')\n",
    "\n",
    "    # Merge games played with team stats\n",
    "    team_stats = pd.merge(team_stats, games_played, on=['SEASON', 'TEAM_NAME'])\n",
    "\n",
    "    # Calculate stats per game\n",
    "    for stat in RELEVANT_STATS:\n",
    "        team_stats[f'{stat}_per_game'] = team_stats[stat] / team_stats['GAMES_PLAYED']\n",
    "\n",
    "    # Add period column\n",
    "    team_stats['PERIOD'] = period\n",
    "\n",
    "    if debug:\n",
    "        print(f\"{period} team-level statistics head:\")\n",
    "        print(team_stats.head())\n",
    "\n",
    "    return team_stats\n",
    "\n",
    "def process_champion_team_data(player_data, champions, debug=False):\n",
    "    \"\"\"Process the game logs to get data for the champion teams.\"\"\"\n",
    "    champion_team_stats = pd.DataFrame()\n",
    "\n",
    "    for season, champ_info in champions.items():\n",
    "        champ_name = champ_info['ChampionTeamName']\n",
    "\n",
    "        # Filter player data for champion team\n",
    "        champ_data = player_data[(player_data['SEASON'] == season) & (player_data['TEAM_NAME'] == champ_name)]\n",
    "\n",
    "        if champ_data.empty:\n",
    "            if debug:\n",
    "                print(f\"No data found for champion team {champ_name} in season {season}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate team statistics\n",
    "        champ_stats = calculate_team_stats(champ_data, 'Champion', debug)\n",
    "        champ_stats['ChampionTeamName'] = champ_name\n",
    "\n",
    "        champion_team_stats = pd.concat([champion_team_stats, champ_stats], ignore_index=True)\n",
    "\n",
    "    # Calculate eFG%\n",
    "    champion_team_stats['eFG%_per_game'] = (\n",
    "        (champion_team_stats['FGM_per_game'] + 0.5 * champion_team_stats['FG3M_per_game']) / champion_team_stats['FGA_per_game']\n",
    "    )\n",
    "\n",
    "    # Calculate percentiles for champion teams within their season\n",
    "    champion_team_stats = calculate_percentiles(champion_team_stats, debug)\n",
    "\n",
    "    return champion_team_stats\n",
    "\n",
    "def calculate_post_trade_team_stats(player_data, traded_players, trade_date, season_data, debug=False):\n",
    "    \"\"\"Calculate post-trade team-level statistics, using entire season if necessary.\"\"\"\n",
    "    if debug:\n",
    "        print(\"Calculating post-trade team-level statistics.\")\n",
    "\n",
    "    # Convert trade_date to datetime\n",
    "    trade_date = pd.to_datetime(trade_date)\n",
    "\n",
    "    # Determine the start of the season based on the SEASON column\n",
    "    season_start_year = int(player_data['SEASON'].iloc[0].split('-')[0])\n",
    "    season_start_date = pd.to_datetime(f\"{season_start_year}-10-01\")  # NBA season typically starts in October\n",
    "\n",
    "    # Determine whether to use entire season data or data after trade date\n",
    "    if trade_date < season_start_date:\n",
    "        if debug:\n",
    "            print(f\"Warning: Trade date {trade_date} is earlier than the start of the season {season_start_date}. Using entire season data.\")\n",
    "        post_trade_data = season_data  # Use the entire season data\n",
    "    else:\n",
    "        post_trade_data = player_data[player_data['GAME_DATE'] >= trade_date].copy()\n",
    "\n",
    "    if debug:\n",
    "        print(\"Post-trade player data head:\")\n",
    "        print(post_trade_data.head())\n",
    "\n",
    "    # Calculate post-trade stats\n",
    "    post_trade_stats = calculate_team_stats(post_trade_data, 'Post-trade', debug)\n",
    "\n",
    "    # Calculate traded players' post-trade averages\n",
    "    traded_player_stats = {}\n",
    "    for player_id, (new_team_name, player_name) in traded_players.items():\n",
    "        player_post_trade_stats = post_trade_data[post_trade_data['PLAYER_ID'] == player_id][RELEVANT_STATS].mean()\n",
    "        traded_player_stats[player_id] = player_post_trade_stats.to_dict()\n",
    "        if debug:\n",
    "            print(f\"{player_name} averages post-trade (to {new_team_name}): {traded_player_stats[player_id]}\")\n",
    "\n",
    "    # Adjust post-trade stats based on traded players\n",
    "    for player_id, (new_team_name, player_name) in traded_players.items():\n",
    "        old_team_name = player_data[player_data['PLAYER_ID'] == player_id]['TEAM_NAME'].iloc[0]\n",
    "        post_trade_games = post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == new_team_name, 'GAMES_PLAYED'].values[0]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nAdjusting stats for trade: {player_name} from {old_team_name} to {new_team_name}\")\n",
    "\n",
    "        # Remove player's stats from old team\n",
    "        for stat in RELEVANT_STATS:\n",
    "            if debug:\n",
    "                print(f\"  Before adjustment - {old_team_name} {stat}: {post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == old_team_name, stat].values[0]}\")\n",
    "            post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == old_team_name, stat] -= traded_player_stats[player_id][stat] * post_trade_games\n",
    "            if debug:\n",
    "                print(f\"  After adjustment - {old_team_name} {stat}: {post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == old_team_name, stat].values[0]}\")\n",
    "\n",
    "        # Add player's stats to new team\n",
    "        for stat in RELEVANT_STATS:\n",
    "            if debug:\n",
    "                print(f\"  Before adjustment - {new_team_name} {stat}: {post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == new_team_name, stat].values[0]}\")\n",
    "            post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == new_team_name, stat] += traded_player_stats[player_id][stat] * post_trade_games\n",
    "            if debug:\n",
    "                print(f\"  After adjustment - {new_team_name} {stat}: {post_trade_stats.loc[post_trade_stats['TEAM_NAME'] == new_team_name, stat].values[0]}\")\n",
    "\n",
    "    # Recalculate per-game stats\n",
    "    for stat in RELEVANT_STATS:\n",
    "        post_trade_stats[f'{stat}_per_game'] = post_trade_stats[stat] / post_trade_stats['GAMES_PLAYED']\n",
    "\n",
    "    if debug:\n",
    "        print(\"Post-trade team stats calculated successfully.\")\n",
    "        print(\"Post-trade team stats head:\")\n",
    "        print(post_trade_stats.head())\n",
    "\n",
    "    return post_trade_stats\n",
    "\n",
    "def calculate_average_champion_stats(champion_team_data, debug=False):\n",
    "    \"\"\"Calculate the average statistics for all champion teams.\"\"\"\n",
    "    if debug:\n",
    "        print(\"Calculating average champion team statistics.\")\n",
    "    \n",
    "    # Calculate average stats for all champion teams\n",
    "    avg_stats = champion_team_data[RELEVANT_STATS + [f'{stat}_per_game' for stat in RELEVANT_STATS] + ['eFG%_per_game']].mean()\n",
    "\n",
    "    # Create a DataFrame for the average stats\n",
    "    avg_row = pd.DataFrame([avg_stats], columns=champion_team_data.columns)\n",
    "    avg_row['SEASON'] = 'Average'\n",
    "    avg_row['TEAM_NAME'] = 'Average Champion'\n",
    "    avg_row['PERIOD'] = 'Champion'\n",
    "    avg_row['ChampionTeamName'] = 'Average Champion'\n",
    "\n",
    "    # Append the average row to the champion team data\n",
    "    champion_team_data = pd.concat([champion_team_data, avg_row], ignore_index=True)\n",
    "\n",
    "    # Recalculate percentiles for champion teams within their data\n",
    "    champion_team_data = calculate_percentiles(champion_team_data, debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nChampion Team Stats with Average:\")\n",
    "        print(tabulate(champion_team_data, headers='keys', tablefmt='grid'))\n",
    "\n",
    "    # Return the updated champion data with the new average\n",
    "    return champion_team_data\n",
    "\n",
    "def compare_team_performance(percentiles, average_champion_stats, traded_teams, debug=True):\n",
    "    \"\"\"Generate a comparison table for team performance before and after trades.\"\"\"\n",
    "    if debug:\n",
    "        print(\"Comparing team performance:\")\n",
    "        print(\"Percentiles data head:\")\n",
    "        print(percentiles.head())\n",
    "        print(\"Percentiles columns:\")\n",
    "        print(percentiles.columns)\n",
    "        print(\"Average champion stats:\")\n",
    "        print(average_champion_stats)\n",
    "\n",
    "    comparison_data = []\n",
    "    \n",
    "    for team in traded_teams:\n",
    "        if debug:\n",
    "            print(f\"Processing team: {team}\")\n",
    "        \n",
    "        pre_trade_stats = percentiles[(percentiles['TEAM_NAME'] == team) & (percentiles['PERIOD'] == 'Pre-trade')]\n",
    "        post_trade_stats = percentiles[(percentiles['TEAM_NAME'] == team) & (percentiles['PERIOD'] == 'Post-trade')]\n",
    "        \n",
    "        if not pre_trade_stats.empty and not post_trade_stats.empty:\n",
    "            team_comparison = {'Team': team}\n",
    "            for stat in RELEVANT_STATS + ['eFG%']:\n",
    "                if debug:\n",
    "                    print(f\"Processing stat: {stat}\")\n",
    "                    print(f\"Pre-trade stats columns: {pre_trade_stats.columns}\")\n",
    "                    print(f\"Post-trade stats columns: {post_trade_stats.columns}\")\n",
    "                \n",
    "                per_game_col = f'{stat}_per_game'\n",
    "                percentile_col = f'{stat}_percentile'\n",
    "                \n",
    "                # Pre-trade stats\n",
    "                if per_game_col in pre_trade_stats.columns:\n",
    "                    team_comparison[f'{stat} Pre-trade'] = pre_trade_stats[per_game_col].values[0]\n",
    "                else:\n",
    "                    print(f\"Warning: {per_game_col} not found in pre_trade_stats\")\n",
    "                    team_comparison[f'{stat} Pre-trade'] = None\n",
    "                \n",
    "                if percentile_col in pre_trade_stats.columns:\n",
    "                    team_comparison[f'{stat} Pre-trade Percentile'] = pre_trade_stats[percentile_col].values[0]\n",
    "                else:\n",
    "                    print(f\"Warning: {percentile_col} not found in pre_trade_stats\")\n",
    "                    team_comparison[f'{stat} Pre-trade Percentile'] = None\n",
    "                \n",
    "                # Post-trade stats\n",
    "                if per_game_col in post_trade_stats.columns:\n",
    "                    team_comparison[f'{stat} Post-trade'] = post_trade_stats[per_game_col].values[0]\n",
    "                else:\n",
    "                    print(f\"Warning: {per_game_col} not found in post_trade_stats\")\n",
    "                    team_comparison[f'{stat} Post-trade'] = None\n",
    "                \n",
    "                if percentile_col in post_trade_stats.columns:\n",
    "                    team_comparison[f'{stat} Post-trade Percentile'] = post_trade_stats[percentile_col].values[0]\n",
    "                else:\n",
    "                    print(f\"Warning: {percentile_col} not found in post_trade_stats\")\n",
    "                    team_comparison[f'{stat} Post-trade Percentile'] = None\n",
    "                \n",
    "                # Champion stats\n",
    "                if per_game_col in average_champion_stats.columns:\n",
    "                    team_comparison[f'{stat} Champion'] = average_champion_stats[per_game_col].values[0]\n",
    "                else:\n",
    "                    print(f\"Warning: {per_game_col} not found in average_champion_stats\")\n",
    "                    team_comparison[f'{stat} Champion'] = None\n",
    "            \n",
    "            comparison_data.append(team_comparison)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"No data available for comparison for {team}.\")\n",
    "                print(\"Pre-trade stats head:\")\n",
    "                print(pre_trade_stats.head())\n",
    "                print(\"Post-trade stats head:\")\n",
    "                print(post_trade_stats.head())\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nComparison Results:\")\n",
    "        print(comparison_df)\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "def validate_post_trade_stats(player_data, trade_date, traded_teams, post_trade_stats, debug=False):\n",
    "    \"\"\"Validate the post-trade statistics calculation.\"\"\"\n",
    "    trade_date = pd.to_datetime(trade_date)\n",
    "    post_trade_data = player_data[player_data['GAME_DATE'] >= trade_date]\n",
    "\n",
    "    validation_results = {}\n",
    "\n",
    "    for team in traded_teams:\n",
    "        team_data = post_trade_data[post_trade_data['TEAM_NAME'] == team]\n",
    "        \n",
    "        total_points = team_data['PTS'].sum()\n",
    "        games_played = team_data['GAME_ID'].nunique()\n",
    "        calculated_ppg = total_points / games_played if games_played > 0 else 0\n",
    "\n",
    "        reported_ppg = post_trade_stats[post_trade_stats['TEAM_NAME'] == team]['PTS_per_game'].values[0]\n",
    "\n",
    "        validation_results[team] = {\n",
    "            'Calculated PPG': calculated_ppg,\n",
    "            'Reported PPG': reported_ppg,\n",
    "            'Difference': calculated_ppg - reported_ppg,\n",
    "            'Games Played': games_played\n",
    "        }\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nPost-Trade Statistics Validation:\")\n",
    "        print(tabulate(pd.DataFrame(validation_results).T, headers='keys', tablefmt='grid'))\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "import plotly.graph_objects as go\n",
    "from nba_api.stats.endpoints import commonplayerinfo\n",
    "from nba_api.stats.static import teams\n",
    "\n",
    "def load_team_data():\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_df = pd.DataFrame(nba_teams)\n",
    "    return team_df[['id', 'full_name', 'abbreviation']]\n",
    "\n",
    "def load_player_data(start_year, end_year):\n",
    "    player_data = pd.DataFrame()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        data = fetch_season_data_by_year(year)\n",
    "        if data is not None:\n",
    "            player_data = pd.concat([player_data, data], ignore_index=True)\n",
    "    return player_data\n",
    "\n",
    "def trade_impact_simulator():\n",
    "    st.subheader(\"NBA Trade Impact Simulator\")\n",
    "\n",
    "    # Load team and player data\n",
    "    team_data = load_team_data()\n",
    "    player_data = load_player_data(2020, 2023)  # Adjust years as needed\n",
    "\n",
    "    # User inputs\n",
    "    trade_date = st.date_input('Trade Date', datetime(2023, 12, 20))\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        team1 = st.selectbox('Select Team 1', team_data['full_name'].tolist())\n",
    "    with col2:\n",
    "        team2 = st.selectbox('Select Team 2', team_data['full_name'].tolist(), index=1)\n",
    "\n",
    "    team1_players = player_data[player_data['TEAM_NAME'] == team1]['PLAYER_NAME'].unique()\n",
    "    team2_players = player_data[player_data['TEAM_NAME'] == team2]['PLAYER_NAME'].unique()\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        players1 = st.multiselect(f'Select Players from {team1}', team1_players)\n",
    "    with col2:\n",
    "        players2 = st.multiselect(f'Select Players from {team2}', team2_players)\n",
    "\n",
    "    if st.button('Simulate Trade'):\n",
    "        # Convert trade_date to pandas Timestamp for comparison\n",
    "        trade_date = pd.Timestamp(trade_date)\n",
    "\n",
    "        # Prepare traded players data\n",
    "        traded_players = {}\n",
    "        for player in players1:\n",
    "            player_id = player_data[player_data['PLAYER_NAME'] == player]['PLAYER_ID'].iloc[0]\n",
    "            traded_players[player_id] = (team2, player)\n",
    "        for player in players2:\n",
    "            player_id = player_data[player_data['PLAYER_NAME'] == player]['PLAYER_ID'].iloc[0]\n",
    "            traded_players[player_id] = (team1, player)\n",
    "\n",
    "        # Fetch champion data\n",
    "        champions = get_champions(2020, 2023)\n",
    "\n",
    "        # Process champion team data\n",
    "        champion_team_data = process_champion_team_data(player_data, champions)\n",
    "\n",
    "        # Calculate pre-trade and post-trade team statistics\n",
    "        pre_trade_team_stats = calculate_team_stats(player_data[player_data['GAME_DATE'] < trade_date], 'Pre-trade')\n",
    "        post_trade_team_stats = calculate_post_trade_team_stats(player_data, traded_players, trade_date, player_data)\n",
    "\n",
    "        # Combine pre-trade and post-trade stats\n",
    "        combined_stats = pd.concat([pre_trade_team_stats, post_trade_team_stats], ignore_index=True)\n",
    "\n",
    "        # Calculate eFG% for the combined dataset\n",
    "        combined_stats['eFG%_per_game'] = (combined_stats['FGM_per_game'] + 0.5 * combined_stats['FG3M_per_game']) / combined_stats['FGA_per_game']\n",
    "\n",
    "        # Calculate percentiles for the combined stats\n",
    "        combined_stats = calculate_percentiles(combined_stats)\n",
    "\n",
    "        # Calculate average champion stats\n",
    "        average_champion_stats = calculate_average_champion_stats(champion_team_data)\n",
    "\n",
    "        # Compare pre-trade and post-trade stats for traded teams\n",
    "        traded_teams = [team1, team2]\n",
    "        comparison_table = compare_team_performance(combined_stats, average_champion_stats, traded_teams)\n",
    "\n",
    "        # Display the comparison table\n",
    "        st.subheader('Trade Impact Comparison')\n",
    "        st.dataframe(comparison_table)\n",
    "\n",
    "        # Visualize the results\n",
    "        st.subheader('Visual Comparison')\n",
    "        metric = st.selectbox('Select Metric', ['PTS', 'AST', 'TOV', 'STL', 'BLK', 'OREB', 'DREB', 'FGM', 'FG3M', 'FGA', 'eFG%'])\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for team in traded_teams:\n",
    "            team_data = comparison_table[comparison_table['Team'] == team]\n",
    "            fig.add_trace(go.Bar(x=[f'{team} Pre-trade'], y=[team_data[f'{metric} Pre-trade'].values[0]], name=f'{team} Pre-trade'))\n",
    "            fig.add_trace(go.Bar(x=[f'{team} Post-trade'], y=[team_data[f'{metric} Post-trade'].values[0]], name=f'{team} Post-trade'))\n",
    "            fig.add_trace(go.Bar(x=[f'{team} Champion'], y=[team_data[f'{metric} Champion'].values[0]], name=f'{team} Champion'))\n",
    "\n",
    "        fig.update_layout(title=f'{metric} Comparison', xaxis_title='Team', yaxis_title=metric)\n",
    "        st.plotly_chart(fig)\n",
    "\n",
    "\n",
    "def main(debug=True):\n",
    "    start_year = 2020\n",
    "    end_year = 2023\n",
    "    trade_date = '2023-12-20'  # Example trade date\n",
    "    \n",
    "    # Traded players with new team names\n",
    "    traded_players = {\n",
    "        1628369: ('Los Angeles Lakers', 'Jayson Tatum'),  # Example Player ID and new team\n",
    "        1630559: ('Boston Celtics', 'Austin Reaves')      # Example Player ID and new team\n",
    "    }\n",
    "    \n",
    "    # Fetch player names\n",
    "    for player_id in traded_players.keys():\n",
    "        player_info = fetch_player_info(player_id, debug)\n",
    "        if player_info is not None:\n",
    "            traded_players[player_id] = (traded_players[player_id][0], player_info['DISPLAY_FIRST_LAST'].values[0])\n",
    "    \n",
    "    # Fetch champion data\n",
    "    champions = get_champions(start_year, end_year, debug)\n",
    "    \n",
    "    # Fetch player data for each season\n",
    "    player_data = pd.DataFrame()\n",
    "    season_data = pd.DataFrame()  # To store the full season data\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        data = fetch_season_data_by_year(year, debug)\n",
    "        if data is not None:\n",
    "            player_data = pd.concat([player_data, data], ignore_index=True)\n",
    "            season_data = player_data  # Assuming season_data should hold the entire season's data\n",
    "\n",
    "    if player_data.empty:\n",
    "        print(\"Failed to fetch player data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Process champion team data\n",
    "    champion_team_data = process_champion_team_data(player_data, champions, debug)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nChampion Team Stats and Percentiles:\")\n",
    "        print(tabulate(champion_team_data, headers='keys', tablefmt='grid'))\n",
    "\n",
    "    # Debug: Print pre-trade stats for traded players and their teams\n",
    "    if debug:\n",
    "        print(\"\\nPre-trade stats:\")\n",
    "        for player_id, (new_team_name, player_name) in traded_players.items():\n",
    "            # Use all available data if trade date is before the season starts\n",
    "            if player_data['GAME_DATE'].min() > pd.to_datetime(trade_date):\n",
    "                player_pre_trade = player_data[player_data['PLAYER_ID'] == player_id]\n",
    "            else:\n",
    "                player_pre_trade = player_data[(player_data['PLAYER_ID'] == player_id) & (player_data['GAME_DATE'] < pd.to_datetime(trade_date))]\n",
    "            \n",
    "            if not player_pre_trade.empty:\n",
    "                old_team_name = player_pre_trade['TEAM_NAME'].iloc[0]\n",
    "                player_total_points = player_pre_trade['PTS'].sum()\n",
    "                team_total_points = player_data[(player_data['TEAM_NAME'] == old_team_name) & (player_data['GAME_DATE'] < pd.to_datetime(trade_date))]['PTS'].sum()\n",
    "                print(f\"{player_name} (Old team: {old_team_name}):\")\n",
    "                print(f\"  Player total points: {player_total_points}\")\n",
    "                print(f\"  Team total points: {team_total_points}\")\n",
    "            else:\n",
    "                print(f\"No data available for {player_name}.\")\n",
    "\n",
    "    # Calculate pre-trade and post-trade team statistics\n",
    "    if player_data['GAME_DATE'].min() > pd.to_datetime(trade_date):\n",
    "        pre_trade_team_stats = calculate_team_stats(player_data, 'Pre-trade', debug)\n",
    "    else:\n",
    "        pre_trade_team_stats = calculate_team_stats(player_data[player_data['GAME_DATE'] < pd.to_datetime(trade_date)], 'Pre-trade', debug)\n",
    "        \n",
    "    post_trade_team_stats = calculate_post_trade_team_stats(player_data, traded_players, trade_date, season_data, debug)\n",
    "\n",
    "    # Debug: Print post-trade stats for traded players and their new teams\n",
    "    if debug:\n",
    "        print(\"\\nPost-trade stats:\")\n",
    "        for player_id, (new_team_name, player_name) in traded_players.items():\n",
    "            player_post_trade = player_data[(player_data['PLAYER_ID'] == player_id) & (player_data['GAME_DATE'] >= pd.to_datetime(trade_date))]\n",
    "            if not player_post_trade.empty:\n",
    "                player_total_points = player_post_trade['PTS'].sum()\n",
    "                team_total_points = player_data[(player_data['TEAM_NAME'] == new_team_name) & (player_data['GAME_DATE'] >= pd.to_datetime(trade_date))]['PTS'].sum()\n",
    "                print(f\"{player_name} (New team: {new_team_name}):\")\n",
    "                print(f\"  Player total points: {player_total_points}\")\n",
    "                print(f\"  Team total points: {team_total_points}\")\n",
    "            else:\n",
    "                print(f\"No post-trade data found for {player_name}.\")\n",
    "\n",
    "    # Combine pre-trade and post-trade stats\n",
    "    combined_stats = pd.concat([pre_trade_team_stats, post_trade_team_stats], ignore_index=True)\n",
    "\n",
    "    # Calculate eFG% for the combined dataset\n",
    "    combined_stats['eFG%_per_game'] = (combined_stats['FGM_per_game'] + 0.5 * combined_stats['FG3M_per_game']) / combined_stats['FGA_per_game']\n",
    "\n",
    "    # Calculate percentiles for the combined stats\n",
    "    percentiles = calculate_percentiles(combined_stats, debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nCombined Team Stats and Percentiles:\")\n",
    "        print(tabulate(percentiles, headers='keys', tablefmt='grid'))\n",
    "    \n",
    "    # Calculate average champion stats\n",
    "    average_champion_stats = calculate_average_champion_stats(champion_team_data, debug)\n",
    "\n",
    "    # Compare pre-trade and post-trade stats for traded teams\n",
    "    traded_teams = list(set([team_name for _, (team_name, _) in traded_players.items()]))\n",
    "    comparison_table = compare_team_performance(percentiles, average_champion_stats, traded_teams, debug)\n",
    "    \n",
    "    # Print the comparison table\n",
    "    if debug:\n",
    "        print(\"\\nTrade Impact Comparison:\")\n",
    "        print(tabulate(comparison_table, headers='keys', tablefmt='grid'))\n",
    "\n",
    "    # Validate post-trade statistics\n",
    "    validation_results = validate_post_trade_stats(player_data, trade_date, traded_teams, post_trade_team_stats, debug)\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/updated/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/updated/app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "\n",
    "# Import functions from other modules\n",
    "from data_loader_preprocessor import load_data, format_season, clean_data, engineer_features, encode_data\n",
    "from model_trainer import train_and_save_models, evaluate_models\n",
    "from model_predictor import predict\n",
    "from trade_utils import analyze_two_team_trade, get_champions\n",
    "from overall_team_trade_impact import (\n",
    "    fetch_season_data_by_year, get_champions, process_champion_team_data,\n",
    "    calculate_team_stats, calculate_post_trade_team_stats,\n",
    "    calculate_average_champion_stats, compare_team_performance,\n",
    "    trade_impact_simulator\n",
    ")\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def load_team_data():\n",
    "    nba_teams = teams.get_teams()\n",
    "    team_df = pd.DataFrame(nba_teams)\n",
    "    return team_df[['id', 'full_name', 'abbreviation']]\n",
    "\n",
    "@st.cache_data\n",
    "def load_player_data(start_year, end_year):\n",
    "    player_data = pd.DataFrame()\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        data = fetch_season_data_by_year(year)\n",
    "        if data is not None:\n",
    "            player_data = pd.concat([player_data, data], ignore_index=True)\n",
    "    return player_data\n",
    "\n",
    "def identify_overpaid_underpaid(predictions_df):\n",
    "    # Adjust Predicted_Salary calculation\n",
    "    predictions_df['Predicted_Salary'] = predictions_df['Predicted_Salary'] * predictions_df['Salary_Cap_Inflated']\n",
    "    \n",
    "    predictions_df['Salary_Difference'] = predictions_df['Salary'] - predictions_df['Predicted_Salary']\n",
    "    predictions_df['Overpaid'] = predictions_df['Salary_Difference'] > 0\n",
    "    predictions_df['Underpaid'] = predictions_df['Salary_Difference'] < 0\n",
    "    \n",
    "    overpaid = predictions_df[predictions_df['Overpaid']].sort_values('Salary_Difference', ascending=False)\n",
    "    underpaid = predictions_df[predictions_df['Underpaid']].sort_values('Salary_Difference')\n",
    "    \n",
    "    return overpaid.head(10), underpaid.head(10)\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def load_processed_data(file_path):\n",
    "    data = load_data(file_path)\n",
    "    data = format_season(data)\n",
    "    data = clean_data(data)\n",
    "    data = engineer_features(data)\n",
    "    return data\n",
    "\n",
    "def filter_data_by_season(data, season):\n",
    "    return data[data['Season'] == season]\n",
    "\n",
    "# Data visualization functions\n",
    "def plot_feature_distribution(data, feature):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.histplot(data[feature], kde=True, ax=ax)\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    return fig\n",
    "\n",
    "def plot_correlation_heatmap(data):\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    corr = numeric_data.corr()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm', ax=ax)\n",
    "    ax.set_title('Correlation Heatmap')\n",
    "    return fig\n",
    "\n",
    "# Model metrics function\n",
    "def display_model_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    st.subheader(\"Model Performance Metrics\")\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    col1.metric(\"Mean Squared Error\", f\"{mse:.4f}\")\n",
    "    col2.metric(\"Root Mean Squared Error\", f\"{rmse:.4f}\")\n",
    "    col3.metric(\"Mean Absolute Error\", f\"{mae:.4f}\")\n",
    "    col4.metric(\"R-squared\", f\"{r2:.4f}\")\n",
    "\n",
    "# Trade impact display function\n",
    "def display_trade_impact(result, team1, team2):\n",
    "    for team_abbr in [team1, team2]:\n",
    "        st.subheader(f\"{team_abbr} Trade Impact\")\n",
    "        \n",
    "        team_data = result[team_abbr]\n",
    "        \n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        col1.metric(\"Current Salary\", f\"${team_data['current_salary']:,.2f}\")\n",
    "        col2.metric(\"Salary After Trade\", f\"${team_data['new_salary']:,.2f}\")\n",
    "        col3.metric(\"Salary Difference\", f\"${team_data['new_salary'] - team_data['current_salary']:,.2f}\")\n",
    "        \n",
    "        st.subheader(\"Stat Comparisons\")\n",
    "        \n",
    "        # Create a DataFrame for the main stat comparisons\n",
    "        comparison_data = []\n",
    "        for stat, values in team_data['comparison'].items():\n",
    "            comparison_data.append({\n",
    "                'Stat': stat,\n",
    "                'Current': f\"{values['Current']:.2f} ({values['Current Percentile']:.1f}%ile)\",\n",
    "                'After Trade': f\"{values['After Trade']:.2f} ({values['After Trade Percentile']:.1f}%ile)\",\n",
    "                'Champion Average': f\"{values['Champ Average']:.2f}\",\n",
    "                'League Average': f\"{values['League Average']:.2f}\",\n",
    "                'Change vs League': f\"{values['After Trade vs League'] - values['Current vs League']:.2f}\",\n",
    "                'Change vs Champ': f\"{values['After Trade vs Champ'] - values['Current vs Champ']:.2f}\"\n",
    "            })\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        st.table(comparison_df)\n",
    "        \n",
    "        st.subheader(\"Percentile Counts\")\n",
    "        percentile_data = []\n",
    "        for stat, values in team_data['comparison'].items():\n",
    "            stat_data = {'Stat': stat}\n",
    "            for percentile in [99, 98, 97, 96, 95, 90, 75, 50]:\n",
    "                percentile_key = f\"Top {100-percentile}%\"\n",
    "                stat_data[f\"Current {percentile_key}\"] = values['Current Percentile Counts'][percentile_key]\n",
    "                stat_data[f\"After Trade {percentile_key}\"] = values['After Trade Percentile Counts'][percentile_key]\n",
    "                stat_data[f\"Champion {percentile_key}\"] = values['Champ Percentile Counts'][percentile_key]\n",
    "            percentile_data.append(stat_data)\n",
    "        \n",
    "        percentile_df = pd.DataFrame(percentile_data)\n",
    "        st.table(percentile_df)\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "\n",
    "def display_overpaid_underpaid(predictions_df):\n",
    "    st.subheader(\"Top 10 Overpaid and Underpaid Players\")\n",
    "\n",
    "    # Add filters\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        team_filter = st.multiselect(\"Filter by Team\", options=sorted(predictions_df['Team'].unique()))\n",
    "    with col2:\n",
    "        position_filter = st.multiselect(\"Filter by Position\", options=sorted(predictions_df['Position'].unique()))\n",
    "\n",
    "    # Apply filters\n",
    "    filtered_df = predictions_df\n",
    "    if team_filter:\n",
    "        filtered_df = filtered_df[filtered_df['Team'].isin(team_filter)]\n",
    "    if position_filter:\n",
    "        filtered_df = filtered_df[filtered_df['Position'].isin(position_filter)]\n",
    "\n",
    "    # Identify overpaid and underpaid players\n",
    "    overpaid, underpaid = identify_overpaid_underpaid(filtered_df)\n",
    "\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.subheader(\"Top 10 Overpaid Players\")\n",
    "        st.dataframe(overpaid[['Player', 'Team', 'Position', 'Salary', 'Predicted_Salary', 'Salary_Difference']])\n",
    "\n",
    "    with col2:\n",
    "        st.subheader(\"Top 10 Underpaid Players\")\n",
    "        st.dataframe(underpaid[['Player', 'Team', 'Position', 'Salary', 'Predicted_Salary', 'Salary_Difference']])\n",
    "\n",
    "\n",
    "# Main Streamlit app\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"NBA Salary Prediction and Trade Analysis\", layout=\"wide\")\n",
    "    st.title(\"NBA Salary Prediction and Trade Analysis\")\n",
    "\n",
    "    # Sidebar navigation\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    page = st.sidebar.radio(\"Go to\", [\"Data Analysis\", \"Model Results\", \"Salary Evaluation\", \"Trade Analysis\", \"Trade Impact Simulator\"])\n",
    "    \n",
    "    # Load base data\n",
    "    data = load_processed_data('data/processed/nba_player_data_final_inflated.csv')\n",
    "\n",
    "    # Load existing predictions for 2023\n",
    "    initial_predictions_df = pd.read_csv('data/processed/predictions_df.csv')\n",
    "\n",
    "    # Season selection\n",
    "    seasons = sorted(data['Season'].unique(), reverse=True)\n",
    "    selected_season = st.selectbox(\"Select Season\", seasons)\n",
    "\n",
    "    # Load models at the beginning of main()\n",
    "    model_save_path = 'data/models'\n",
    "    rf_model = joblib.load(f\"{model_save_path}/best_rf_model.pkl\")\n",
    "    xgb_model = joblib.load(f\"{model_save_path}/best_xgb_model.pkl\")\n",
    "\n",
    "    # Use initial predictions if 2023 is selected, otherwise retrain\n",
    "    if selected_season == 2023:\n",
    "        predictions_df = initial_predictions_df\n",
    "    else:\n",
    "        # Train model and make predictions\n",
    "        train_data = data[data['Season'] < selected_season]\n",
    "        test_data = data[data['Season'] == selected_season]\n",
    "\n",
    "        # Prepare the data for training\n",
    "        X_train = train_data.drop(['SalaryPct', 'Salary', 'Player'], axis=1)\n",
    "        y_train = train_data['SalaryPct']\n",
    "\n",
    "        # Encode the training data\n",
    "        X_train_encoded, _, encoders, scaler, numeric_cols, player_encoder = encode_data(X_train)\n",
    "\n",
    "        # Train and save models\n",
    "        train_and_save_models(X_train_encoded, y_train, model_save_path, scaler, X_train_encoded.columns, encoders, player_encoder, numeric_cols)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        predictions_df = predict(test_data, model_save_path)\n",
    "\n",
    "\n",
    "\n",
    "    if page == \"Data Analysis\":\n",
    "        st.header(\"Data Analysis\")\n",
    "\n",
    "        # Filter data by selected season\n",
    "        season_data = filter_data_by_season(data, selected_season)\n",
    "\n",
    "        # Display basic statistics\n",
    "        st.subheader(\"Basic Statistics\")\n",
    "        st.write(season_data.describe())\n",
    "\n",
    "        # Feature distribution\n",
    "        st.subheader(\"Feature Distribution\")\n",
    "        feature = st.selectbox(\"Select Feature\", season_data.columns)\n",
    "        fig = plot_feature_distribution(season_data, feature)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # Correlation heatmap\n",
    "        st.subheader(\"Correlation Heatmap\")\n",
    "        fig = plot_correlation_heatmap(season_data)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # Data handling explanation\n",
    "        st.subheader(\"Data Handling\")\n",
    "        st.write(\"\"\"\n",
    "        We preprocessed the data to ensure it's suitable for our models:\n",
    "        1. Cleaned missing values and outliers\n",
    "        2. Engineered new features like PPG, APG, etc.\n",
    "        3. Encoded categorical variables (Position, Team, Injury Risk)\n",
    "        4. Scaled numerical features\n",
    "        \"\"\")\n",
    "\n",
    "    elif page == \"Model Results\":\n",
    "        st.header(\"Model Results\")\n",
    "\n",
    "        # Model selection\n",
    "        model_choice = st.selectbox(\"Select Model\", [\"Random Forest\", \"XGBoost\"])\n",
    "\n",
    "        if model_choice == \"Random Forest\":\n",
    "            model = rf_model\n",
    "            y_pred = predictions_df['RF_Predictions']\n",
    "        else:\n",
    "            model = xgb_model\n",
    "            y_pred = predictions_df['XGB_Predictions']\n",
    "\n",
    "        # Display model metrics\n",
    "        display_model_metrics(predictions_df['SalaryPct'], y_pred)\n",
    "\n",
    "        # Feature importance\n",
    "        st.subheader(\"Feature Importance\")\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': model.feature_names_in_,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        st.bar_chart(feature_importance.set_index('feature'))\n",
    "\n",
    "\n",
    "        # Model explanation\n",
    "        st.subheader(\"Model Explanation\")\n",
    "        st.write(f\"\"\"\n",
    "        The {model_choice} model was trained on historical NBA player data to predict salary percentages.\n",
    "        We used the following techniques to improve model performance:\n",
    "        1. Feature engineering to create relevant statistics\n",
    "        2. Proper encoding of categorical variables\n",
    "        3. Scaling of numerical features\n",
    "        4. Hyperparameter tuning using GridSearchCV\n",
    "        \"\"\")\n",
    "        \n",
    "    elif page == \"Salary Evaluation\":\n",
    "        st.header(\"Salary Evaluation\")\n",
    "        display_overpaid_underpaid(predictions_df)\n",
    "\n",
    "    elif page == \"Trade Analysis\":\n",
    "        st.header(\"Trade Analysis\")\n",
    "        st.write(\"\"\"\n",
    "        Analyze potential trades and their impact on team statistics and salary cap.\n",
    "        For more information on trade rules, visit: [NBA Trade Rules](https://www.hoopsrumors.com/2023/09/salary-matching-rules-for-trades-during-2023-24-season.html)\n",
    "        \"\"\")\n",
    "\n",
    "        # Team selection\n",
    "        teams = sorted(predictions_df['Team'].unique())\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            team1 = st.selectbox(\"Select Team 1\", teams)\n",
    "        with col2:\n",
    "            team2 = st.selectbox(\"Select Team 2\", teams, index=1)\n",
    "\n",
    "        # Player selection\n",
    "        team1_players = predictions_df[predictions_df['Team'] == team1]['Player'].tolist()\n",
    "        team2_players = predictions_df[predictions_df['Team'] == team2]['Player'].tolist()\n",
    "\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            players_leaving_team1 = st.multiselect(f\"Select players leaving {team1}\", team1_players)\n",
    "        with col2:\n",
    "            players_leaving_team2 = st.multiselect(f\"Select players leaving {team2}\", team2_players)\n",
    "\n",
    "        if st.button(\"Analyze Trade\"):\n",
    "            champions = get_champions(selected_season - 10, selected_season - 1)\n",
    "            result = analyze_two_team_trade(team1, team2, players_leaving_team1, players_leaving_team2, predictions_df, champions)\n",
    "            \n",
    "            if result:\n",
    "                display_trade_impact(result, team1, team2)\n",
    "            else:\n",
    "                st.error(\"Trade analysis failed. Please check your selections.\")\n",
    "\n",
    "        # Trade analysis explanation\n",
    "        st.subheader(\"Trade Analysis Explanation\")\n",
    "        st.write(\"\"\"\n",
    "        Our trade analysis compares team statistics before and after the proposed trade.\n",
    "        We consider:\n",
    "        1. Changes in key performance metrics (PPG, RPG, APG, etc.)\n",
    "        2. Salary implications and cap space impact\n",
    "        3. Comparison to league averages and recent championship teams\n",
    "        4. Distribution of top performers in various statistical categories\n",
    "        5. Overpaid/Underpaid player analysis\n",
    "        \"\"\")\n",
    "\n",
    "    elif page == \"Trade Impact Simulator\":\n",
    "        trade_impact_simulator()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
