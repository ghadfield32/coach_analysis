{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/coach_analysis/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Dataframe shape: (10660, 60)\n",
      "\n",
      "Columns: Index(['Player', 'Salary', 'Season', 'Position', 'Age', 'Team', 'TeamID',\n",
      "       'Years of Service', 'GP', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA',\n",
      "       '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB',\n",
      "       'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PER', 'TS%', '3PAr',\n",
      "       'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%',\n",
      "       'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP', 'Wins',\n",
      "       'Losses', 'Min Salary', 'Max Salary', 'Salary Cap', '2022 Dollars',\n",
      "       'Salary_Cap_Inflated'],\n",
      "      dtype='object')\n",
      "\n",
      "First few rows:\n",
      "          Player    Salary  Season       Position   Age Team        TeamID  \\\n",
      "0  Stephen Curry  51915615    2023          Guard  36.0  GSW  1.610613e+09   \n",
      "1   Jrue Holiday  37620347    2023          Guard  34.0  BOS  1.610613e+09   \n",
      "2   James Harden  35680595    2023          Guard  34.0  LAC  1.610613e+09   \n",
      "3  DeMar DeRozan  28600000    2023  Guard-Forward  34.0  CHI  1.610613e+09   \n",
      "4     Taj Gibson   4776302    2023        Forward  39.0  NYK  1.610613e+09   \n",
      "\n",
      "   Years of Service    GP    GS  ...  DBPM  BPM  VORP  Wins  Losses  \\\n",
      "0              14.0  74.0  74.0  ...  -1.1  5.2   4.4  46.0    36.0   \n",
      "1              14.0  69.0  69.0  ...   1.1  2.1   2.4  64.0    18.0   \n",
      "2              14.0  72.0  72.0  ...   0.3  4.1   3.8  51.0    31.0   \n",
      "3              14.0  79.0  79.0  ...  -0.3  1.8   2.8  39.0    43.0   \n",
      "4              14.0  16.0   1.0  ...   0.1 -5.3  -0.2  14.0    68.0   \n",
      "\n",
      "   Min Salary  Max Salary  Salary Cap  2022 Dollars  Salary_Cap_Inflated  \n",
      "0   3196448.0  47607350.0   136021000           NaN            136021000  \n",
      "1   3196448.0  47607350.0   136021000           NaN            136021000  \n",
      "2   3196448.0  47607350.0   136021000           NaN            136021000  \n",
      "3   3196448.0  47607350.0   136021000           NaN            136021000  \n",
      "4   3196448.0  47607350.0   136021000           NaN            136021000  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "\n",
      "NaN values:\n",
      "Player                   0\n",
      "Salary                   0\n",
      "Season                   0\n",
      "Position                93\n",
      "Age                      0\n",
      "Team                     0\n",
      "TeamID                   0\n",
      "Years of Service         0\n",
      "GP                       0\n",
      "GS                       0\n",
      "MP                       0\n",
      "FG                       0\n",
      "FGA                      0\n",
      "FG%                      0\n",
      "3P                       0\n",
      "3PA                      0\n",
      "3P%                      0\n",
      "2P                       0\n",
      "2PA                      0\n",
      "2P%                      0\n",
      "eFG%                     0\n",
      "FT                       0\n",
      "FTA                      0\n",
      "FT%                      0\n",
      "ORB                      0\n",
      "DRB                      0\n",
      "TRB                      0\n",
      "AST                      0\n",
      "STL                      0\n",
      "BLK                      0\n",
      "TOV                      0\n",
      "PF                       0\n",
      "PTS                      0\n",
      "PER                    168\n",
      "TS%                    198\n",
      "3PAr                   200\n",
      "FTr                    200\n",
      "ORB%                   168\n",
      "DRB%                   168\n",
      "TRB%                   168\n",
      "AST%                   168\n",
      "STL%                   168\n",
      "BLK%                   168\n",
      "TOV%                   192\n",
      "USG%                   168\n",
      "OWS                    165\n",
      "DWS                    165\n",
      "WS                     165\n",
      "WS/48                  168\n",
      "OBPM                   165\n",
      "DBPM                   165\n",
      "BPM                    165\n",
      "VORP                   165\n",
      "Wins                   926\n",
      "Losses                 926\n",
      "Min Salary               0\n",
      "Max Salary               0\n",
      "Salary Cap               0\n",
      "2022 Dollars           991\n",
      "Salary_Cap_Inflated      0\n",
      "dtype: int64\n",
      "\n",
      "Loading predictions...\n",
      "Debug: df_actual shape: (10660, 60)\n",
      "Debug: df_actual columns: Index(['Player', 'Salary', 'Season', 'Position', 'Age', 'Team', 'TeamID',\n",
      "       'Years of Service', 'GP', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA',\n",
      "       '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB',\n",
      "       'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PER', 'TS%', '3PAr',\n",
      "       'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%',\n",
      "       'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP', 'Wins',\n",
      "       'Losses', 'Min Salary', 'Max Salary', 'Salary Cap', '2022 Dollars',\n",
      "       'Salary_Cap_Inflated'],\n",
      "      dtype='object')\n",
      "Debug: Unique teams in df_actual: ['GSW' 'BOS' 'LAC' 'CHI' 'NYK' 'IND' 'PHI' 'PHX' 'MIN' 'TOR' 'HOU' 'DEN'\n",
      " 'POR' 'UTA' 'ATL' 'BKN' 'DET' 'MEM' 'SAS' 'ORL' 'OKC' 'DAL' 'MIL' 'LAL'\n",
      " 'SAC' 'CHA' 'MIA' 'NOP' 'CLE' 'WAS' 'NOH' 'NJN' 'SEA' 'NOK' 'CHH' 'VAN']\n",
      "Debug: df_predictions shape: (2159, 7)\n",
      "Debug: df_predictions columns: Index(['Player', 'Predicted_Season', 'Age', 'Predicted_Salary_Pct',\n",
      "       'Predicted_Salary', 'Previous_Season_Salary', 'Salary_Change'],\n",
      "      dtype='object')\n",
      "Debug: df_merged shape after merge: (2159, 9)\n",
      "Debug: df_merged columns after merge: Index(['Player', 'Predicted_Season', 'Age', 'Predicted_Salary_Pct',\n",
      "       'Predicted_Salary', 'Previous_Season_Salary', 'Salary_Change', 'Team',\n",
      "       'Season'],\n",
      "      dtype='object')\n",
      "Debug: df_merged columns after renaming: Index(['Player', 'Original_Season', 'Age', 'Predicted_Salary_Pct',\n",
      "       'Predicted_Salary', 'Previous_Season_Salary', 'Salary_Change', 'Team',\n",
      "       'Season'],\n",
      "      dtype='object')\n",
      "Debug: Sample of merged data:\n",
      "          Player  Season Team  Original_Season\n",
      "0  Stephen Curry     NaN  NaN             2024\n",
      "1   Jrue Holiday     NaN  NaN             2024\n",
      "2   James Harden     NaN  NaN             2024\n",
      "3  DeMar DeRozan     NaN  NaN             2024\n",
      "4     Taj Gibson     NaN  NaN             2024\n",
      "Debug: Unique teams in df_merged: [nan]\n",
      "Debug: Number of unique teams in df_merged: 0\n",
      "\n",
      "Predictions shape: (2159, 9)\n",
      "\n",
      "Predictions columns: Index(['Player', 'Original_Season', 'Age', 'Predicted_Salary_Pct',\n",
      "       'Predicted_Salary', 'Previous_Season_Salary', 'Salary_Change', 'Team',\n",
      "       'Season'],\n",
      "      dtype='object')\n",
      "\n",
      "First few rows of predictions:\n",
      "          Player  Original_Season  Age  Predicted_Salary_Pct  \\\n",
      "0  Stephen Curry             2024   37              0.300236   \n",
      "1   Jrue Holiday             2024   35              0.218131   \n",
      "2   James Harden             2024   35              0.276265   \n",
      "3  DeMar DeRozan             2024   35              0.269226   \n",
      "4     Taj Gibson             2024   40              0.049338   \n",
      "\n",
      "   Predicted_Salary  Previous_Season_Salary  Salary_Change Team  Season  \n",
      "0      4.083837e+07              51915615.0  -1.107724e+07  NaN     NaN  \n",
      "1      2.967045e+07              37620347.0  -7.949901e+06  NaN     NaN  \n",
      "2      3.757787e+07              35680595.0   1.897274e+06  NaN     NaN  \n",
      "3      3.662045e+07              28600000.0   8.020449e+06  NaN     NaN  \n",
      "4      6.711018e+06               4776302.0   1.934716e+06  NaN     NaN  \n",
      "\n",
      "NaN values in predictions:\n",
      "Player                       0\n",
      "Original_Season              0\n",
      "Age                          0\n",
      "Predicted_Salary_Pct         0\n",
      "Predicted_Salary             0\n",
      "Previous_Season_Salary       0\n",
      "Salary_Change                0\n",
      "Team                      2159\n",
      "Season                    2159\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ../src/salary_predict/data_loader.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_project_root():\n",
    "    \"\"\"\n",
    "    This function returns the path to the project root directory, \n",
    "    which is assumed to be the directory containing this file or its ancestors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the __file__ attribute to determine the current directory\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # Fallback to current working directory if __file__ is not available\n",
    "        current_dir = os.getcwd()\n",
    "\n",
    "    # Define the expected name of the root directory\n",
    "    root_dir_name = 'coach_analysis'\n",
    "    \n",
    "    # Traverse upwards in the directory hierarchy to find the root directory\n",
    "    while True:\n",
    "        if os.path.basename(current_dir) == root_dir_name:\n",
    "            return current_dir\n",
    "        parent_dir = os.path.dirname(current_dir)\n",
    "        if parent_dir == current_dir:\n",
    "            raise FileNotFoundError(f\"Root directory '{root_dir_name}' not found.\")\n",
    "        current_dir = parent_dir\n",
    "        \n",
    "def load_data(inflated=False):\n",
    "    root_dir = get_project_root()\n",
    "    file_name = 'final_salary_data_with_yos_and_inflated_cap_2000_on.csv'\n",
    "    file_path = os.path.join(root_dir, 'data', 'processed', file_name)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist. Please check the file path.\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'Salary' not in df.columns:\n",
    "        raise KeyError(\"The 'Salary' column is missing in the dataset.\")\n",
    "    \n",
    "    # Convert 'Season' to the correct format if necessary\n",
    "    if df['Season'].dtype == 'object':\n",
    "        df['Season'] = df['Season'].str[:4].astype(int)\n",
    "    \n",
    "    # Ensure both 'Salary Cap' and 'Salary_Cap_Inflated' columns are present\n",
    "    if 'Salary Cap' not in df.columns:\n",
    "        raise KeyError(\"The 'Salary Cap' column is missing in the dataset.\")\n",
    "    if 'Salary_Cap_Inflated' not in df.columns:\n",
    "        df['Salary_Cap_Inflated'] = df['Salary Cap']  # Use non-inflated as fallback\n",
    "    \n",
    "    # Use the appropriate salary cap column based on the 'inflated' parameter\n",
    "    if inflated:\n",
    "        df['Salary Cap'] = df['Salary_Cap_Inflated']\n",
    "    else:\n",
    "        df['Salary_Cap_Inflated'] = df['Salary Cap']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_predictions(inflated=False, team=None):\n",
    "    # Load the actual data\n",
    "    df_actual = load_data(inflated)\n",
    "    \n",
    "    print(\"Debug: df_actual shape:\", df_actual.shape)\n",
    "    print(\"Debug: df_actual columns:\", df_actual.columns)\n",
    "    print(\"Debug: Unique teams in df_actual:\", df_actual['Team'].unique())\n",
    "    \n",
    "    # Load predictions\n",
    "    root_dir = get_project_root()\n",
    "    predictions_file = 'salary_predictions_inflated.csv' if inflated else 'salary_predictions.csv'\n",
    "    predictions_path = os.path.join(root_dir, 'data', 'predictions', predictions_file)\n",
    "    \n",
    "    if not os.path.exists(predictions_path):\n",
    "        raise FileNotFoundError(f\"The predictions file {predictions_path} does not exist.\")\n",
    "    \n",
    "    df_predictions = pd.read_csv(predictions_path)\n",
    "    \n",
    "    print(\"Debug: df_predictions shape:\", df_predictions.shape)\n",
    "    print(\"Debug: df_predictions columns:\", df_predictions.columns)\n",
    "    \n",
    "    # Merge predictions with actual data to get team information\n",
    "    df_merged = pd.merge(df_predictions, df_actual[['Player', 'Team', 'Season']], \n",
    "                         left_on=['Player', 'Predicted_Season'], \n",
    "                         right_on=['Player', 'Season'], \n",
    "                         how='left')\n",
    "    \n",
    "    print(\"Debug: df_merged shape after merge:\", df_merged.shape)\n",
    "    print(\"Debug: df_merged columns after merge:\", df_merged.columns)\n",
    "    \n",
    "    # Handle the 'Season' columns\n",
    "    if 'Season_x' in df_merged.columns and 'Season_y' in df_merged.columns:\n",
    "        df_merged = df_merged.rename(columns={'Season_x': 'Original_Season', 'Season_y': 'Season'})\n",
    "    elif 'Season' in df_merged.columns and 'Predicted_Season' in df_merged.columns:\n",
    "        df_merged = df_merged.rename(columns={'Predicted_Season': 'Original_Season'})\n",
    "    \n",
    "    # Drop any duplicate columns\n",
    "    df_merged = df_merged.loc[:, ~df_merged.columns.duplicated()]\n",
    "    \n",
    "    print(\"Debug: df_merged columns after renaming:\", df_merged.columns)\n",
    "    print(\"Debug: Sample of merged data:\")\n",
    "    print(df_merged[['Player', 'Season', 'Team', 'Original_Season']].head())\n",
    "    \n",
    "    # Ensure all required columns are present\n",
    "    required_columns = ['Player', 'Season', 'Team', 'Age', 'Predicted_Salary', 'Previous_Season_Salary', 'Salary_Change']\n",
    "    missing_columns = [col for col in required_columns if col not in df_merged.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Debug: Missing columns: {missing_columns}\")\n",
    "        raise KeyError(f\"The following required columns are missing in the merged dataframe: {', '.join(missing_columns)}\")\n",
    "    \n",
    "    print(\"Debug: Unique teams in df_merged:\", df_merged['Team'].unique())\n",
    "    print(\"Debug: Number of unique teams in df_merged:\", df_merged['Team'].nunique())\n",
    "    \n",
    "    # Filter by team if specified\n",
    "    if team:\n",
    "        df_merged = df_merged[df_merged['Team'] == team]\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "def merge_predictions_with_original(predictions, original_data):\n",
    "    merged = predictions.merge(original_data[['Player', 'Position']], on='Player', how='left')\n",
    "    merged['Position'] = merged['Position'].fillna('Unknown')\n",
    "    merged.rename(columns={'Previous_Season_Salary': 'Salary'}, inplace=True)\n",
    "    return merged\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_project_root()\n",
    "    # Example usage\n",
    "    print(\"Loading data...\")\n",
    "    df = load_data(inflated=False)\n",
    "    print(\"\\nDataframe shape:\", df.shape)\n",
    "    print(\"\\nColumns:\", df.columns)\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nNaN values:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    print(\"\\nLoading predictions...\")\n",
    "    predictions = load_predictions(inflated=False)\n",
    "    print(\"\\nPredictions shape:\", predictions.shape)\n",
    "    print(\"\\nPredictions columns:\", predictions.columns)\n",
    "    print(\"\\nFirst few rows of predictions:\")\n",
    "    print(predictions.head())\n",
    "    print(\"\\nNaN values in predictions:\")\n",
    "    print(predictions.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Handling missing values...\n",
      "Number of numeric columns: 57\n",
      "Numeric columns: Index(['Salary', 'Season', 'Age', 'TeamID', 'Years of Service', 'GP', 'GS',\n",
      "       'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%',\n",
      "       'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK',\n",
      "       'TOV', 'PF', 'PTS', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%',\n",
      "       'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48',\n",
      "       'OBPM', 'DBPM', 'BPM', 'VORP', 'Wins', 'Losses', 'Min Salary',\n",
      "       'Max Salary', 'Salary Cap', '2022 Dollars', 'Salary_Cap_Inflated'],\n",
      "      dtype='object')\n",
      "Number of numeric columns after dropping all-NaN columns: 57\n",
      "Shape of imputed data: (10660, 57)\n",
      "Shape of original numeric data: (10660, 57)\n",
      "NaN values after handling:\n",
      "Player                  0\n",
      "Salary                  0\n",
      "Season                  0\n",
      "Position               93\n",
      "Age                     0\n",
      "Team                    0\n",
      "TeamID                  0\n",
      "Years of Service        0\n",
      "GP                      0\n",
      "GS                      0\n",
      "MP                      0\n",
      "FG                      0\n",
      "FGA                     0\n",
      "FG%                     0\n",
      "3P                      0\n",
      "3PA                     0\n",
      "3P%                     0\n",
      "2P                      0\n",
      "2PA                     0\n",
      "2P%                     0\n",
      "eFG%                    0\n",
      "FT                      0\n",
      "FTA                     0\n",
      "FT%                     0\n",
      "ORB                     0\n",
      "DRB                     0\n",
      "TRB                     0\n",
      "AST                     0\n",
      "STL                     0\n",
      "BLK                     0\n",
      "TOV                     0\n",
      "PF                      0\n",
      "PTS                     0\n",
      "PER                     0\n",
      "TS%                     0\n",
      "3PAr                    0\n",
      "FTr                     0\n",
      "ORB%                    0\n",
      "DRB%                    0\n",
      "TRB%                    0\n",
      "AST%                    0\n",
      "STL%                    0\n",
      "BLK%                    0\n",
      "TOV%                    0\n",
      "USG%                    0\n",
      "OWS                     0\n",
      "DWS                     0\n",
      "WS                      0\n",
      "WS/48                   0\n",
      "OBPM                    0\n",
      "DBPM                    0\n",
      "BPM                     0\n",
      "VORP                    0\n",
      "Wins                    0\n",
      "Losses                  0\n",
      "Min Salary              0\n",
      "Max Salary              0\n",
      "Salary Cap              0\n",
      "2022 Dollars            0\n",
      "Salary_Cap_Inflated     0\n",
      "dtype: int64\n",
      "\n",
      "Performing feature engineering...\n",
      "New columns after feature engineering: Index(['Player', 'Salary', 'Season', 'Position', 'Age', 'Team', 'TeamID',\n",
      "       'Years of Service', 'GP', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA',\n",
      "       '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB',\n",
      "       'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PER', 'TS%', '3PAr',\n",
      "       'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%',\n",
      "       'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP', 'Wins',\n",
      "       'Losses', 'Min Salary', 'Max Salary', 'Salary Cap', '2022 Dollars',\n",
      "       'Salary_Cap_Inflated', 'PPG', 'APG', 'RPG', 'SPG', 'BPG', 'TOPG',\n",
      "       'WinPct', 'Availability', 'SalaryPct'],\n",
      "      dtype='object')\n",
      "\n",
      "Calculating VORP salary ratio...\n",
      "VORP salary ratio stats:\n",
      "count    10660.000000\n",
      "mean         0.094289\n",
      "std          0.677130\n",
      "min        -13.021116\n",
      "25%         -0.058824\n",
      "50%          0.037872\n",
      "75%          0.214092\n",
      "max         22.960857\n",
      "Name: VORP_Salary_Ratio, dtype: float64\n",
      "\n",
      "Clustering career trajectories...\n",
      "Cluster distribution:\n",
      "Cluster_Definition\n",
      "Rising Role Players    3813\n",
      "Young Bench Players    2572\n",
      "Veteran Players        2022\n",
      "Superstars             1700\n",
      "Star Players            553\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ../src/salary_predict/data_preprocessor.py\n",
    "\n",
    "# data_preprocessor.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    df = df.copy()\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"Number of numeric columns: {len(numeric_columns)}\")\n",
    "    print(f\"Numeric columns: {numeric_columns}\")\n",
    "    \n",
    "    # Remove columns with all NaN values\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"Number of numeric columns after dropping all-NaN columns: {len(numeric_columns)}\")\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    imputed_data = imputer.fit_transform(df[numeric_columns])\n",
    "    print(f\"Shape of imputed data: {imputed_data.shape}\")\n",
    "    print(f\"Shape of original numeric data: {df[numeric_columns].shape}\")\n",
    "    \n",
    "    df[numeric_columns] = imputed_data\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df, use_inflated_data=False):\n",
    "    df = df.copy()\n",
    "    # Calculate per-game stats if not already present\n",
    "    if 'PPG' not in df.columns:\n",
    "        df['PPG'] = df['PTS'] / df['GP']\n",
    "    if 'APG' not in df.columns:\n",
    "        df['APG'] = df['AST'] / df['GP']\n",
    "    if 'RPG' not in df.columns:\n",
    "        df['RPG'] = df['TRB'] / df['GP']\n",
    "    if 'SPG' not in df.columns:\n",
    "        df['SPG'] = df['STL'] / df['GP']\n",
    "    if 'BPG' not in df.columns:\n",
    "        df['BPG'] = df['BLK'] / df['GP']\n",
    "    if 'TOPG' not in df.columns:\n",
    "        df['TOPG'] = df['TOV'] / df['GP']\n",
    "    \n",
    "    # Calculate win percentage if not already present\n",
    "    if 'WinPct' not in df.columns:\n",
    "        df['WinPct'] = df['Wins'] / (df['Wins'] + df['Losses'])\n",
    "    \n",
    "    # Calculate availability if not already present\n",
    "    if 'Availability' not in df.columns:\n",
    "        df['Availability'] = df['GP'] / 82\n",
    "    \n",
    "    # Calculate SalaryPct using the correct Salary Cap column\n",
    "    salary_cap_column = 'Salary_Cap_Inflated' if use_inflated_data else 'Salary Cap'\n",
    "    if salary_cap_column not in df.columns:\n",
    "        raise KeyError(f\"The '{salary_cap_column}' column is missing in the dataset.\")\n",
    "    df['SalaryPct'] = df['Salary'] / df[salary_cap_column]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_vorp_salary_ratio(df):\n",
    "    df['Salary_M'] = df['Salary'] / 1e6\n",
    "    if 'VORP' in df.columns:\n",
    "        df['VORP_Salary_Ratio'] = df['VORP'] / df['Salary_M']\n",
    "    else:\n",
    "        print(\"Warning: 'VORP' column not found. VORP/Salary ratio cannot be calculated.\")\n",
    "    return df\n",
    "\n",
    "def cluster_career_trajectories(df):\n",
    "    features = ['Age', 'Years of Service', 'PTS', 'TRB', 'AST', 'PER', 'WS', 'VORP']\n",
    "    X = df[features]\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Adding cluster definitions\n",
    "    cluster_definitions = {\n",
    "        0: \"Young Bench Players\",\n",
    "        1: \"Rising Role Players\",\n",
    "        2: \"Star Players\",\n",
    "        3: \"Superstars\",\n",
    "        4: \"Veteran Players\"\n",
    "    }\n",
    "    \n",
    "    df['Cluster_Definition'] = df['Cluster'].map(cluster_definitions)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from data_loader import load_data\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    df = load_data(inflated=False)\n",
    "    \n",
    "    print(\"\\nHandling missing values...\")\n",
    "    df = handle_missing_values(df)\n",
    "    print(\"NaN values after handling:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    print(\"\\nPerforming feature engineering...\")\n",
    "    df = feature_engineering(df)\n",
    "    print(\"New columns after feature engineering:\", df.columns)\n",
    "    \n",
    "    print(\"\\nCalculating VORP salary ratio...\")\n",
    "    df = calculate_vorp_salary_ratio(df)\n",
    "    print(\"VORP salary ratio stats:\")\n",
    "    print(df['VORP_Salary_Ratio'].describe())\n",
    "    \n",
    "    print(\"\\nClustering career trajectories...\")\n",
    "    df = cluster_career_trajectories(df)\n",
    "    print(\"Cluster distribution:\")\n",
    "    print(df['Cluster_Definition'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/model_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/model_trainer.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "from sklearn.inspection import permutation_importance\n",
    "from data_loader import get_project_root, load_data\n",
    "import os\n",
    "\n",
    "def retrain_models(X, y, model_params):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    models = {}\n",
    "    for name, (model, params) in model_params.items():\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    return models, X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "def save_models(models, scaler, selected_features, inflated=False):\n",
    "    root_dir = get_project_root()\n",
    "    suffix = '_inflated' if inflated else ''\n",
    "    model_name_mapping = {\n",
    "        'Random Forest': 'Random_Forest',\n",
    "        'Gradient Boosting': 'Gradient_Boosting',\n",
    "        'Ridge Regression': 'Ridge_Regression',\n",
    "        'ElasticNet': 'ElasticNet',\n",
    "        'SVR': 'SVR',\n",
    "        'Decision Tree': 'Decision_Tree'\n",
    "    }\n",
    "    for name, model in models.items():\n",
    "        formatted_name = model_name_mapping[name]\n",
    "        joblib.dump(model, os.path.join(root_dir, 'data', 'models', f'{formatted_name}_salary_prediction_model{suffix}.joblib'))\n",
    "    joblib.dump(scaler, os.path.join(root_dir, 'data', 'models', f'scaler{suffix}.joblib'))\n",
    "    joblib.dump(selected_features, os.path.join(root_dir, 'data', 'models', f'selected_features{suffix}.joblib'))\n",
    "\n",
    "def evaluate_models(models, X_test, y_test):\n",
    "    evaluations = {}\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        evaluations[name] = {\"MSE\": mse, \"R²\": r2}\n",
    "    return evaluations\n",
    "\n",
    "def retrain_and_save_models(use_inflated_data):\n",
    "    # Load the appropriate data\n",
    "    data = load_data(use_inflated_data)\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['2022 Dollars', 'Luxury Tax', '1st Apron', '2nd Apron', 'BAE', 'Standard /Non-Taxpayer', 'Taxpayer', 'Team Room /Under Cap']\n",
    "    data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "\n",
    "    # Convert 'Season' to an integer if necessary\n",
    "    if data['Season'].dtype == 'object':\n",
    "        data['Season'] = data['Season'].str[:4].astype(int)\n",
    "\n",
    "    # Handle missing values for numerical columns\n",
    "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data[numerical_cols] = imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Feature engineering\n",
    "    data = feature_engineering(data, use_inflated_data)\n",
    "\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = ['Player', 'Season', 'Position', 'Team']\n",
    "    numerical_cols = data.columns.difference(categorical_cols + ['Salary', 'SalaryPct', 'Salary Cap', 'Salary_Cap_Inflated'])\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "    encoded_cats = pd.DataFrame(encoder.fit_transform(data[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # Combine the numerical and encoded categorical data\n",
    "    data = pd.concat([data[numerical_cols], encoded_cats, data[['Player', 'Season', 'Salary', 'SalaryPct', 'Salary Cap', 'Salary_Cap_Inflated']]], axis=1)\n",
    "\n",
    "    # Select initial features\n",
    "    initial_features = ['Age', 'Years of Service', 'GP', 'PPG', 'APG', 'RPG', 'SPG', 'BPG', 'TOPG', 'FG%', '3P%', 'FT%', 'PER', 'WS', 'VORP', 'Availability'] + list(encoded_cats.columns)\n",
    "\n",
    "    # Create a new DataFrame with only the features we're interested in and the target variable\n",
    "    data_subset = data[initial_features + ['SalaryPct']].copy()\n",
    "\n",
    "    # Drop rows with any missing values\n",
    "    data_cleaned = data_subset.dropna()\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = data_cleaned[initial_features]\n",
    "    y = data_cleaned['SalaryPct']\n",
    "\n",
    "    # Perform feature selection\n",
    "    rfe = RFE(estimator=RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=10)\n",
    "    rfe = rfe.fit(X, y)\n",
    "    selected_features = [feature for feature, selected in zip(initial_features, rfe.support_) if selected]\n",
    "\n",
    "    print(\"Selected features by RFE:\", selected_features)\n",
    "\n",
    "    X = data_cleaned[selected_features]\n",
    "    y = data_cleaned['SalaryPct']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define models with updated parameters\n",
    "    models = {\n",
    "        'Random_Forest': RandomForestRegressor(random_state=42),\n",
    "        'Gradient_Boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'Ridge_Regression': Ridge(),\n",
    "        'ElasticNet': ElasticNet(max_iter=10000),\n",
    "        'SVR': SVR(),\n",
    "        'Decision_Tree': DecisionTreeRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    # Define parameter grids\n",
    "    param_grids = {\n",
    "        'Random_Forest': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'max_depth': [8, 10, 12],\n",
    "            'min_samples_split': [5, 10, 15],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'Gradient_Boosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'Ridge_Regression': {'alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "        'ElasticNet': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]},\n",
    "        'SVR': {'C': [0.1, 1, 10], 'epsilon': [0.1, 0.2, 0.5]},\n",
    "        'Decision_Tree': {'max_depth': [6, 8, 10], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "    }\n",
    "\n",
    "    # Train and evaluate models\n",
    "    best_models = {}\n",
    "    evaluations = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(best_models[name], X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "        print(f\"{name} - Best params: {grid_search.best_params_}\")\n",
    "        print(f\"{name} - Cross-validation MSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Test set performance\n",
    "        y_pred = best_models[name].predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"{name} - Test MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "        \n",
    "        evaluations[name] = {\"MSE\": mse, \"R²\": r2}\n",
    "        \n",
    "        # Feature importance\n",
    "        if name in ['Random_Forest', 'Gradient_Boosting', 'Decision_Tree']:\n",
    "            importances = best_models[name].feature_importances_\n",
    "            feature_importance = pd.DataFrame({'feature': selected_features, 'importance': importances})\n",
    "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "            print(f\"\\n{name} - Top 5 important features:\")\n",
    "            print(feature_importance.head())\n",
    "        else:\n",
    "            perm_importance = permutation_importance(best_models[name], X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "            feature_importance = pd.DataFrame({'feature': selected_features, 'importance': perm_importance.importances_mean})\n",
    "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "            print(f\"\\n{name} - Top 5 important features (Permutation Importance):\")\n",
    "            print(feature_importance.head())\n",
    "        \n",
    "        # Save the model\n",
    "        root_dir = get_project_root()\n",
    "        suffix = '_inflated' if use_inflated_data else ''\n",
    "        model_filename = os.path.join(root_dir, 'data', 'models', f'{name}_salary_prediction_model{suffix}.joblib')\n",
    "        joblib.dump(best_models[name], model_filename)\n",
    "        print(f\"{name} model saved to '{model_filename}'\")\n",
    "\n",
    "    # Identify the best overall model\n",
    "    best_model_name = min(evaluations, key=lambda x: evaluations[x]['MSE'])\n",
    "    best_model = best_models[best_model_name]\n",
    "\n",
    "    print(f\"Best overall model: {best_model_name}\")\n",
    "\n",
    "    # Save the scaler, selected features, and best model name\n",
    "    root_dir = get_project_root()\n",
    "    suffix = '_inflated' if use_inflated_data else ''\n",
    "    \n",
    "    scaler_filename = os.path.join(root_dir, 'data', 'models', f'scaler{suffix}.joblib')\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"Scaler saved to '{scaler_filename}'\")\n",
    "    \n",
    "    selected_features_filename = os.path.join(root_dir, 'data', 'models', f'selected_features{suffix}.joblib')\n",
    "    joblib.dump(selected_features, selected_features_filename)\n",
    "    print(f\"Selected features saved to '{selected_features_filename}'\")\n",
    "    \n",
    "    with open(os.path.join(root_dir, 'data', 'models', f'best_model_name{suffix}.txt'), 'w') as f:\n",
    "        f.write(best_model_name)\n",
    "\n",
    "    return best_model_name, best_model, evaluations, selected_features, scaler, data[salary_cap_column].max()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Retraining models...\")\n",
    "    best_model_name, best_model, evaluations, selected_features, scaler, max_salary_cap = retrain_and_save_models(use_inflated_data=False)\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    print(\"\\nModel evaluations:\")\n",
    "    for model, metrics in evaluations.items():\n",
    "        print(f\"{model}:\")\n",
    "        print(f\"  MSE: {metrics['MSE']:.4f}\")\n",
    "        print(f\"  R²: {metrics['R²']:.4f}\")\n",
    "    \n",
    "    print(\"\\nSelected features:\")\n",
    "    print(selected_features)\n",
    "    \n",
    "    print(f\"\\nMax salary cap: ${max_salary_cap:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/predictor.py\n",
    "\n",
    "import joblib\n",
    "from data_loader import get_project_root\n",
    "from data_preprocessor import feature_engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "\n",
    "def load_model_and_scaler(model_name, inflated=False):\n",
    "    root_dir = get_project_root()\n",
    "    suffix = '_inflated' if inflated else ''\n",
    "    \n",
    "    # Convert model name to a consistent format\n",
    "    model_name = model_name.replace(' ', '_')\n",
    "    \n",
    "    if 'Best' in model_name:\n",
    "        model_file_name = f'{model_name}_salary_prediction_model{suffix}.joblib'\n",
    "    else:\n",
    "        model_file_name = f'{model_name}_salary_prediction_model{suffix}.joblib'\n",
    "    \n",
    "    model_path = os.path.join(root_dir, 'data', 'models', model_file_name)\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        # Try alternative naming conventions\n",
    "        alternative_names = [\n",
    "            f'{model_name.lower()}_salary_prediction_model{suffix}.joblib',\n",
    "            f'{model_name.upper()}_salary_prediction_model{suffix}.joblib',\n",
    "            f'{model_name.capitalize()}_salary_prediction_model{suffix}.joblib'\n",
    "        ]\n",
    "        \n",
    "        for alt_name in alternative_names:\n",
    "            alt_path = os.path.join(root_dir, 'data', 'models', alt_name)\n",
    "            if os.path.exists(alt_path):\n",
    "                model_path = alt_path\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"The model file for '{model_name}' does not exist. Tried the following paths:\\n\"\n",
    "                                    f\"- {model_path}\\n\" + \"\\n\".join(f\"- {os.path.join(root_dir, 'data', 'models', name)}\" for name in alternative_names))\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(os.path.join(root_dir, 'data', 'models', f'scaler{suffix}.joblib'))\n",
    "    selected_features = joblib.load(os.path.join(root_dir, 'data', 'models', f'selected_features{suffix}.joblib'))\n",
    "    return model, scaler, selected_features\n",
    "\n",
    "def make_predictions(df, model, scaler, selected_features, season, use_inflated_data, max_salary_cap):\n",
    "    df = df[df['Season'] == season].copy()\n",
    "    df = feature_engineering(df, use_inflated_data)\n",
    "    df['Age'] += 1\n",
    "    df['Season'] += 1\n",
    "    \n",
    "    if not all(feature in df.columns for feature in selected_features):\n",
    "        missing_features = [f for f in selected_features if f not in df.columns]\n",
    "        raise ValueError(f\"Missing features in dataframe: {missing_features}\")\n",
    "    \n",
    "    X = df[selected_features]\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X_scaled = scaler.transform(X_imputed)\n",
    "    \n",
    "    df.loc[:, 'Predicted_Salary_Pct'] = model.predict(X_scaled)\n",
    "    \n",
    "    salary_cap_column = 'Salary_Cap_Inflated' if use_inflated_data else 'Salary Cap'\n",
    "    \n",
    "    if salary_cap_column not in df.columns:\n",
    "        raise ValueError(f\"Salary cap column '{salary_cap_column}' not found in dataframe\")\n",
    "    \n",
    "    df.loc[:, 'Predicted_Salary'] = df['Predicted_Salary_Pct'] * df[salary_cap_column]\n",
    "    df.loc[:, 'Salary_Change'] = df['Predicted_Salary'] - df['Salary']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from data_loader import load_data\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model, scaler, selected_features = load_model_and_scaler('Random_Forest', inflated=False)\n",
    "    \n",
    "    print(\"\\nLoading data...\")\n",
    "    df = load_data(inflated=False)\n",
    "    \n",
    "    print(\"\\nMaking predictions...\")\n",
    "    season = df['Season'].max()\n",
    "    predictions = make_predictions(df, model, scaler, selected_features, season, use_inflated_data=False, max_salary_cap=df['Salary Cap'].max())\n",
    "    \n",
    "    print(\"\\nPredictions shape:\", predictions.shape)\n",
    "    print(\"\\nFirst few rows of predictions:\")\n",
    "    print(predictions[['Player', 'Salary', 'Predicted_Salary', 'Salary_Change']].head())\n",
    "    \n",
    "    print(\"\\nNaN values in predictions:\")\n",
    "    print(predictions.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging Trade Analysis Functionality\n",
      "Debug: df_actual shape: (10660, 60)\n",
      "Debug: df_actual columns: Index(['Player', 'Salary', 'Season', 'Position', 'Age', 'Team', 'TeamID',\n",
      "       'Years of Service', 'GP', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA',\n",
      "       '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB',\n",
      "       'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'PER', 'TS%', '3PAr',\n",
      "       'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%',\n",
      "       'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP', 'Wins',\n",
      "       'Losses', 'Min Salary', 'Max Salary', 'Salary Cap', '2022 Dollars',\n",
      "       'Salary_Cap_Inflated'],\n",
      "      dtype='object')\n",
      "Debug: Unique teams in df_actual: ['GSW' 'BOS' 'LAC' 'CHI' 'NYK' 'IND' 'PHI' 'PHX' 'MIN' 'TOR' 'HOU' 'DEN'\n",
      " 'POR' 'UTA' 'ATL' 'BKN' 'DET' 'MEM' 'SAS' 'ORL' 'OKC' 'DAL' 'MIL' 'LAL'\n",
      " 'SAC' 'CHA' 'MIA' 'NOP' 'CLE' 'WAS' 'NOH' 'NJN' 'SEA' 'NOK' 'CHH' 'VAN']\n",
      "Debug: df_predictions shape: (2159, 7)\n",
      "Debug: df_predictions columns: Index(['Player', 'Predicted_Season', 'Age', 'Predicted_Salary_Pct',\n",
      "       'Predicted_Salary', 'Previous_Season_Salary', 'Salary_Change'],\n",
      "      dtype='object')\n",
      "Debug: df_merged shape after merge: (2159, 9)\n",
      "Debug: df_merged columns after merge: Index(['Player', 'Predicted_Season', 'Age', 'Predicted_Salary_Pct',\n",
      "       'Predicted_Salary', 'Previous_Season_Salary', 'Salary_Change', 'Team',\n",
      "       'Season'],\n",
      "      dtype='object')\n",
      "Debug: df_merged columns after renaming: Index(['Player', 'Original_Season', 'Age', 'Predicted_Salary_Pct',\n",
      "       'Predicted_Salary', 'Previous_Season_Salary', 'Salary_Change', 'Team',\n",
      "       'Season'],\n",
      "      dtype='object')\n",
      "Debug: Sample of merged data:\n",
      "          Player  Season Team  Original_Season\n",
      "0  Stephen Curry     NaN  NaN             2024\n",
      "1   Jrue Holiday     NaN  NaN             2024\n",
      "2   James Harden     NaN  NaN             2024\n",
      "3  DeMar DeRozan     NaN  NaN             2024\n",
      "4     Taj Gibson     NaN  NaN             2024\n",
      "Debug: Unique teams in df_merged: [nan]\n",
      "Debug: Number of unique teams in df_merged: 0\n",
      "\n",
      "Predictions DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2159 entries, 0 to 2158\n",
      "Data columns (total 9 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Player                  2159 non-null   object \n",
      " 1   Original_Season         2159 non-null   int64  \n",
      " 2   Age                     2159 non-null   int64  \n",
      " 3   Predicted_Salary_Pct    2159 non-null   float64\n",
      " 4   Predicted_Salary        2159 non-null   float64\n",
      " 5   Previous_Season_Salary  2159 non-null   float64\n",
      " 6   Salary_Change           2159 non-null   float64\n",
      " 7   Team                    0 non-null      object \n",
      " 8   Season                  0 non-null      float64\n",
      "dtypes: float64(5), int64(2), object(2)\n",
      "memory usage: 151.9+ KB\n",
      "None\n",
      "\n",
      "Unique Teams:\n",
      "[nan]\n",
      "\n",
      "WARNING: Less than 2 unique teams found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ../src/salary_predict/app.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from data_loader import load_predictions, get_project_root, load_data\n",
    "# from data_preprocessor import calculate_percentiles\n",
    "\n",
    "RELEVANT_STATS = ['PTS', 'TRB', 'AST', 'FG%', '3P%', 'FT%', 'PER', 'WS', 'VORP']\n",
    "\n",
    "def calculate_team_percentiles(team_players):\n",
    "    team_percentiles = {}\n",
    "    for stat in RELEVANT_STATS:\n",
    "        if stat in team_players.columns:\n",
    "            values = team_players[stat].values\n",
    "            team_percentiles[stat] = {\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'above_average': np.sum(values > np.mean(values)),\n",
    "                'total_players': len(values)\n",
    "            }\n",
    "    return team_percentiles\n",
    "\n",
    "def analyze_trade(players1, players2, predictions_df):\n",
    "    group1_data = predictions_df[predictions_df['Player'].isin(players1)]\n",
    "    group2_data = predictions_df[predictions_df['Player'].isin(players2)]\n",
    "    \n",
    "    group1_percentiles = calculate_team_percentiles(group1_data)\n",
    "    group2_percentiles = calculate_team_percentiles(group2_data)\n",
    "    \n",
    "    return {\n",
    "        'group1': {\n",
    "            'players': group1_data,\n",
    "            'percentiles': group1_percentiles,\n",
    "            'salary_before': group1_data['Previous_Season_Salary'].sum(),\n",
    "            'salary_after': group1_data['Predicted_Salary'].sum(),\n",
    "        },\n",
    "        'group2': {\n",
    "            'players': group2_data,\n",
    "            'percentiles': group2_percentiles,\n",
    "            'salary_before': group2_data['Previous_Season_Salary'].sum(),\n",
    "            'salary_after': group2_data['Predicted_Salary'].sum(),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def plot_trade_impact(trade_analysis, team1, team2):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = range(len(RELEVANT_STATS))\n",
    "    width = 0.35\n",
    "    \n",
    "    group1_stats = [trade_analysis['group1']['percentiles'].get(stat, {}).get('mean', 0) for stat in RELEVANT_STATS]\n",
    "    group2_stats = [trade_analysis['group2']['percentiles'].get(stat, {}).get('mean', 0) for stat in RELEVANT_STATS]\n",
    "    \n",
    "    ax.bar([i - width/2 for i in x], group1_stats, width, label=team1)\n",
    "    ax.bar([i + width/2 for i in x], group2_stats, width, label=team2)\n",
    "    \n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Trade Impact on Team Stats')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(RELEVANT_STATS, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def simulate_trade(team_players, new_player_stats):\n",
    "    # Remove a player (e.g., the lowest-ranked player) to make room for the new player\n",
    "    team_players = team_players.sort_values('PTS', ascending=True).iloc[1:]\n",
    "    \n",
    "    # Add the new player to the team\n",
    "    new_team = pd.concat([team_players, new_player_stats], ignore_index=True)\n",
    "    \n",
    "    return new_team\n",
    "\n",
    "def compare_percentiles(current_percentiles, simulated_percentiles, champ_percentiles):\n",
    "    comparison = {}\n",
    "    for stat in RELEVANT_STATS:\n",
    "        comparison[stat] = {\n",
    "            'Current': current_percentiles[stat]['mean'],\n",
    "            'With New Player': simulated_percentiles[stat]['mean'],\n",
    "            'Champ Average': champ_percentiles[stat]['mean'],\n",
    "            'Current Diff': current_percentiles[stat]['mean'] - champ_percentiles[stat]['mean'],\n",
    "            'Simulated Diff': simulated_percentiles[stat]['mean'] - champ_percentiles[stat]['mean']\n",
    "        }\n",
    "    return comparison\n",
    "\n",
    "def analyze_trade_impact(team_abbr, new_player_name, season, predictions_df, min_minutes_per_game=10, min_games=20):\n",
    "    # Get team players\n",
    "    team_players = predictions_df[predictions_df['Team'] == team_abbr]\n",
    "\n",
    "    # Get new player's stats\n",
    "    new_player_stats = predictions_df[predictions_df['Player'] == new_player_name]\n",
    "\n",
    "    if new_player_stats.empty:\n",
    "        print(f\"Could not find {new_player_name}'s stats.\")\n",
    "        return\n",
    "\n",
    "    # Calculate current team percentiles\n",
    "    current_team_percentiles = calculate_team_percentiles(team_players)\n",
    "\n",
    "    # Simulate trade\n",
    "    team_with_new_player = simulate_trade(team_players, new_player_stats)\n",
    "\n",
    "    # Calculate simulated team percentiles\n",
    "    simulated_team_percentiles = calculate_team_percentiles(team_with_new_player)\n",
    "\n",
    "    # Get championship team percentiles (you may need to adapt this part)\n",
    "    champ_percentiles = calculate_team_percentiles(predictions_df)\n",
    "\n",
    "    # Compare percentiles\n",
    "    comparison = compare_percentiles(current_team_percentiles, simulated_team_percentiles, champ_percentiles)\n",
    "\n",
    "    return comparison\n",
    "\n",
    "def debug_trade_analysis():\n",
    "    print(\"Debugging Trade Analysis Functionality\")\n",
    "    \n",
    "    # Load predictions data\n",
    "    use_inflated_data = False\n",
    "    predictions = load_predictions(use_inflated_data)\n",
    "    \n",
    "    print(\"\\nPredictions DataFrame Info:\")\n",
    "    print(predictions.info())\n",
    "    \n",
    "    print(\"\\nUnique Teams:\")\n",
    "    print(predictions['Team'].unique())\n",
    "    \n",
    "    if len(predictions['Team'].unique()) < 2:\n",
    "        print(\"\\nWARNING: Less than 2 unique teams found in the dataset.\")\n",
    "        return\n",
    "    \n",
    "    # Select two teams for testing\n",
    "    team1, team2 = predictions['Team'].unique()[:2]\n",
    "    \n",
    "    print(f\"\\nAnalyzing trade between {team1} and {team2}\")\n",
    "    \n",
    "    # Select players from each team\n",
    "    players1 = predictions[predictions['Team'] == team1]['Player'].head(2).tolist()\n",
    "    players2 = predictions[predictions['Team'] == team2]['Player'].head(2).tolist()\n",
    "    \n",
    "    print(f\"Players from {team1}: {players1}\")\n",
    "    print(f\"Players from {team2}: {players2}\")\n",
    "    \n",
    "    # Perform trade analysis\n",
    "    trade_analysis = analyze_trade(players1, players2, predictions)\n",
    "    \n",
    "    print(\"\\nTrade Analysis Results:\")\n",
    "    for group, data in trade_analysis.items():\n",
    "        print(f\"\\n{group.upper()}:\")\n",
    "        print(f\"Salary Before: ${data['salary_before']/1e6:.2f}M\")\n",
    "        print(f\"Salary After: ${data['salary_after']/1e6:.2f}M\")\n",
    "        print(f\"Salary Change: ${(data['salary_after'] - data['salary_before'])/1e6:.2f}M\")\n",
    "        \n",
    "        print(\"\\nPercentiles:\")\n",
    "        for stat, values in data['percentiles'].items():\n",
    "            print(f\"{stat}: Mean = {values['mean']:.2f}, Std = {values['std']:.2f}\")\n",
    "    \n",
    "    # Plot trade impact\n",
    "    fig = plot_trade_impact(trade_analysis, team1, team2)\n",
    "    plt.show()\n",
    "\n",
    "    # Test trade impact analysis\n",
    "    print(\"\\nTesting Trade Impact Analysis\")\n",
    "    new_player_name = predictions[predictions['Team'] != team1]['Player'].iloc[0]\n",
    "    season = predictions['Season'].max()\n",
    "    \n",
    "    impact_comparison = analyze_trade_impact(team1, new_player_name, season, predictions)\n",
    "    \n",
    "    print(f\"\\nTrade Impact Analysis for {team1} acquiring {new_player_name}:\")\n",
    "    print(\"{:<10} {:<15} {:<15} {:<20} {:<15} {:<15}\".format(\n",
    "        \"Stat\", \"Current\", f\"With {new_player_name}\", \"Champ Average\", \"Current Diff\", \"Simulated Diff\"))\n",
    "    print(\"-\" * 90)\n",
    "    for stat, values in impact_comparison.items():\n",
    "        print(\"{:<10} {:<15.2f} {:<15.2f} {:<20.2f} {:<15.2f} {:<15.2f}\".format(\n",
    "            stat, \n",
    "            values['Current'], \n",
    "            values['With New Player'], \n",
    "            values['Champ Average'],\n",
    "            values['Current Diff'],\n",
    "            values['Simulated Diff']\n",
    "        ))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    debug_trade_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from data_loader import load_data, load_predictions, get_project_root\n",
    "from data_preprocessor import handle_missing_values, feature_engineering, calculate_vorp_salary_ratio, cluster_career_trajectories\n",
    "from predictor import load_model_and_scaler, make_predictions\n",
    "from model_trainer import retrain_and_save_models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "import joblib\n",
    "from champ_percentile_ranks import calculate_percentiles, analyze_team_percentiles, get_champions\n",
    "from data_loader import load_predictions, get_project_root\n",
    "from trade_utils import analyze_trade, plot_trade_impact, analyze_trade_impact\n",
    "\n",
    "def filter_by_position(df, selected_positions):\n",
    "    if not selected_positions:\n",
    "        return df\n",
    "    return df[df['Position'].apply(lambda x: any(pos in x.split('-') for pos in selected_positions))]\n",
    "\n",
    "def format_salary_df(df):\n",
    "    formatted_df = df.copy()\n",
    "    salary_columns = ['Salary', 'Predicted_Salary', 'Salary_Change']\n",
    "    \n",
    "    for col in salary_columns:\n",
    "        if col in formatted_df.columns:\n",
    "            formatted_df[col] = formatted_df[col].apply(lambda x: f\"${x/1e6:.2f}M\")\n",
    "    \n",
    "    return formatted_df[['Player', 'Position', 'Age', 'Salary', 'Predicted_Salary', 'Salary_Change']]\n",
    "\n",
    "def load_selected_model(model_name, use_inflated_data):\n",
    "    try:\n",
    "        model, scaler, selected_features = load_model_and_scaler(model_name, use_inflated_data)\n",
    "        df = load_data(use_inflated_data)\n",
    "        df = feature_engineering(df, use_inflated_data)\n",
    "        df = handle_missing_values(df)\n",
    "        \n",
    "        X = df[selected_features]\n",
    "        y = df['SalaryPct']\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        y_pred = model.predict(X_scaled)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        \n",
    "        salary_cap_column = 'Salary_Cap_Inflated' if use_inflated_data else 'Salary Cap'\n",
    "        max_salary_cap = df[salary_cap_column].max()\n",
    "        \n",
    "        return model_name, model, mse, r2, selected_features, scaler, max_salary_cap\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error in load_selected_model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def find_best_model(use_inflated_data):\n",
    "    root_dir = get_project_root()\n",
    "    suffix = '_inflated' if use_inflated_data else ''\n",
    "    \n",
    "    with open(os.path.join(root_dir, 'data', 'models', f'best_model_name{suffix}.txt'), 'r') as f:\n",
    "        best_model_name = f.read().strip()\n",
    "    \n",
    "    return load_selected_model(best_model_name, use_inflated_data)\n",
    "\n",
    "\n",
    "\n",
    "def load_champions_data():\n",
    "    root_dir = get_project_root()\n",
    "    champions_file = os.path.join(root_dir, 'data', 'processed', 'nba_champions.csv')\n",
    "    return pd.read_csv(champions_file)\n",
    "\n",
    "RELEVANT_STATS = ['PTS', 'TRB', 'AST', 'FG%', '3P%', 'FT%', 'PER', 'WS', 'VORP']\n",
    "\n",
    "def calculate_team_percentiles(team_players):\n",
    "    team_percentiles = {}\n",
    "    for stat in RELEVANT_STATS:\n",
    "        if stat in team_players.columns:\n",
    "            values = team_players[stat].values\n",
    "            team_percentiles[stat] = {\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'above_average': np.sum(values > np.mean(values)),\n",
    "                'total_players': len(values)\n",
    "            }\n",
    "    return team_percentiles\n",
    "\n",
    "def analyze_trade(players1, players2, predictions_df):\n",
    "    group1_data = predictions_df[predictions_df['Player'].isin(players1)]\n",
    "    group2_data = predictions_df[predictions_df['Player'].isin(players2)]\n",
    "    \n",
    "    group1_percentiles = calculate_team_percentiles(group1_data)\n",
    "    group2_percentiles = calculate_team_percentiles(group2_data)\n",
    "    \n",
    "    return {\n",
    "        'group1': {\n",
    "            'players': group1_data,\n",
    "            'percentiles': group1_percentiles,\n",
    "            'salary_before': group1_data['Previous_Season_Salary'].sum(),\n",
    "            'salary_after': group1_data['Predicted_Salary'].sum(),\n",
    "        },\n",
    "        'group2': {\n",
    "            'players': group2_data,\n",
    "            'percentiles': group2_percentiles,\n",
    "            'salary_before': group2_data['Previous_Season_Salary'].sum(),\n",
    "            'salary_after': group2_data['Predicted_Salary'].sum(),\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_salary_distribution(df):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    sns.histplot(df['Salary_M'], bins=30, kde=True, ax=ax1)\n",
    "    ax1.set_title('Distribution of NBA Player Salaries (in Millions)')\n",
    "    ax1.set_xlabel('Salary (in Millions)')\n",
    "    sns.boxplot(y='Salary_M', x='Position', data=df, ax=ax2)\n",
    "    ax2.set_title('NBA Player Salaries by Position (in Millions)')\n",
    "    ax2.set_xlabel('Position')\n",
    "    ax2.set_ylabel('Salary (in Millions)')\n",
    "    plt.xticks(rotation=45)\n",
    "    return fig\n",
    "\n",
    "def plot_age_vs_salary(df):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.scatterplot(x='Age', y='Salary_M', hue='Position', data=df, ax=ax)\n",
    "    ax.set_title('Age vs Salary (in Millions)')\n",
    "    ax.set_xlabel('Age')\n",
    "    ax.set_ylabel('Salary (in Millions)')\n",
    "    return fig\n",
    "\n",
    "def plot_vorp_vs_salary(df):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.scatterplot(x='VORP', y='Salary_M', hue='Position', size='Age', data=df, ax=ax)\n",
    "    ax.set_title('VORP vs Salary')\n",
    "    ax.set_xlabel('VORP')\n",
    "    ax.set_ylabel('Salary (in Millions)')\n",
    "    return fig\n",
    "\n",
    "def plot_career_clusters(df):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.scatterplot(x='Age', y='Salary_M', hue='Cluster_Definition', style='Position', data=df, ax=ax)\n",
    "    ax.set_title('Career Clusters: Age vs Salary')\n",
    "    ax.set_xlabel('Age')\n",
    "    ax.set_ylabel('Salary (in Millions)')\n",
    "    return fig\n",
    "\n",
    "def plot_salary_change_distribution(filtered_df):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.histplot(filtered_df['Salary_Change'] / 1e6, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title('Distribution of Predicted Salary Changes')\n",
    "    ax.set_xlabel('Salary Change (in Millions)')\n",
    "    ax.set_ylabel('Count')\n",
    "    return fig\n",
    "\n",
    "def plot_player_comparison(comparison_df):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    comparison_df['Salary_M'] = comparison_df['Predicted_Salary'] / 1e6\n",
    "    sns.barplot(x='Player', y='Salary_M', data=comparison_df, ax=ax)\n",
    "    ax.set_title('Predicted Salaries for Selected Players')\n",
    "    ax.set_xlabel('Player')\n",
    "    ax.set_ylabel('Predicted Salary (in Millions)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    return fig\n",
    "\n",
    "def plot_performance_metrics_comparison(df, selected_players):\n",
    "    metrics = ['PTS', 'TRB', 'AST', 'PER', 'WS', 'VORP']\n",
    "    metrics_df = df[df['Player'].isin(selected_players)][['Player'] + metrics]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        sns.barplot(x='Player', y=metric, data=metrics_df, ax=axes[i//3, i%3])\n",
    "        axes[i//3, i%3].set_title(f'{metric} Comparison')\n",
    "        axes[i//3, i%3].set_xticklabels(axes[i//3, i%3].get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_salary_difference_distribution(filtered_df):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.histplot(filtered_df['Salary_Difference'] / 1e6, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title('Distribution of Salary Differences')\n",
    "    ax.set_xlabel('Salary Difference (in Millions)')\n",
    "    ax.set_ylabel('Count')\n",
    "    return fig\n",
    "\n",
    "def plot_category_analysis(avg_predictions, category):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    avg_predictions[['Salary', 'Predicted_Salary']].plot(kind='bar', ax=ax)\n",
    "    ax.set_title(f'Average Actual vs Predicted Salary by {category}')\n",
    "    ax.set_ylabel('Salary')\n",
    "    plt.xticks(rotation=45)\n",
    "    return fig\n",
    "\n",
    "def plot_model_evaluation(df, y_pred, model_choice):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.scatter(df['SalaryPct'], y_pred, alpha=0.5)\n",
    "    ax.plot([df['SalaryPct'].min(), df['SalaryPct'].max()], [df['SalaryPct'].min(), df['SalaryPct'].max()], 'r--', lw=2)\n",
    "    ax.set_xlabel(\"Actual Salary Percentage\")\n",
    "    ax.set_ylabel(\"Predicted Salary Percentage\")\n",
    "    ax.set_title(f\"Actual vs Predicted Salary Percentage - {model_choice}\")\n",
    "    return fig\n",
    "\n",
    "def plot_feature_importance(feature_importance, model_choice):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    feature_importance.plot(x='feature', y='importance', kind='bar', ax=ax)\n",
    "    ax.set_title(f\"Feature Importances - {model_choice}\")\n",
    "    ax.set_xlabel(\"Features\")\n",
    "    ax.set_ylabel(\"Importance\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.sidebar.title(\"Navigation\")\n",
    "    sections = [\"Introduction\", \"Data Overview\", \"Exploratory Data Analysis\", \n",
    "                \"Advanced Analytics\", \"Salary Predictions\", \"Player Comparisons\", \n",
    "                \"Salary Comparison\", \"Analysis by Categories\", \"Model Selection and Evaluation\",\n",
    "                \"Model Retraining\", \"Trade Analysis\"]\n",
    "    choice = st.sidebar.radio(\"Go to\", sections)\n",
    "    \n",
    "    # Update model selection dropdown\n",
    "    model_options = ['Random_Forest', 'Gradient_Boosting', 'Ridge_Regression', 'ElasticNet', 'SVR', 'Decision_Tree']\n",
    "    selected_model = st.sidebar.selectbox(\"Select Model\", model_options)\n",
    "\n",
    "    use_inflated_data = st.sidebar.checkbox(\"Use Inflation Adjusted Salary Cap Data\")\n",
    "    st.sidebar.markdown(\"### All Salaries in Millions\")\n",
    "\n",
    "    # Load the selected model\n",
    "    model_name, model, mse, r2, selected_features, scaler, max_salary_cap = load_selected_model(selected_model, use_inflated_data)\n",
    "\n",
    "    # Display model info in sidebar\n",
    "    st.sidebar.markdown(f\"### Selected Model: {model_name}\")\n",
    "    st.sidebar.write(f\"MSE: {mse:.4f}\")\n",
    "    st.sidebar.write(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    df = load_data(use_inflated_data)\n",
    "    df = feature_engineering(df)\n",
    "    df = handle_missing_values(df)\n",
    "\n",
    "    seasons = df['Season'].unique()\n",
    "    selected_season = st.sidebar.selectbox(\"Select Season\", seasons)\n",
    "    \n",
    "    df = calculate_vorp_salary_ratio(df)\n",
    "    df = cluster_career_trajectories(df)\n",
    "\n",
    "    if model and selected_features and scaler:\n",
    "        predictions = make_predictions(df, model, scaler, selected_features, selected_season, use_inflated_data, max_salary_cap)\n",
    "    else:\n",
    "        predictions = None\n",
    "    \n",
    "    \n",
    "    if choice == \"Introduction\":\n",
    "        st.title(\"Enhanced NBA Player Salary Analysis\")\n",
    "        st.write(\"Welcome to the NBA Salary Analysis and Prediction App! This project aims to provide comprehensive insights into NBA player salaries, advanced metrics, and future salary predictions based on historical data. Here's a detailed breakdown of the steps involved in creating this app:\")\n",
    "\n",
    "        st.subheader(\"Data Collection\")\n",
    "        \n",
    "        st.write(\"### Salary Data\")\n",
    "        st.write(\"- **Sources**:\")\n",
    "        st.write(\"  - [Basketball Reference Salary Cap History](https://www.basketball-reference.com/contracts/salary-cap-history.html)\")\n",
    "        st.write(\"- **Description**: Data on the NBA salary cap from various seasons, along with maximum salary details for players based on years of service.\")\n",
    "\n",
    "        st.write(\"### Advanced Metrics\")\n",
    "        st.write(\"- **Source**: [Basketball Reference](https://www.basketball-reference.com)\")\n",
    "        st.write(\"- **Description**: Advanced player metrics such as Player Efficiency Rating (PER), True Shooting Percentage (TS%), and Value Over Replacement Player (VORP) were scraped using BeautifulSoup.\")\n",
    "\n",
    "        st.write(\"### Player Salaries and Team Data\")\n",
    "        st.write(\"- **Source**: [Hoopshype](https://hoopshype.com)\")\n",
    "        st.write(\"- **Description**: Player salary data was scraped for multiple seasons, with detailed information on individual player earnings and team salaries.\")\n",
    "\n",
    "        st.subheader(\"Data Processing\")\n",
    "\n",
    "        st.write(\"### Inflation Adjustment\")\n",
    "        st.write(\"- **Source**: [Adjusting for Inflation in Python](https://medium.com/analytics-vidhya/adjusting-for-inflation-when-analysing-historical-data-with-python-9d69a8dcbc27)\")\n",
    "        st.write(\"- **Description**: Adjusted historical salary data for inflation to provide a consistent basis for comparison.\")\n",
    "\n",
    "        st.write(\"### Data Aggregation\")\n",
    "        st.write(\"- Steps:\")\n",
    "        st.write(\"  1. Loaded salary data and combined it with team standings and advanced metrics.\")\n",
    "        st.write(\"  2. Merged multiple data sources to create a comprehensive dataset containing player performance, salaries, and advanced metrics.\")\n",
    "\n",
    "        st.subheader(\"Model Training and Prediction\")\n",
    "\n",
    "        st.write(\"### Data Preprocessing\")\n",
    "        st.write(\"- Implemented functions to handle missing values, perform feature engineering, and calculate key metrics such as points per game (PPG), assists per game (APG), and salary growth.\")\n",
    "\n",
    "        st.write(\"### Model Selection\")\n",
    "        st.write(\"- Utilized various machine learning models including Random Forest, Gradient Boosting, Ridge Regression, and others to predict future player salaries.\")\n",
    "        st.write(\"- Employed grid search for hyperparameter tuning and selected the best-performing models based on evaluation metrics like Mean Squared Error (MSE) and R² score.\")\n",
    "\n",
    "        st.write(\"### Feature Importance and Clustering\")\n",
    "        st.write(\"- Analyzed feature importance to understand the key factors influencing player salaries.\")\n",
    "        st.write(\"- Clustered players into categories based on career trajectories, providing insights into player development and value.\")\n",
    "\n",
    "        st.subheader(\"App Development\")\n",
    "\n",
    "        st.write(\"### Streamlit App\")\n",
    "        st.write(\"- Built an interactive app using Streamlit to visualize data, perform exploratory data analysis, and make salary predictions.\")\n",
    "        st.write(\"- **Features**:\")\n",
    "        st.write(\"  - **Data Overview**: Display raw and processed data.\")\n",
    "        st.write(\"  - **Exploratory Data Analysis**: Visualize salary distributions, age vs. salary, and other key metrics.\")\n",
    "        st.write(\"  - **Advanced Analytics**: Analyze VORP to salary ratio, career trajectory clusters, and other advanced metrics.\")\n",
    "        st.write(\"  - **Salary Predictions**: Predict future salaries and compare actual vs. predicted values.\")\n",
    "        st.write(\"  - **Player Comparisons**: Compare selected players based on predicted salaries and performance metrics.\")\n",
    "        st.write(\"  - **Model Evaluation**: Evaluate different models and display their performance metrics and feature importance.\")\n",
    "\n",
    "        st.write(\"### Data Files\")\n",
    "        st.write(\"- Stored processed data and model files in a structured format to facilitate easy loading and analysis within the app.\")\n",
    "\n",
    "        st.subheader(\"Improvements:\")\n",
    "        \n",
    "        st.write(\"### Add Injury Data:\")\n",
    "        st.write(\"- **Source**: [Kaggle NBA Injury Stats 1951-2023](https://www.kaggle.com/datasets/loganlauton/nba-injury-stats-1951-2023/data)\")\n",
    "        st.write(\"- **Description**: This dataset provides detailed statistics on NBA injuries from 1951 to 2023, allowing for analysis of player availability and its impact on performance and salaries.\")\n",
    "\n",
    "        st.subheader(\"Conclusion\")\n",
    "\n",
    "        st.write(\"This app provides a robust platform for analyzing NBA player salaries, understanding the factors influencing earnings, and predicting future salaries based on historical data and advanced metrics. Explore the app to gain insights into player performance, salary trends, and much more.\")\n",
    "\n",
    "\n",
    "    elif choice == \"Data Overview\":\n",
    "        st.header(\"Data Overview\")\n",
    "        st.write(\"First few rows of the current season's dataset:\")\n",
    "        st.write(df[['Player', 'Season', 'Salary', 'GP', 'PTS', 'TRB', 'AST', 'Injured', 'Injury_Periods', 'Position', 'Age', 'Team', 'Years of Service', 'PER', 'WS', 'VORP', 'Salary Cap', 'Salary_Cap_Inflated']].head())\n",
    "        st.write(\"\\nFirst few rows of the predictions dataset:\")\n",
    "        st.write(predictions.head())\n",
    "        \n",
    "        if use_inflated_data:\n",
    "            st.write(\"\\nNote: This data uses inflated salary cap projections.\")\n",
    "        else:\n",
    "            st.write(\"\\nNote: This data uses the standard salary cap.\")\n",
    "\n",
    "    elif choice == \"Exploratory Data Analysis\":\n",
    "        st.header(\"Exploratory Data Analysis\")\n",
    "        \n",
    "        st.subheader(\"Salary Distribution\")\n",
    "        df['Salary_M'] = df['Salary'] / 1e6\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        sns.histplot(df['Salary_M'], bins=30, kde=True, ax=ax1)\n",
    "        ax1.set_title('Distribution of NBA Player Salaries (in Millions)')\n",
    "        ax1.set_xlabel('Salary (in Millions)')\n",
    "        sns.boxplot(y='Salary_M', x='Position', data=df, ax=ax2)\n",
    "        ax2.set_title('NBA Player Salaries by Position (in Millions)')\n",
    "        ax2.set_xlabel('Position')\n",
    "        ax2.set_ylabel('Salary (in Millions)')\n",
    "        plt.xticks(rotation=45)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        st.subheader(\"Age vs Salary\")\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.scatterplot(x='Age', y='Salary_M', hue='Position', data=df, ax=ax)\n",
    "        ax.set_title('Age vs Salary (in Millions)')\n",
    "        ax.set_xlabel('Age')\n",
    "        ax.set_ylabel('Salary (in Millions)')\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    elif choice == \"Advanced Analytics\":\n",
    "        st.header(\"Advanced Analytics\")\n",
    "\n",
    "        st.subheader(\"VORP to Salary Ratio\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.scatterplot(x='VORP', y='Salary_M', hue='Position', size='Age', data=df, ax=ax)\n",
    "        ax.set_title('VORP vs Salary')\n",
    "        ax.set_xlabel('VORP')\n",
    "        ax.set_ylabel('Salary (in Millions)')\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        top_value_players = df.nlargest(10, 'VORP_Salary_Ratio')\n",
    "        st.write(\"Top 10 Value Players (Highest VORP to Salary Ratio):\")\n",
    "        st.write(top_value_players[['Player', 'Position', 'Age', 'Salary_M', 'VORP', 'VORP_Salary_Ratio']])\n",
    "\n",
    "        st.subheader(\"Career Trajectory Clusters\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.scatterplot(x='Age', y='Salary_M', hue='Cluster_Definition', style='Position', data=df, ax=ax)\n",
    "        ax.set_title('Career Clusters: Age vs Salary')\n",
    "        ax.set_xlabel('Age')\n",
    "        ax.set_ylabel('Salary (in Millions)')\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        st.write(\"Average Metrics by Cluster:\")\n",
    "        cluster_averages = df.groupby('Cluster_Definition')[['Age', 'Salary_M', 'PTS', 'TRB', 'AST', 'PER', 'WS', 'VORP']].mean()\n",
    "        st.write(cluster_averages)\n",
    "\n",
    "\n",
    "    elif choice == \"Salary Predictions\":\n",
    "        st.header(\"Salary Predictions\")\n",
    "        \n",
    "        if model:\n",
    "            predictions = make_predictions(df, model, scaler, selected_features, selected_season, use_inflated_data, max_salary_cap)\n",
    "            \n",
    "            st.sidebar.subheader(\"Filter by Position\")\n",
    "            unique_positions = sorted(set([pos for sublist in predictions['Position'].str.split('-') for pos in sublist]))\n",
    "            selected_positions = st.sidebar.multiselect(\"Select positions\", unique_positions, default=unique_positions)\n",
    "            filtered_df = filter_by_position(predictions, selected_positions)\n",
    "            \n",
    "            st.write(\"### Top 10 Highest Predicted Salaries\")\n",
    "            st.write(format_salary_df(filtered_df.nlargest(10, 'Predicted_Salary')))\n",
    "            \n",
    "            st.subheader(\"Salary Change Distribution\")\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            sns.histplot(filtered_df['Salary_Change'] / 1e6, bins=30, kde=True, ax=ax)\n",
    "            ax.set_title('Distribution of Predicted Salary Changes')\n",
    "            ax.set_xlabel('Salary Change (in Millions)')\n",
    "            ax.set_ylabel('Count')\n",
    "            st.pyplot(fig)\n",
    "\n",
    "            if use_inflated_data:\n",
    "                st.write(\"\\nNote: These predictions are based on inflated salary cap projections.\")\n",
    "            else:\n",
    "                st.write(\"\\nNote: These predictions are based on the standard salary cap.\")\n",
    "        else:\n",
    "            st.warning(\"No model found. Please select a valid model or retrain the models.\")\n",
    "\n",
    "\n",
    "            \n",
    "    elif choice == \"Player Comparisons\":\n",
    "        st.header(\"Player Comparisons\")\n",
    "        \n",
    "        players = sorted(predictions['Player'].unique())\n",
    "        selected_players = st.multiselect(\"Select players to compare\", players)\n",
    "        \n",
    "        if selected_players:\n",
    "            comparison_df = predictions[predictions['Player'].isin(selected_players)]\n",
    "            st.write(format_salary_df(comparison_df))\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            comparison_df['Salary_M'] = comparison_df['Predicted_Salary'] / 1e6\n",
    "            sns.barplot(x='Player', y='Salary_M', data=comparison_df, ax=ax)\n",
    "            ax.set_title('Predicted Salaries for Selected Players')\n",
    "            ax.set_xlabel('Player')\n",
    "            ax.set_ylabel('Predicted Salary (in Millions)')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            st.pyplot(fig)\n",
    "\n",
    "            st.subheader(\"Performance Metrics Comparison\")\n",
    "            metrics = ['PTS', 'TRB', 'AST', 'PER', 'WS', 'VORP']\n",
    "            metrics_df = df[df['Player'].isin(selected_players)][['Player'] + metrics]\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            for i, metric in enumerate(metrics):\n",
    "                sns.barplot(x='Player', y=metric, data=metrics_df, ax=axes[i//3, i%3])\n",
    "                axes[i//3, i%3].set_title(f'{metric} Comparison')\n",
    "                axes[i//3, i%3].set_xticklabels(axes[i//3, i%3].get_xticklabels(), rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            st.pyplot(fig)\n",
    "\n",
    "    elif choice == \"Salary Comparison\":\n",
    "        st.header(\"Salary Comparison\")\n",
    "\n",
    "        st.sidebar.subheader(\"Filter by Position\")\n",
    "        unique_positions = sorted(set([pos for sublist in predictions['Position'].str.split('-') for pos in sublist]))\n",
    "        selected_positions = st.sidebar.multiselect(\"Select positions\", unique_positions, default=unique_positions)\n",
    "        filtered_df = filter_by_position(predictions, selected_positions)\n",
    "\n",
    "        filtered_df['Salary_Difference'] = filtered_df['Salary'] - filtered_df['Predicted_Salary']\n",
    "        \n",
    "        top_overpaid_count = st.sidebar.slider(\"Number of Top Overpaid Players to Display\", min_value=1, max_value=50, value=10)\n",
    "        top_underpaid_count = st.sidebar.slider(\"Number of Top Underpaid Players to Display\", min_value=1, max_value=50, value=10)\n",
    "        \n",
    "        st.subheader(\"Overpaid vs Underpaid Players\")\n",
    "        st.write(\"### Top Overpaid Players\")\n",
    "        st.write(format_salary_df(filtered_df.nlargest(top_overpaid_count, 'Salary_Difference')))\n",
    "        \n",
    "        st.write(\"### Top Underpaid Players\")\n",
    "        st.write(format_salary_df(filtered_df.nsmallest(top_underpaid_count, 'Salary_Difference')))\n",
    "        \n",
    "        st.subheader(\"Salary Difference Distribution\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.histplot(filtered_df['Salary_Difference'] / 1e6, bins=30, kde=True, ax=ax)\n",
    "        ax.set_title('Distribution of Salary Differences')\n",
    "        ax.set_xlabel('Salary Difference (in Millions)')\n",
    "        ax.set_ylabel('Count')\n",
    "        st.pyplot(fig)\n",
    "        \n",
    "    elif choice == \"Analysis by Categories\":\n",
    "        st.header(\"Analysis by Categories\")\n",
    "        \n",
    "        category = st.selectbox(\"Select Category\", ['Position', 'Age', 'Team'])\n",
    "        \n",
    "        if category == 'Age':\n",
    "            predictions['Age_Group'] = pd.cut(predictions['Age'], bins=[0, 25, 30, 35, 100], labels=['Under 25', '25-30', '30-35', 'Over 35'])\n",
    "            category = 'Age_Group'\n",
    "        \n",
    "        avg_predictions = predictions.groupby(category)[['Salary', 'Predicted_Salary', 'Salary_Change']].mean()\n",
    "        \n",
    "        st.write(f\"Average Salaries by {category}\")\n",
    "        st.write(avg_predictions)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        avg_predictions[['Salary', 'Predicted_Salary']].plot(kind='bar', ax=ax)\n",
    "        ax.set_title(f'Average Actual vs Predicted Salary by {category}')\n",
    "        ax.set_ylabel('Salary')\n",
    "        plt.xticks(rotation=45)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    elif choice == \"Model Selection and Evaluation\":\n",
    "        st.header(\"Model Selection and Evaluation\")\n",
    "        \n",
    "        models = ['Random_Forest', 'Gradient_Boosting', 'Ridge_Regression', 'ElasticNet', 'SVR', 'Decision_Tree']\n",
    "        model_choice = st.selectbox(\"Select Model to Evaluate\", models)\n",
    "        \n",
    "        if model_choice:\n",
    "            try:\n",
    "                # Load the model, scaler, and selected features\n",
    "                model, scaler, selected_features = load_model_and_scaler(model_choice, use_inflated_data)\n",
    "                \n",
    "                # Ensure we have the correct features in our dataframe\n",
    "                df_features = df[selected_features]\n",
    "                \n",
    "                # Scale the features\n",
    "                X_scaled = scaler.transform(df_features)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_scaled)\n",
    "                \n",
    "                # Calculate evaluation metrics\n",
    "                mse = mean_squared_error(df['SalaryPct'], y_pred)\n",
    "                r2 = r2_score(df['SalaryPct'], y_pred)\n",
    "                \n",
    "                st.write(f\"### Evaluation Metrics for {model_choice}\")\n",
    "                st.write(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "                st.write(f\"R² Score: {r2:.4f}\")\n",
    "                \n",
    "                # Create a scatter plot of actual vs predicted values\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                ax.scatter(df['SalaryPct'], y_pred, alpha=0.5)\n",
    "                ax.plot([df['SalaryPct'].min(), df['SalaryPct'].max()], [df['SalaryPct'].min(), df['SalaryPct'].max()], 'r--', lw=2)\n",
    "                ax.set_xlabel(\"Actual Salary Percentage\")\n",
    "                ax.set_ylabel(\"Predicted Salary Percentage\")\n",
    "                ax.set_title(f\"Actual vs Predicted Salary Percentage - {model_choice}\")\n",
    "                st.pyplot(fig)\n",
    "                \n",
    "                # Display feature importances for tree-based models\n",
    "                if model_choice in ['Random_Forest', 'Gradient_Boosting', 'Decision_Tree']:\n",
    "                    feature_importance = pd.DataFrame({\n",
    "                        'feature': selected_features,\n",
    "                        'importance': model.feature_importances_\n",
    "                    }).sort_values('importance', ascending=False)\n",
    "                    \n",
    "                    st.write(\"### Feature Importances\")\n",
    "                    st.write(feature_importance)\n",
    "                    \n",
    "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                    feature_importance.plot(x='feature', y='importance', kind='bar', ax=ax)\n",
    "                    ax.set_title(f\"Feature Importances - {model_choice}\")\n",
    "                    ax.set_xlabel(\"Features\")\n",
    "                    ax.set_ylabel(\"Importance\")\n",
    "                    plt.xticks(rotation=45, ha='right')\n",
    "                    st.pyplot(fig)\n",
    "                \n",
    "            except FileNotFoundError as e:\n",
    "                st.error(\"Error: Model file not found\")\n",
    "                st.error(str(e))\n",
    "                st.error(\"Please make sure the model file exists and the name is correct.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"An unexpected error occurred: {str(e)}\")\n",
    "                st.error(\"Please check the logs for more details and ensure all required files are present.\")\n",
    "\n",
    "    # Update the Model Retraining section in your main() function\n",
    "    elif choice == \"Model Retraining\":\n",
    "        st.header(\"Model Retraining\")\n",
    "        \n",
    "        if st.button(\"Retrain Models\"):\n",
    "            try:\n",
    "                with st.spinner(\"Retraining models... This may take a while.\"):\n",
    "                    best_model_name, best_model, evaluations, selected_features, scaler, max_salary_cap = retrain_and_save_models(use_inflated_data)\n",
    "                \n",
    "                st.success(\"Retraining completed successfully!\")\n",
    "                st.write(f\"Best model: {best_model_name}\")\n",
    "                st.write(\"Model performance:\")\n",
    "                for model, metrics in evaluations.items():\n",
    "                    st.write(f\"{model}:\")\n",
    "                    st.write(f\"  MSE: {metrics['MSE']:.4f}\")\n",
    "                    st.write(f\"  R²: {metrics['R²']:.4f}\")\n",
    "                \n",
    "                st.write(\"All models have been retrained and saved. The best model will be used for future predictions.\")\n",
    "                \n",
    "                # Refresh the app to use the new models\n",
    "                st.rerun()\n",
    "            except Exception as e:\n",
    "                st.error(f\"An error occurred during model retraining: {str(e)}\")\n",
    "                st.error(\"Please check the logs for more details.\")\n",
    "\n",
    "    elif choice == \"Trade Analysis\":\n",
    "        st.header(\"Trade Analysis\")\n",
    "        \n",
    "        try:\n",
    "            # Load the necessary data\n",
    "            use_inflated_data_trade = st.checkbox(\"Use Inflation Adjusted Salary Cap Data\", key=\"trade_analysis_inflated_data\")\n",
    "            \n",
    "            predictions = load_predictions(use_inflated_data_trade)\n",
    "            \n",
    "            st.write(\"Debug: Predictions DataFrame shape:\", predictions.shape)\n",
    "            st.write(\"Debug: Predictions DataFrame columns:\", predictions.columns)\n",
    "            \n",
    "            if 'Team' not in predictions.columns:\n",
    "                st.error(\"The 'Team' column is missing from the predictions data. Please check your data loading process.\")\n",
    "                st.write(\"Debug: Available columns:\", predictions.columns)\n",
    "            else:\n",
    "                st.write(\"Debug: 'Team' column exists in predictions DataFrame\")\n",
    "                st.write(\"Debug: Unique teams:\", predictions['Team'].unique())\n",
    "                st.write(\"Debug: Number of unique teams:\", predictions['Team'].nunique())\n",
    "                \n",
    "                # Team filter\n",
    "                all_teams = sorted(predictions['Team'].unique())\n",
    "                st.write(\"Debug: all_teams list:\", all_teams)\n",
    "                \n",
    "                if len(all_teams) > 0:\n",
    "                    team1 = st.selectbox(\"Select Team 1\", all_teams, key=\"trade_analysis_team1\")\n",
    "                    team2 = st.selectbox(\"Select Team 2\", all_teams, index=1, key=\"trade_analysis_team2\")\n",
    "                    \n",
    "                    predictions1 = predictions[predictions['Team'] == team1]\n",
    "                    predictions2 = predictions[predictions['Team'] == team2]\n",
    "                    \n",
    "                    st.write(f\"Debug: Number of players in {team1}:\", len(predictions1))\n",
    "                    st.write(f\"Debug: Number of players in {team2}:\", len(predictions2))\n",
    "                    \n",
    "                    st.subheader(f\"Available Players for {team1}\")\n",
    "                    st.write(predictions1[['Player', 'Age', 'Previous_Season_Salary', 'Predicted_Salary', 'PTS', 'TRB', 'AST']])\n",
    "                    \n",
    "                    st.subheader(f\"Available Players for {team2}\")\n",
    "                    st.write(predictions2[['Player', 'Age', 'Previous_Season_Salary', 'Predicted_Salary', 'PTS', 'TRB', 'AST']])\n",
    "                    \n",
    "                    # Player selection\n",
    "                    players1 = st.multiselect(f\"Select players from {team1}\", predictions1['Player'].unique(), key=\"trade_analysis_players1\")\n",
    "                    players2 = st.multiselect(f\"Select players from {team2}\", predictions2['Player'].unique(), key=\"trade_analysis_players2\")\n",
    "                    \n",
    "                    if st.button(\"Analyze Trade\", key=\"trade_analysis_button\"):\n",
    "                        if not players1 or not players2:\n",
    "                            st.warning(\"Please select players from both teams.\")\n",
    "                        else:\n",
    "                            combined_predictions = pd.concat([predictions1, predictions2])\n",
    "                            trade_analysis = analyze_trade(players1, players2, combined_predictions)\n",
    "                            \n",
    "                        \n",
    "                        st.subheader(\"Trade Impact\")\n",
    "                        \n",
    "                        for group, data in trade_analysis.items():\n",
    "                            st.write(f\"\\n{group.upper()} Analysis:\")\n",
    "                            st.write(f\"Total Salary Before: ${data['salary_before']/1e6:.2f}M\")\n",
    "                            st.write(f\"Total Salary After: ${data['salary_after']/1e6:.2f}M\")\n",
    "                            st.write(f\"Salary Change: ${(data['salary_after'] - data['salary_before'])/1e6:.2f}M\")\n",
    "                            \n",
    "                            st.write(\"\\nPlayer Details:\")\n",
    "                            st.write(data['players'][['Player', 'Age', 'Previous_Season_Salary', 'Predicted_Salary', 'Salary_Change', 'PTS', 'TRB', 'AST', 'PER', 'WS', 'VORP']])\n",
    "                            \n",
    "                            st.write(\"\\nTeam Percentiles:\")\n",
    "                            for stat in RELEVANT_STATS:\n",
    "                                if stat in data['percentiles']:\n",
    "                                    st.write(f\"{stat}: {data['percentiles'][stat]['mean']:.2f}\")\n",
    "                        \n",
    "                        st.subheader(\"Salary Comparison\")\n",
    "                        group1_trade_salary = trade_analysis['group1']['salary_after']\n",
    "                        group2_trade_salary = trade_analysis['group2']['salary_after']\n",
    "                        salary_difference = abs(group1_trade_salary - group2_trade_salary)\n",
    "                        \n",
    "                        st.write(f\"{team1} is trading ${group1_trade_salary/1e6:.2f}M in salary\")\n",
    "                        st.write(f\"{team2} is trading ${group2_trade_salary/1e6:.2f}M in salary\")\n",
    "                        st.write(f\"Salary difference: ${salary_difference/1e6:.2f}M\")\n",
    "                        \n",
    "                        if salary_difference > 5e6:  # Assuming a 5 million threshold for salary matching\n",
    "                            st.warning(\"The salaries in this trade are not well-matched. This may not be a valid trade under NBA rules.\")\n",
    "                        else:\n",
    "                            st.success(\"The salaries in this trade are well-matched.\")\n",
    "                        \n",
    "                        # Visualize the trade impact\n",
    "                        fig = plot_trade_impact(trade_analysis, team1, team2)\n",
    "                        st.pyplot(fig)\n",
    "\n",
    "            else:\n",
    "                st.error(\"No teams found in the predictions data. Please check your data loading process.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        st.error(f\"Error: {str(e)}\")\n",
    "        st.error(\"Please make sure the predictions file exists in the correct location.\")\n",
    "    except KeyError as e:\n",
    "        st.error(f\"Error: {str(e)}\")\n",
    "        st.error(\"Please check your data files and ensure they contain all required columns.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"An unexpected error occurred: {str(e)}\")\n",
    "        st.error(\"Please check the data and try again.\")\n",
    "        st.write(\"Debug: Exception details:\", str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
