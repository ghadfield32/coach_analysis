{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data using regular salary cap...\n",
      "Preparing data for training...\n",
      "Training models with target variable: SalaryPct\n",
      "Selected features by RFE: ['Years of Service', 'PPG', 'APG', 'SPG', 'TOPG', 'PER', 'VORP', 'Availability', 'Player', 'Team']\n",
      "Training Random Forest...\n",
      "Random Forest - Best params: {'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Random Forest - Cross-validation MSE: 0.0014 (+/- 0.0005)\n",
      "Random Forest - Test MSE: 0.0017, R²: 0.7570\n",
      "\n",
      "Random Forest - Top 5 important features:\n",
      "            feature  importance\n",
      "1               PPG    0.228450\n",
      "0  Years of Service    0.201281\n",
      "6              VORP    0.155494\n",
      "4              TOPG    0.153137\n",
      "2               APG    0.103165\n",
      "Random Forest model saved to '../data/models/Random Forest_salary_prediction_model_inflated.joblib'\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting - Best params: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Gradient Boosting - Cross-validation MSE: 0.0012 (+/- 0.0004)\n",
      "Gradient Boosting - Test MSE: 0.0019, R²: 0.7318\n",
      "\n",
      "Gradient Boosting - Top 5 important features:\n",
      "            feature  importance\n",
      "1               PPG    0.519038\n",
      "0  Years of Service    0.284089\n",
      "2               APG    0.042336\n",
      "5               PER    0.034514\n",
      "4              TOPG    0.028423\n",
      "Gradient Boosting model saved to '../data/models/Gradient Boosting_salary_prediction_model_inflated.joblib'\n",
      "Training Ridge Regression...\n",
      "Ridge Regression - Best params: {'alpha': 10.0}\n",
      "Ridge Regression - Cross-validation MSE: 0.0023 (+/- 0.0009)\n",
      "Ridge Regression - Test MSE: 0.0023, R²: 0.6748\n",
      "\n",
      "Ridge Regression - Top 5 important features (Permutation Importance):\n",
      "            feature  importance\n",
      "1               PPG    0.482681\n",
      "0  Years of Service    0.252445\n",
      "6              VORP    0.127208\n",
      "4              TOPG    0.035825\n",
      "7      Availability    0.008536\n",
      "Ridge Regression model saved to '../data/models/Ridge Regression_salary_prediction_model_inflated.joblib'\n",
      "Training ElasticNet...\n",
      "ElasticNet - Best params: {'alpha': 0.1, 'l1_ratio': 0.1}\n",
      "ElasticNet - Cross-validation MSE: 0.0026 (+/- 0.0010)\n",
      "ElasticNet - Test MSE: 0.0023, R²: 0.6628\n",
      "\n",
      "ElasticNet - Top 5 important features (Permutation Importance):\n",
      "            feature  importance\n",
      "1               PPG    0.310391\n",
      "0  Years of Service    0.201309\n",
      "6              VORP    0.110915\n",
      "4              TOPG    0.060379\n",
      "2               APG    0.000028\n",
      "ElasticNet model saved to '../data/models/ElasticNet_salary_prediction_model_inflated.joblib'\n",
      "Training SVR...\n",
      "SVR - Best params: {'C': 1, 'epsilon': 0.1}\n",
      "SVR - Cross-validation MSE: 0.0050 (+/- 0.0007)\n",
      "SVR - Test MSE: 0.0045, R²: 0.3543\n",
      "\n",
      "SVR - Top 5 important features (Permutation Importance):\n",
      "            feature  importance\n",
      "0  Years of Service    0.233636\n",
      "1               PPG    0.219239\n",
      "4              TOPG    0.157458\n",
      "6              VORP    0.087088\n",
      "2               APG    0.078044\n",
      "SVR model saved to '../data/models/SVR_salary_prediction_model_inflated.joblib'\n",
      "Training Decision Tree...\n",
      "Decision Tree - Best params: {'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Decision Tree - Cross-validation MSE: 0.0016 (+/- 0.0005)\n",
      "Decision Tree - Test MSE: 0.0026, R²: 0.6270\n",
      "\n",
      "Decision Tree - Top 5 important features:\n",
      "            feature  importance\n",
      "1               PPG    0.545914\n",
      "0  Years of Service    0.296893\n",
      "5               PER    0.060854\n",
      "2               APG    0.047639\n",
      "7      Availability    0.020047\n",
      "Decision Tree model saved to '../data/models/Decision Tree_salary_prediction_model_inflated.joblib'\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def load_and_preprocess_data(file_path, use_inflated_cap=True):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    if use_inflated_cap:\n",
    "        data.drop(columns=['Salary Cap'], inplace=True, errors='ignore')\n",
    "        salary_cap_column = 'Salary_Cap_Inflated'\n",
    "    else:\n",
    "        data.drop(columns=['Salary_Cap_Inflated'], inplace=True, errors='ignore')\n",
    "        salary_cap_column = 'Salary Cap'\n",
    "\n",
    "    data['Season'] = data['Season'].str[:4].astype(int)\n",
    "\n",
    "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data[numerical_cols] = imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Feature engineering\n",
    "    data['PPG'] = data['PTS'] / data['GP']\n",
    "    data['APG'] = data['AST'] / data['GP']\n",
    "    data['RPG'] = data['TRB'] / data['GP']\n",
    "    data['SPG'] = data['STL'] / data['GP']\n",
    "    data['BPG'] = data['BLK'] / data['GP']\n",
    "    data['TOPG'] = data['TOV'] / data['GP']\n",
    "    data['WinPct'] = data['Wins'] / (data['Wins'] + data['Losses'])\n",
    "    data['SalaryGrowth'] = data.groupby('Player')['Salary'].pct_change().fillna(0)\n",
    "    data['Availability'] = data['GP'] / 82\n",
    "    data['SalaryPct'] = data['Salary'] / data[salary_cap_column]\n",
    "\n",
    "    return data, salary_cap_column\n",
    "\n",
    "def prepare_data_for_training(data, salary_cap_column):\n",
    "    # Using Label Encoding for categorical columns\n",
    "    label_encoders = {}\n",
    "    for column in ['Player', 'Season', 'Position', 'Team']:\n",
    "        le = LabelEncoder()\n",
    "        data[column] = le.fit_transform(data[column])\n",
    "        label_encoders[column] = le\n",
    "    \n",
    "    numerical_cols = data.columns.difference(['Salary', 'SalaryPct'])\n",
    "\n",
    "    initial_features = ['Age', 'Years of Service', 'GP', 'PPG', 'APG', 'RPG', 'SPG', 'BPG', 'TOPG', 'FG%', '3P%', 'FT%', 'PER', 'WS', 'VORP', 'Availability'] + ['Player', 'Season', 'Position', 'Team']\n",
    "\n",
    "    data_subset = data[initial_features + ['SalaryPct', 'Salary']].copy()\n",
    "    data_cleaned = data_subset.dropna()\n",
    "\n",
    "    return data_cleaned, initial_features, label_encoders\n",
    "\n",
    "def plot_correlation_matrix(data):\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    # Select only the numerical columns\n",
    "    numerical_data = data.select_dtypes(include=[np.number])\n",
    "    correlation_matrix = numerical_data.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .8})\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def train_models(data_cleaned, initial_features, target_variable='SalaryPct', n_features_to_select=10):\n",
    "    X = data_cleaned[initial_features]\n",
    "    y = data_cleaned[target_variable]\n",
    "\n",
    "    rfe = RFE(estimator=RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=n_features_to_select)\n",
    "    rfe = rfe.fit(X, y)\n",
    "    selected_features = [feature for feature, selected in zip(initial_features, rfe.support_) if selected]\n",
    "\n",
    "    print(\"Selected features by RFE:\", selected_features)\n",
    "\n",
    "    X = data_cleaned[selected_features]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    models = {\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'Ridge Regression': Ridge(),\n",
    "        'ElasticNet': ElasticNet(max_iter=10000),\n",
    "        'SVR': SVR(),\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    param_grids = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'max_depth': [8, 10, 12],\n",
    "            'min_samples_split': [5, 10, 15],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'Ridge Regression': {'alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "        'ElasticNet': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]},\n",
    "        'SVR': {'C': [0.1, 1, 10], 'epsilon': [0.1, 0.2, 0.5]},\n",
    "        'Decision Tree': {'max_depth': [6, 8, 10], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        \n",
    "        cv_scores = cross_val_score(best_models[name], X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "        print(f\"{name} - Best params: {grid_search.best_params_}\")\n",
    "        print(f\"{name} - Cross-validation MSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        y_pred = best_models[name].predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"{name} - Test MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "        \n",
    "        if name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:\n",
    "            importances = best_models[name].feature_importances_\n",
    "            feature_importance = pd.DataFrame({'feature': selected_features, 'importance': importances})\n",
    "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "            print(f\"\\n{name} - Top 5 important features:\")\n",
    "            print(feature_importance.head())\n",
    "        else:\n",
    "            perm_importance = permutation_importance(best_models[name], X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "            feature_importance = pd.DataFrame({'feature': selected_features, 'importance': perm_importance.importances_mean})\n",
    "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "            print(f\"\\n{name} - Top 5 important features (Permutation Importance):\")\n",
    "            print(feature_importance.head())\n",
    "        \n",
    "        model_filename = f\"../data/models/{name}_salary_prediction_model_{'inflated' if target_variable == 'SalaryPct' else 'regular'}.joblib\"\n",
    "        joblib.dump(best_models[name], model_filename)\n",
    "        print(f\"{name} model saved to '{model_filename}'\")\n",
    "\n",
    "    return best_models, scaler, selected_features\n",
    "\n",
    "def main():\n",
    "    processed_file_path = '../data/processed/nba_player_data_final_inflated.csv'\n",
    "\n",
    "    use_inflated_cap = input(\"Use inflated salary cap? (yes/no): \").strip().lower() == 'yes'\n",
    "    target_variable = input(\"Choose target variable (SalaryPct/Salary): \").strip()\n",
    "    \n",
    "    print(f\"Loading and preprocessing data using {'inflated' if use_inflated_cap else 'regular'} salary cap...\")\n",
    "    data, salary_cap_column = load_and_preprocess_data(processed_file_path, use_inflated_cap)\n",
    "    \n",
    "    # print(\"Plotting correlation matrix...\")\n",
    "    # plot_correlation_matrix(data)\n",
    "    \n",
    "    print(\"Preparing data for training...\")\n",
    "    data_cleaned, initial_features, label_encoders = prepare_data_for_training(data, salary_cap_column)\n",
    "    \n",
    "    print(f\"Training models with target variable: {target_variable}\")\n",
    "    best_models, scaler, selected_features = train_models(data_cleaned, initial_features, target_variable)\n",
    "    \n",
    "    print(\"Model training completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
