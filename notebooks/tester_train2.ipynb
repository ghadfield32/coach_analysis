{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Script for Not inflated data:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../data/processed/final_salary_data_with_yos_and_inflated_cap_2000_on.csv')\n",
    "\n",
    "# Initial data inspection\n",
    "# print(\"Initial Data Overview:\")\n",
    "# print(data.head())\n",
    "# print(\"Initial Data Describe:\")\n",
    "# print(data.describe())\n",
    "# print(\"\\nData Info:\")\n",
    "# print(data.info())\n",
    "# print(\"\\nMissing Values:\")\n",
    "# print(data.isnull().sum())\n",
    "\n",
    "# Drop the '2022 Dollars' column\n",
    "data.drop(columns=['2022 Dollars', 'Salary_Cap_Inflated'], inplace=True)\n",
    "\n",
    "# Convert 'Season' to an integer\n",
    "data['Season'] = data['Season'].str[:4].astype(int)\n",
    "\n",
    "# Handle missing values for numerical columns\n",
    "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data[numerical_cols] = imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Feature engineering\n",
    "data['PPG'] = data['PTS'] / data['GP']\n",
    "data['APG'] = data['AST'] / data['GP']\n",
    "data['RPG'] = data['TRB'] / data['GP']\n",
    "data['SPG'] = data['STL'] / data['GP']\n",
    "data['BPG'] = data['BLK'] / data['GP']\n",
    "data['TOPG'] = data['TOV'] / data['GP']\n",
    "data['WinPct'] = data['Wins'] / (data['Wins'] + data['Losses'])\n",
    "data['SalaryGrowth'] = data['Salary'].pct_change().fillna(0)\n",
    "data['Availability'] = data['GP'] / 82\n",
    "data['SalaryPct'] = data['Salary'] / data['Salary Cap']\n",
    "\n",
    "\n",
    "#for training the Salary_cap_inflated data \n",
    "# Drop the '2022 Dollars' column\n",
    "# data.drop(columns=['2022 Dollars', 'Salary Cap'], inplace=True)\n",
    "\n",
    "# # Convert 'Season' to an integer\n",
    "# data['Season'] = data['Season'].str[:4].astype(int)\n",
    "\n",
    "# # Handle missing values for numerical columns\n",
    "# numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# data[numerical_cols] = imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "# # Feature engineering\n",
    "# data['PPG'] = data['PTS'] / data['GP']\n",
    "# data['APG'] = data['AST'] / data['GP']\n",
    "# data['RPG'] = data['TRB'] / data['GP']\n",
    "# data['SPG'] = data['STL'] / data['GP']\n",
    "# data['BPG'] = data['BLK'] / data['GP']\n",
    "# data['TOPG'] = data['TOV'] / data['GP']\n",
    "# data['WinPct'] = data['Wins'] / (data['Wins'] + data['Losses'])\n",
    "# data['SalaryGrowth'] = data['Salary'].pct_change().fillna(0)\n",
    "# data['Availability'] = data['GP'] / 82\n",
    "# data['SalaryPct'] = data['Salary'] / data['Salary_Cap_Inflated']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['Player', 'Season', 'Position', 'Team']\n",
    "numerical_cols = data.columns.difference(categorical_cols + ['Salary', 'SalaryPct'])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_cats = pd.DataFrame(encoder.fit_transform(data[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Combine the numerical and encoded categorical data\n",
    "data = pd.concat([data[numerical_cols], encoded_cats, data[['Player', 'Season', 'Salary', 'SalaryPct']]], axis=1)\n",
    "\n",
    "# Select initial features\n",
    "initial_features = ['Age', 'Years of Service', 'GP', 'PPG', 'APG', 'RPG', 'SPG', 'BPG', 'TOPG', 'FG%', '3P%', 'FT%', 'PER', 'WS', 'VORP', 'Availability'] + list(encoded_cats.columns)\n",
    "\n",
    "# Create a new DataFrame with only the features we're interested in and the target variable\n",
    "data_subset = data[initial_features + ['SalaryPct']].copy()\n",
    "\n",
    "# Drop rows with any missing values\n",
    "data_cleaned = data_subset.dropna()\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_cleaned[initial_features]\n",
    "y = data_cleaned['SalaryPct']\n",
    "\n",
    "# Perform feature selection\n",
    "rfe = RFE(estimator=RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=10)\n",
    "rfe = rfe.fit(X, y)\n",
    "selected_features = [feature for feature, selected in zip(initial_features, rfe.support_) if selected]\n",
    "\n",
    "print(\"Selected features by RFE:\", selected_features)\n",
    "\n",
    "# Use only the selected features\n",
    "X = data_cleaned[selected_features]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_filename = \"../data/models/scaler_inflated.joblib\"\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"Scaler saved to '{scaler_filename}'\")\n",
    "\n",
    "# Define models with updated parameters\n",
    "models = {\n",
    "    'Random_Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient_Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Ridge_Regression': Ridge(),\n",
    "    'ElasticNet': ElasticNet(max_iter=10000),\n",
    "    'SVR': SVR(),\n",
    "    'Decision_Tree': DecisionTreeRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [8, 10, 12],\n",
    "        'min_samples_split': [5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'Ridge Regression': {'alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "    'ElasticNet': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]},\n",
    "    'SVR': {'C': [0.1, 1, 10], 'epsilon': [0.1, 0.2, 0.5]},\n",
    "    'Decision Tree': {'max_depth': [6, 8, 10], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "}\n",
    "\n",
    "# Save the selected features\n",
    "selected_features_filename = \"../data/models/selected_features.joblib\"\n",
    "joblib.dump(selected_features, selected_features_filename)\n",
    "print(f\"Selected features saved to '{selected_features_filename}'\")\n",
    "\n",
    "# Train and evaluate models\n",
    "best_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(best_models[name], X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    print(f\"{name} - Best params: {grid_search.best_params_}\")\n",
    "    print(f\"{name} - Cross-validation MSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Test set performance\n",
    "    y_pred = best_models[name].predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{name} - Test MSE: {mse:.4f}, RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    if name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:\n",
    "        importances = best_models[name].feature_importances_\n",
    "        feature_importance = pd.DataFrame({'feature': selected_features, 'importance': importances})\n",
    "        feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "        print(f\"\\n{name} - Top 5 important features:\")\n",
    "        print(feature_importance.head())\n",
    "    else:\n",
    "        perm_importance = permutation_importance(best_models[name], X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "        feature_importance = pd.DataFrame({'feature': selected_features, 'importance': perm_importance.importances_mean})\n",
    "        feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "        print(f\"\\n{name} - Top 5 important features (Permutation Importance):\")\n",
    "        print(feature_importance.head())\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f\"../data/models/{name}_salary_prediction_model.joblib\"\n",
    "    joblib.dump(best_models[name], model_filename)\n",
    "    print(f\"{name} model saved to '{model_filename}'\")\n",
    "\n",
    "\n",
    "# Identify the best overall model\n",
    "best_model_name = min(best_models, key=lambda x: -cross_val_score(best_models[x], X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error').mean())\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"Best overall model: {best_model_name}\")\n",
    "\n",
    "# Create predictions dataset\n",
    "all_players = data['Player'].unique()\n",
    "predictions = []\n",
    "\n",
    "for player in all_players:\n",
    "    player_data = data[data['Player'] == player]\n",
    "    latest_season = player_data['Season'].max()\n",
    "    next_season_data = player_data[player_data['Season'] == latest_season].copy()\n",
    "    next_season_data['Age'] += 1\n",
    "    next_season_data['Season'] += 1\n",
    "    next_season_data_scaled = scaler.transform(next_season_data[selected_features])\n",
    "    predicted_salary_pct = best_model.predict(next_season_data_scaled)[0]\n",
    "    predicted_salary = predicted_salary_pct * next_season_data['Salary Cap'].values[0]\n",
    "    \n",
    "    predictions.append({\n",
    "        'Player': player,\n",
    "        'Predicted_Season': int(next_season_data['Season'].values[0]),\n",
    "        'Age': int(next_season_data['Age'].values[0]),\n",
    "        'Predicted_Salary_Pct': predicted_salary_pct,\n",
    "        'Predicted_Salary': predicted_salary,\n",
    "        'Previous_Season_Salary': player_data[player_data['Season'] == latest_season]['Salary'].values[0],\n",
    "        'Salary_Change': predicted_salary - player_data[player_data['Season'] == latest_season]['Salary'].values[0]\n",
    "    })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df.to_csv('../data/predictions/salary_predictions.csv', index=False)\n",
    "print(\"Predictions saved to '../data/predictions/salary_predictions.csv'\")\n",
    "\n",
    "# Display some statistics about the predictions\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(predictions_df[['Predicted_Salary_Pct', 'Predicted_Salary', 'Salary_Change']].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
