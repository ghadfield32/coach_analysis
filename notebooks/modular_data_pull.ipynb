{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_utils.py\n",
    "import time\n",
    "from nba_api.stats.endpoints import commonallplayers, commonplayerinfo, playercareerstats, leaguestandings\n",
    "from requests.exceptions import RequestException\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "MAX_REQUESTS_PER_MINUTE = 30\n",
    "DELAY_BETWEEN_REQUESTS = 2\n",
    "\n",
    "def fetch_with_retry(endpoint, max_retries=5, delay=5, timeout=60, **kwargs):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Fetching data using {endpoint.__name__} (Attempt {attempt + 1}) with parameters: {kwargs}\")\n",
    "            data = endpoint(**kwargs, timeout=timeout).get_data_frames()\n",
    "            return data[0] if isinstance(data, list) else data\n",
    "        except (RequestException, JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to fetch data after {max_retries} attempts\")\n",
    "                return None\n",
    "            print(f\"Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "def fetch_all_players(season):\n",
    "    all_players_data = fetch_with_retry(commonallplayers.CommonAllPlayers, season=season)\n",
    "    all_players = {}\n",
    "    if all_players_data is not None:\n",
    "        for _, row in all_players_data.iterrows():\n",
    "            player_name = row['DISPLAY_FIRST_LAST'].strip().lower()\n",
    "            player_id = row['PERSON_ID']\n",
    "            team_id = row['TEAM_ID']\n",
    "            all_players[player_name] = {\n",
    "                'player_id': player_id,\n",
    "                'team_id': team_id\n",
    "            }\n",
    "    return all_players\n",
    "\n",
    "def fetch_player_info(player_id):\n",
    "    return fetch_with_retry(commonplayerinfo.CommonPlayerInfo, player_id=player_id)\n",
    "\n",
    "def fetch_career_stats(player_id):\n",
    "    return fetch_with_retry(playercareerstats.PlayerCareerStats, player_id=player_id)\n",
    "\n",
    "def fetch_league_standings(season):\n",
    "    return fetch_with_retry(leaguestandings.LeagueStandings, season=season)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_utils.py\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_salary_cap_history():\n",
    "    url = \"https://basketball.realgm.com/nba/info/salary_cap\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', class_='basketball compact')\n",
    "        \n",
    "        if not table:\n",
    "            print(\"Could not find the salary cap table on the page.\")\n",
    "            return None\n",
    "\n",
    "        data = []\n",
    "        headers = [th.text.strip() for th in table.find('thead').find_all('th')]\n",
    "        for row in table.find('tbody').find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                row_data = [col.text.strip() for col in cols]\n",
    "                data.append(row_data)\n",
    "\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Clean up the data\n",
    "        df['Season'] = df['Season'].str.extract(r'(\\d{4}-\\d{4})')\n",
    "        df['Salary Cap'] = df['Salary Cap'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "        \n",
    "        # Convert other columns to float, handling non-numeric values\n",
    "        for col in df.columns:\n",
    "            if col not in ['Season', 'Salary Cap']:\n",
    "                df[col] = pd.to_numeric(df[col].str.replace('$', '').str.replace(',', ''), errors='coerce')\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping salary cap history: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_player_salary_data(start_season, end_season, player_name=\"Stephen Curry\"):\n",
    "    all_data = []\n",
    "    \n",
    "    for season in range(start_season, end_season + 1):\n",
    "        season_str = f\"{season}-{str(season+1)[-2:]}\"\n",
    "        url = f\"https://hoopshype.com/salaries/players/{season}-{season+1}/\"\n",
    "        print(f\"Scraping data for {season_str} from URL: {url}\")\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', class_='hh-salaries-ranking-table')\n",
    "        \n",
    "        if table:\n",
    "            rows = table.find_all('tr')[1:]\n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                if len(cols) >= 3:\n",
    "                    player = cols[1].get_text(strip=True)\n",
    "                    if player.lower() == player_name.lower():  # Filter for the specific player\n",
    "                        salary_text = cols[2].get_text(strip=True)\n",
    "                        salary = int(salary_text.replace('$', '').replace(',', ''))\n",
    "                        all_data.append({'Player': player, 'Salary': salary, 'Season': season_str})\n",
    "                        break  # Break after finding the player in the season\n",
    "        else:\n",
    "            print(f\"No salary data found for season {season_str}\")\n",
    "        \n",
    "        time.sleep(2)  # Delay between requests to avoid hitting rate limits\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    print(f\"Scraped salary data for {player_name} from seasons {start_season}-{end_season}:\")\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "def scrape_team_salary_data(season):\n",
    "    url = f\"https://hoopshype.com/salaries/{season}/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', class_='hh-salaries-ranking-table')\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        team = cols[1].get_text(strip=True)\n",
    "        salary = int(cols[2].get_text(strip=True).replace('$', '').replace(',', ''))\n",
    "        data.append({'Team': team, 'Team_Salary': salary, 'Season': season})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_utils.py\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from fetch_utils import fetch_player_info, fetch_career_stats, fetch_league_standings\n",
    "\n",
    "def process_player_data(player, season, all_players):\n",
    "    if player.lower() not in all_players:\n",
    "        print(f\"No player ID found for {player}\")\n",
    "        return None\n",
    "\n",
    "    player_id = all_players[player.lower()]['player_id']\n",
    "    team_id = all_players[player.lower()]['team_id']\n",
    "    \n",
    "    player_info = fetch_player_info(player_id)\n",
    "    career_stats = fetch_career_stats(player_id)\n",
    "    league_standings = fetch_league_standings(season)\n",
    "    advanced_metrics = scrape_advanced_metrics(player, season)\n",
    "\n",
    "    if player_info is None or career_stats is None or career_stats.empty:\n",
    "        print(f\"Unable to fetch complete data for {player}\")\n",
    "        return None\n",
    "\n",
    "    season_stats = career_stats[career_stats['SEASON_ID'].str.contains(season.split('-')[0], na=False)]\n",
    "    if season_stats.empty:\n",
    "        print(f\"No stats found for {player} in season {season}\")\n",
    "        return None\n",
    "\n",
    "    latest_season_stats = season_stats.iloc[0]\n",
    "    \n",
    "    try:\n",
    "        draft_year = int(player_info['DRAFT_YEAR'].iloc[0])\n",
    "    except ValueError:\n",
    "        draft_year = int(player_info['FROM_YEAR'].iloc[0])\n",
    "\n",
    "    current_season_year = int(season.split('-')[0])\n",
    "    years_of_service = max(0, current_season_year - draft_year)\n",
    "\n",
    "    player_stats = calculate_player_stats(latest_season_stats, player_info, years_of_service, team_id, league_standings, advanced_metrics)\n",
    "    player_stats.update({'Player': player, 'Season': season})\n",
    "\n",
    "    return player_stats\n",
    "\n",
    "def calculate_player_stats(stats, player_info, years_of_service, team_id, league_standings, advanced_metrics):\n",
    "    fg = stats.get('FGM', 0) or 0\n",
    "    fga = stats.get('FGA', 0) or 0\n",
    "    fg3 = stats.get('FG3M', 0) or 0\n",
    "    fg3a = stats.get('FG3A', 0) or 0\n",
    "    efg = (fg + 0.5 * fg3) / fga if fga != 0 else 0\n",
    "    fg2 = fg - fg3\n",
    "    fg2a = fga - fg3a\n",
    "    fg2_pct = (fg2 / fg2a) if fg2a != 0 else 0\n",
    "\n",
    "    player_stats = {\n",
    "        'Position': player_info.iloc[0]['POSITION'],\n",
    "        'Age': stats.get('PLAYER_AGE', None),\n",
    "        'Team': stats.get('TEAM_ABBREVIATION', None),\n",
    "        'TeamID': team_id,\n",
    "        'Years of Service': years_of_service,\n",
    "        'GP': stats.get('GP', None),\n",
    "        'GS': stats.get('GS', None),\n",
    "        'MP': stats.get('MIN', None),\n",
    "        'FG': fg,\n",
    "        'FGA': fga,\n",
    "        'FG%': stats.get('FG_PCT', None),\n",
    "        '3P': fg3,\n",
    "        '3PA': fg3a,\n",
    "        '3P%': stats.get('FG3_PCT', None),\n",
    "        '2P': fg2,\n",
    "        '2PA': fg2a,\n",
    "        '2P%': fg2_pct,\n",
    "        'eFG%': efg,\n",
    "        'FT': stats.get('FTM', None),\n",
    "        'FTA': stats.get('FTA', None),\n",
    "        'FT%': stats.get('FT_PCT', None),\n",
    "        'ORB': stats.get('OREB', None),\n",
    "        'DRB': stats.get('DREB', None),\n",
    "        'TRB': stats.get('REB', None),\n",
    "        'AST': stats.get('AST', None),\n",
    "        'STL': stats.get('STL', None),\n",
    "        'BLK': stats.get('BLK', None),\n",
    "        'TOV': stats.get('TOV', None),\n",
    "        'PF': stats.get('PF', None),\n",
    "        'PTS': stats.get('PTS', None),\n",
    "    }\n",
    "    player_stats.update(advanced_metrics)\n",
    "\n",
    "    if league_standings is not None and team_id is not None:\n",
    "        team_standings = league_standings[league_standings['TeamID'] == team_id]\n",
    "        if not team_standings.empty:\n",
    "            player_stats.update({\n",
    "                'Wins': team_standings['WINS'].values[0],\n",
    "                'Losses': team_standings['LOSSES'].values[0]\n",
    "            })\n",
    "\n",
    "    return player_stats\n",
    "\n",
    "def scrape_advanced_metrics(player_name, season):\n",
    "    try:\n",
    "        search_url = f\"https://www.basketball-reference.com/search/search.fcgi?search={player_name.replace(' ', '+')}\"\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        search_results = soup.find('div', {'class': 'search-results'})\n",
    "        if search_results:\n",
    "            for item in search_results.find_all('div', {'class': 'search-item'}):\n",
    "                link = item.find('a')\n",
    "                if link and 'players' in link['href']:\n",
    "                    player_url = f\"https://www.basketball-reference.com{link['href']}\"\n",
    "                    break\n",
    "            else:\n",
    "                return {}\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "        response = requests.get(player_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'advanced'})\n",
    "        if table:\n",
    "            df = pd.read_html(StringIO(str(table)))[0]\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                df.columns = df.columns.droplevel()\n",
    "            df['Season'] = df['Season'].astype(str)\n",
    "            df = df[df['Season'].str.contains(season.split('-')[0], na=False)]\n",
    "            if not df.empty:\n",
    "                row = df.iloc[0]\n",
    "                metrics = ['PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']\n",
    "                return {col: row[col] for col in metrics if col in row.index}\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping advanced metrics for {player_name}: {e}\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/salary_predict/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/salary_predict/training.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nba_api.stats.endpoints import commonallplayers, commonplayerinfo, playercareerstats, leaguestandings\n",
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "from json.decoder import JSONDecodeError\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "from sklearn.inspection import permutation_importance\n",
    "from fetch_utils import fetch_with_retry\n",
    "from scrape_utils import scrape_player_salary_data, scrape_team_salary_data, scrape_salary_cap_history\n",
    "from player_utils import fetch_all_players, process_player_data\n",
    "\n",
    "# Import other necessary functions (fetch_with_retry, scrape_functions, etc.)\n",
    "\n",
    "def load_and_preprocess_data(file_path, use_inflated_cap=True):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    if use_inflated_cap:\n",
    "        data.drop(columns=['2022 Dollars', 'Salary Cap'], inplace=True)\n",
    "        salary_cap_column = 'Salary_Cap_Inflated'\n",
    "    else:\n",
    "        data.drop(columns=['2022 Dollars', 'Salary_Cap_Inflated'], inplace=True)\n",
    "        salary_cap_column = 'Salary Cap'\n",
    "\n",
    "    data['Season'] = data['Season'].str[:4].astype(int)\n",
    "\n",
    "    numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data[numerical_cols] = imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "    # Feature engineering\n",
    "    data['PPG'] = data['PTS'] / data['GP']\n",
    "    data['APG'] = data['AST'] / data['GP']\n",
    "    data['RPG'] = data['TRB'] / data['GP']\n",
    "    data['SPG'] = data['STL'] / data['GP']\n",
    "    data['BPG'] = data['BLK'] / data['GP']\n",
    "    data['TOPG'] = data['TOV'] / data['GP']\n",
    "    data['WinPct'] = data['Wins'] / (data['Wins'] + data['Losses'])\n",
    "    data['SalaryGrowth'] = data['Salary'].pct_change().fillna(0)\n",
    "    data['Availability'] = data['GP'] / 82\n",
    "    data['SalaryPct'] = data['Salary'] / data[salary_cap_column]\n",
    "\n",
    "    return data, salary_cap_column\n",
    "\n",
    "def prepare_data_for_training(data, salary_cap_column):\n",
    "    categorical_cols = ['Player', 'Season', 'Position', 'Team']\n",
    "    numerical_cols = data.columns.difference(categorical_cols + ['Salary', 'SalaryPct'])\n",
    "\n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    encoded_cats = pd.DataFrame(encoder.fit_transform(data[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    data = pd.concat([data[numerical_cols], encoded_cats, data[['Player', 'Season', 'Salary', 'SalaryPct']]], axis=1)\n",
    "\n",
    "    initial_features = ['Age', 'Years of Service', 'GP', 'PPG', 'APG', 'RPG', 'SPG', 'BPG', 'TOPG', 'FG%', '3P%', 'FT%', 'PER', 'WS', 'VORP', 'Availability'] + list(encoded_cats.columns)\n",
    "\n",
    "    data_subset = data[initial_features + ['SalaryPct', 'Salary']].copy()\n",
    "    data_cleaned = data_subset.dropna()\n",
    "\n",
    "    return data_cleaned, initial_features\n",
    "\n",
    "def train_models(data_cleaned, initial_features, target_variable='SalaryPct', n_features_to_select=10):\n",
    "    X = data_cleaned[initial_features]\n",
    "    y = data_cleaned[target_variable]\n",
    "\n",
    "    rfe = RFE(estimator=RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=n_features_to_select)\n",
    "    rfe = rfe.fit(X, y)\n",
    "    selected_features = [feature for feature, selected in zip(initial_features, rfe.support_) if selected]\n",
    "\n",
    "    print(\"Selected features by RFE:\", selected_features)\n",
    "\n",
    "    X = data_cleaned[selected_features]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    models = {\n",
    "        'Random_Forest': RandomForestRegressor(random_state=42),\n",
    "        'Gradient_Boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'Ridge_Regression': Ridge(),\n",
    "        'ElasticNet': ElasticNet(max_iter=10000),\n",
    "        'SVR': SVR(),\n",
    "        'Decision_Tree': DecisionTreeRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    param_grids = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'max_depth': [8, 10, 12],\n",
    "            'min_samples_split': [5, 10, 15],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        },\n",
    "        'Ridge Regression': {'alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "        'ElasticNet': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]},\n",
    "        'SVR': {'C': [0.1, 1, 10], 'epsilon': [0.1, 0.2, 0.5]},\n",
    "        'Decision Tree': {'max_depth': [6, 8, 10], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        \n",
    "        cv_scores = cross_val_score(best_models[name], X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "        print(f\"{name} - Best params: {grid_search.best_params_}\")\n",
    "        print(f\"{name} - Cross-validation MSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        y_pred = best_models[name].predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"{name} - Test MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "        \n",
    "        if name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:\n",
    "            importances = best_models[name].feature_importances_\n",
    "            feature_importance = pd.DataFrame({'feature': selected_features, 'importance': importances})\n",
    "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "            print(f\"\\n{name} - Top 5 important features:\")\n",
    "            print(feature_importance.head())\n",
    "        else:\n",
    "            perm_importance = permutation_importance(best_models[name], X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "            feature_importance = pd.DataFrame({'feature': selected_features, 'importance': perm_importance.importances_mean})\n",
    "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "            print(f\"\\n{name} - Top 5 important features (Permutation Importance):\")\n",
    "            print(feature_importance.head())\n",
    "        \n",
    "        model_filename = f\"../data/models/{name}_salary_prediction_model_{'inflated' if target_variable == 'SalaryPct' else 'regular'}.joblib\"\n",
    "        joblib.dump(best_models[name], model_filename)\n",
    "        print(f\"{name} model saved to '{model_filename}'\")\n",
    "\n",
    "    return best_models, scaler, selected_features\n",
    "\n",
    "def main():\n",
    "    start_year = 2024  # Current year\n",
    "    end_year = 2022    # Adjust as needed\n",
    "    processed_file_path = '../data/processed/nba_player_data.csv'\n",
    "    salary_cap_file_path = '../data/processed/salary_cap_history.csv'\n",
    "\n",
    "    player_filter = input(\"Enter player name or 'all' for all players: \").strip().lower()\n",
    "    min_avg_minutes = None\n",
    "    if player_filter == 'all':\n",
    "        min_avg_minutes = float(input(\"Enter the minimum average minutes per game (default 25 mins): \") or 25)\n",
    "\n",
    "    existing_data = load_existing_data(processed_file_path)\n",
    "\n",
    "    try:\n",
    "        print(f\"Updating data for years {start_year} to {end_year}\")\n",
    "        updated_data = update_data(existing_data, start_year, end_year, player_filter, min_avg_minutes)\n",
    "        \n",
    "        if not updated_data.equals(existing_data):\n",
    "            print(\"New data retrieved. Merging with existing data...\")\n",
    "            \n",
    "            print(\"Fetching salary cap data...\")\n",
    "            salary_cap_data = scrape_salary_cap_history()\n",
    "            \n",
    "            if salary_cap_data is not None:\n",
    "                print(\"Salary cap data successfully retrieved.\")\n",
    "                \n",
    "                salary_cap_data.to_csv(salary_cap_file_path, index=False)\n",
    "                print(f\"Salary cap data saved to {salary_cap_file_path}\")\n",
    "                \n",
    "                print(\"Merging salary cap data with player data...\")\n",
    "                \n",
    "                salary_cap_columns = [col for col in updated_data.columns if 'Salary Cap' in col]\n",
    "                if salary_cap_columns:\n",
    "                    print(f\"Removing existing Salary Cap columns: {salary_cap_columns}\")\n",
    "                    updated_data = updated_data.drop(columns=salary_cap_columns)\n",
    "                    \n",
    "                updated_data = pd.merge(updated_data, salary_cap_data[['Season', 'Salary Cap']], on='Season', how='left')\n",
    "                \n",
    "                print(\"Final data shape:\", updated_data.shape)\n",
    "                print(\"Final data columns:\", updated_data.columns)\n",
    "            else:\n",
    "                print(\"Warning: Failed to retrieve salary cap data. Skipping merge.\")\n",
    "\n",
    "            updated_data.to_csv(processed_file_path, index=False)\n",
    "            print(f\"Updated data saved to {processed_file_path}\")\n",
    "        else:\n",
    "            print(\"No new data to save. The dataset is already up-to-date.\")\n",
    "\n",
    "        # Ask if user wants to train models\n",
    "        train_models_option = input(\"Do you want to train the models? (yes/no): \").strip().lower()\n",
    "        if train_models_option == 'yes':\n",
    "            use_inflated_cap = input(\"Use inflated salary cap? (yes/no): \").strip().lower() == 'yes'\n",
    "            target_variable = input(\"Choose target variable (SalaryPct/Salary): \").strip()\n",
    "            \n",
    "            data, salary_cap_column = load_and_preprocess_data(processed_file_path, use_inflated_cap)\n",
    "            data_cleaned, initial_features = prepare_data_for_training(data, salary_cap_column)\n",
    "            best_models, scaler, selected_features = train_models(data_cleaned, initial_features, target_variable)\n",
    "            \n",
    "            print(\"Model training completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        print(\"Traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating data for years 2023 to 2024\n",
      "Scraping data for 2023-24 from URL: https://hoopshype.com/salaries/players/2023-2024/\n",
      "Scraping data for 2024-25 from URL: https://hoopshype.com/salaries/players/2024-2025/\n",
      "Scraped salary data for Stephen Curry from seasons 2023-2024:\n",
      "          Player    Salary   Season\n",
      "0  Stephen Curry  51915615  2023-24\n",
      "\n",
      "Processing data for 2023-24...\n",
      "Fetching data using CommonAllPlayers (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76001}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76001}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Alaa Abdelnaby in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76002}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76002}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Zaid Abdul-Aziz in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76003}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76003}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Kareem Abdul-Jabbar in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 51}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 51}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Mahmoud Abdul-Rauf in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1505}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1505}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Tariq Abdul-Wahad in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 949}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 949}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Shareef Abdur-Rahim in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76005}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76005}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Tom Abernethy in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76006}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76006}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Forest Able in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76007}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76007}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for John Abramovic in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 203518}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 203518}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Alex Abrines in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1630173}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1630173}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No salary data found for precious achiuwa in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 101165}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 101165}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Alex Acker in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76008}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76008}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Donald Ackerman in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76009}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76009}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Mark Acres in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76010}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76010}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Charles Acton in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 203112}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 203112}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Quincy Acy in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76011}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76011}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Alvan Adams in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76012}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76012}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Don Adams in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 200801}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 200801}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Hassan Adams in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1629121}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1629121}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Jaylen Adams in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 203919}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 203919}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Jordan Adams in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 149}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 149}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Michael Adams in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 203500}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 203500}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Steven Adams in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 912}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 912}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Rafael Addison in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1628389}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1628389}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No salary data found for bam adebayo in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1629061}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1629061}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Deng Adel in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76015}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76015}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Rick Adelman in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 202399}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 202399}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Jeff Adrien in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 201167}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 201167}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Arron Afflalo in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1630534}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1630534}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No salary data found for ochai agbaji in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 200772}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 200772}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Maurice Ager in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76016}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76016}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Mark Aguirre in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 201336}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 201336}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Blake Ahearn in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76017}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76017}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Danny Ainge in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 201582}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 201582}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Alexis Ajinca in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1642351}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1642351}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "Unable to fetch complete data for Melvin Ajinca\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76018}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76018}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Henry Akin in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 203006}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 203006}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Josh Akognon in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1629152}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1629152}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Devaughn Akoon-Purcell in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 202374}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 202374}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Solomon Alabi in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76019}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76019}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Mark Alarie in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 76020}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 76020}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Gary Alcorn in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 1630583}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 1630583}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No salary data found for santi aldama in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 203128}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 203128}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Furkan Aldemir in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 202332}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 202332}\n",
      "Fetching data using LeagueStandings (Attempt 1) with parameters: {'season': '2023-24'}\n",
      "No stats found for Cole Aldrich in season 2023-24\n",
      "Fetching data using CommonPlayerInfo (Attempt 1) with parameters: {'player_id': 200746}\n",
      "Fetching data using PlayerCareerStats (Attempt 1) with parameters: {'player_id': 200746}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 170\u001b[0m\n\u001b[1;32m    167\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 130\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating data for years \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m     updated_data \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexisting_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_avg_minutes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m updated_data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew data retrieved. Processing and saving...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 55\u001b[0m, in \u001b[0;36mupdate_data\u001b[0;34m(existing_data, start_year, end_year, min_avg_minutes)\u001b[0m\n\u001b[1;32m     52\u001b[0m additional_stats \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m player_name, player_info \u001b[38;5;129;01min\u001b[39;00m all_players\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 55\u001b[0m     player_stats \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_player_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseason\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_players\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m player_stats:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Merge salary data\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         player_salary_row \u001b[38;5;241m=\u001b[39m salary_data[(salary_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeason\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m season) \u001b[38;5;241m&\u001b[39m (salary_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlayer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m player_name)]\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mprocess_player_data\u001b[0;34m(player, season, all_players)\u001b[0m\n\u001b[1;32m     14\u001b[0m team_id \u001b[38;5;241m=\u001b[39m all_players[player\u001b[38;5;241m.\u001b[39mlower()][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m player_info \u001b[38;5;241m=\u001b[39m fetch_player_info(player_id)\n\u001b[0;32m---> 17\u001b[0m career_stats \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_career_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m league_standings \u001b[38;5;241m=\u001b[39m fetch_league_standings(season)\n\u001b[1;32m     19\u001b[0m advanced_metrics \u001b[38;5;241m=\u001b[39m scrape_advanced_metrics(player, season)\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36mfetch_career_stats\u001b[0;34m(player_id)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_career_stats\u001b[39m(player_id):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfetch_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayercareerstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPlayerCareerStats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplayer_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mfetch_with_retry\u001b[0;34m(endpoint, max_retries, delay, timeout, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching data using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) with parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mendpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_data_frames()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m data\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (RequestException, JSONDecodeError, \u001b[38;5;167;01mKeyError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/nba_api/stats/endpoints/playercareerstats.py:315\u001b[0m, in \u001b[0;36mPlayerCareerStats.__init__\u001b[0;34m(self, player_id, per_mode36, league_id_nullable, proxy, headers, timeout, get_request)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlayerID\u001b[39m\u001b[38;5;124m\"\u001b[39m: player_id,\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerMode\u001b[39m\u001b[38;5;124m\"\u001b[39m: per_mode36,\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeagueID\u001b[39m\u001b[38;5;124m\"\u001b[39m: league_id_nullable,\n\u001b[1;32m    313\u001b[0m }\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_request:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/nba_api/stats/endpoints/playercareerstats.py:318\u001b[0m, in \u001b[0;36mPlayerCareerStats.get_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_request\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnba_response \u001b[38;5;241m=\u001b[39m \u001b[43mNBAStatsHTTP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_api_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_response()\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/nba_api/library/http.py:146\u001b[0m, in \u001b[0;36mNBAHTTP.send_api_request\u001b[0;34m(self, endpoint, parameters, referer, proxy, headers, timeout, raise_exception_on_error)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading from file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contents:\n\u001b[0;32m--> 146\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     url \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39murl\n\u001b[1;32m    154\u001b[0m     status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/envs/data_science/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from fetch_utils import fetch_all_players\n",
    "# from scrape_utils import scrape_player_salary_data, scrape_team_salary_data, scrape_salary_cap_history\n",
    "# from player_utils import process_player_data\n",
    "\n",
    "def load_existing_data(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_latest_season(data):\n",
    "    if not data.empty:\n",
    "        return data['Season'].max()\n",
    "    return None\n",
    "\n",
    "def merge_salary_cap_data(player_data, salary_cap_data):\n",
    "    player_data['Season_Year'] = player_data['Season'].str[:4]\n",
    "    salary_cap_data['Season_Year'] = salary_cap_data['Season'].str[:4]\n",
    "    merged_data = pd.merge(player_data, salary_cap_data, on='Season_Year', how='left')\n",
    "    merged_data.drop(columns=['Season_Year', 'Season_y'], inplace=True)\n",
    "    merged_data.rename(columns={'Season_x': 'Season'}, inplace=True)\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def update_data(existing_data, start_year, end_year, min_avg_minutes=None):\n",
    "    all_data = existing_data.copy()\n",
    "    latest_season = get_latest_season(all_data)\n",
    "\n",
    "    # Load injury data\n",
    "    injury_data = pd.read_csv('../data/processed/NBA Player Injury Stats(1951 - 2023).csv')\n",
    "    injury_data['Date'] = pd.to_datetime(injury_data['Date'])\n",
    "    injury_data['Season'] = injury_data['Date'].apply(lambda x: f\"{x.year}-{str(x.year+1)[-2:]}\" if x.month >= 10 else f\"{x.year-1}-{str(x.year)[-2:]}\")\n",
    "\n",
    "    # Fetch salary data for all seasons at once\n",
    "    salary_data = scrape_player_salary_data(start_year, end_year)\n",
    "\n",
    "    for year in range(start_year, end_year):\n",
    "        season = f\"{year}-{str(year+1)[-2:]}\"\n",
    "        if latest_season and season <= latest_season:\n",
    "            print(f\"Skipping {season} as it's already in the dataset.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing data for {season}...\")\n",
    "\n",
    "        team_salary_data = scrape_team_salary_data(season)\n",
    "\n",
    "        all_players = fetch_all_players(season=season)\n",
    "        additional_stats = []\n",
    "\n",
    "        for player_name, player_info in all_players.items():\n",
    "            player_stats = process_player_data(player_name.title(), season, all_players)\n",
    "            if player_stats:\n",
    "                # Merge salary data\n",
    "                player_salary_row = salary_data[(salary_data['Season'] == season) & (salary_data['Player'].str.lower() == player_name)]\n",
    "                if not player_salary_row.empty:\n",
    "                    player_stats['Salary'] = player_salary_row.iloc[0]['Salary']\n",
    "                else:\n",
    "                    print(f\"No salary data found for {player_name} in season {season}\")\n",
    "                    player_stats['Salary'] = None\n",
    "\n",
    "                # Filter by average minutes if required\n",
    "                if min_avg_minutes is None or player_stats['MP'] / player_stats['GP'] >= min_avg_minutes:\n",
    "                    additional_stats.append(player_stats)\n",
    "\n",
    "        additional_stats_df = pd.DataFrame(additional_stats)\n",
    "\n",
    "        if not additional_stats_df.empty:\n",
    "            merged_data = additional_stats_df\n",
    "            merged_data = pd.merge(merged_data, team_salary_data, on=['Team', 'Season'], how='left')\n",
    "\n",
    "            # Process and merge injury data\n",
    "            season_injury_data = injury_data[injury_data['Season'] == season]\n",
    "\n",
    "            # Create a DataFrame with all players and initialize injury columns\n",
    "            all_players_df = pd.DataFrame({'Player': merged_data['Player'].unique()})\n",
    "            all_players_df['Season'] = season\n",
    "            all_players_df['Injured'] = False\n",
    "            all_players_df['Injury_Periods'] = ''\n",
    "\n",
    "            # Update injury information for players with injuries\n",
    "            for player in all_players_df['Player']:\n",
    "                player_injuries = season_injury_data[season_injury_data['Relinquished'].str.contains(player, case=False, na=False)]\n",
    "                if not player_injuries.empty:\n",
    "                    periods = []\n",
    "                    for i in range(0, len(player_injuries), 2):\n",
    "                        try:\n",
    "                            start_date = player_injuries.iloc[i]['Date'].strftime('%Y-%m-%d')\n",
    "                            end_date = player_injuries.iloc[i+1]['Date'].strftime('%Y-%m-%d')\n",
    "                            periods.append(f\"{start_date} - {end_date}\")\n",
    "                        except IndexError:\n",
    "                            periods.append(f\"{start_date} - ongoing\")\n",
    "                    all_players_df.loc[all_players_df['Player'] == player, 'Injured'] = True\n",
    "                    all_players_df.loc[all_players_df['Player'] == player, 'Injury_Periods'] = '; '.join(periods)\n",
    "\n",
    "            # Merge injury data with player stats\n",
    "            merged_data = pd.merge(merged_data, all_players_df, on=['Player', 'Season'], how='outer')\n",
    "\n",
    "            # Fill missing values for players without stats\n",
    "            merged_data = merged_data.fillna({col: 0 for col in merged_data.columns if merged_data[col].dtype in [np.int64, np.float64]})\n",
    "            merged_data = merged_data.fillna('')\n",
    "\n",
    "            all_data = pd.concat([all_data, merged_data], ignore_index=True)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def main():\n",
    "    start_year = 2023  # Adjusted to match the example\n",
    "    end_year = 2024    # Adjusted to match the example\n",
    "    processed_file_path = '../data/processed/nba_player_data.csv'\n",
    "    salary_cap_file_path = '../data/processed/salary_cap_history.csv'\n",
    "\n",
    "    player_filter = input(\"Enter player name or 'all' for all players: \").strip().lower()\n",
    "    min_avg_minutes = None\n",
    "    if player_filter == 'all':\n",
    "        min_avg_minutes = float(input(\"Enter the minimum average minutes per game (default 25 mins): \") or 25)\n",
    "\n",
    "    # Delete existing processed file if it exists\n",
    "    if os.path.exists(processed_file_path):\n",
    "        os.remove(processed_file_path)\n",
    "        print(f\"Deleted existing file: {processed_file_path}\")\n",
    "\n",
    "    existing_data = pd.DataFrame()  # Start with an empty DataFrame\n",
    "\n",
    "    try:\n",
    "        print(f\"Updating data for years {start_year} to {end_year}\")\n",
    "        updated_data = update_data(existing_data, start_year, end_year, min_avg_minutes)\n",
    "\n",
    "        if not updated_data.empty:\n",
    "            print(\"New data retrieved. Processing and saving...\")\n",
    "\n",
    "            print(\"Fetching salary cap data...\")\n",
    "            salary_cap_data = scrape_salary_cap_history()\n",
    "\n",
    "            if salary_cap_data is not None:\n",
    "                print(\"Salary cap data successfully retrieved.\")\n",
    "\n",
    "                salary_cap_data.to_csv(salary_cap_file_path, index=False)\n",
    "                print(f\"Salary cap data saved to {salary_cap_file_path}\")\n",
    "\n",
    "                print(\"Merging salary cap data with player data...\")\n",
    "\n",
    "                updated_data = merge_salary_cap_data(updated_data, salary_cap_data)\n",
    "\n",
    "                print(\"Final data shape:\", updated_data.shape)\n",
    "                print(\"Final data columns:\", updated_data.columns)\n",
    "            else:\n",
    "                print(\"Warning: Failed to retrieve salary cap data. Skipping merge.\")\n",
    "\n",
    "            # Save the cleaned data\n",
    "            updated_data.to_csv(processed_file_path, index=False)\n",
    "            print(f\"Updated and cleaned data saved to {processed_file_path}\")\n",
    "\n",
    "            # Print summary of the data\n",
    "            print(\"\\nData summary\")\n",
    "            print(updated_data[['Season', 'Player', 'Salary', 'GP', 'PTS', 'TRB', 'AST', 'Injured', 'Injury_Periods']].to_string(index=False))\n",
    "        else:\n",
    "            print(\"No new data to save. The dataset is empty.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        print(\"Traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
